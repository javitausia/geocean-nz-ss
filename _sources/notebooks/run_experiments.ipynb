{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19586d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os, sys\n",
    "\n",
    "# arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Object pickling\n",
    "import dill\n",
    "\n",
    "# append sscode to path\n",
    "sys.path.insert(0, '..')\n",
    "# data_path = '/data/' #'/data/storm_surge_data/'\n",
    "# os.environ[\"SSURGE_DATA_PATH\"] = data_path\n",
    "\n",
    "# custom\n",
    "from sscode.config import (\n",
    "    default_location,\n",
    "    default_region,\n",
    "    default_region_reduced\n",
    ")\n",
    "from sscode.data import Loader\n",
    "from sscode.pca_new import PCA_DynamicPred\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for autocomplete code\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb55ef3",
   "metadata": {},
   "source": [
    "We start by loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "load_cfsr_moana_uhslc = Loader(\n",
    "    data_to_load=['cfsr','moana','uhslc'], plot=False, \n",
    "    time_resample='6H', load_winds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b9a94",
   "metadata": {},
   "source": [
    "The experiments we are about to run all involve running the predictors through a first step of Principal Component Analysis (PCA). The analysis can be quite expensive to run and as we don't want to repeat the operation multiple time we first precompute all PCs and store them on drive.\n",
    "The following class has been designed to sequentialy run the PCA analysis for all the sets of predictors considers in the experiments. We found that running the analysis on the largest predictor sets required the full amount of the 128 Gb of RAM available on our server. Also, as the PCA function from scikit learn used under the hood is parralelised this sequential approach made full usage of the CPU resources too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_PreComputer(object):\n",
    "    \"\"\"\n",
    "    This class allows to compute the principal components for givens sets of predictors and\n",
    "    store them on drive\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, slp_data,\n",
    "                 wind_data,\n",
    "                 ss_data,\n",
    "                 sites_to_analyze,\n",
    "                 pca_attrs,\n",
    "                 pcs_folder):\n",
    "        \"\"\"\n",
    "        As the initializator, the __init__ function creates the instance of the class,\n",
    "        given a set of parameters, which are described below\n",
    "\n",
    "        Args:\n",
    "            slp_data (xarray.Dataset): These are the sea-level-pressure fields, previously\n",
    "                loaded with the Loader class, loader.predictor_slp!!\n",
    "            wind_data (xarray.Dataset): These are the wind fields, previously\n",
    "                loaded with the Loader class, loader.predictor_wind!!\n",
    "            ss_data (xarray.Dataset): This is the storm surge from the moana hindcast, previously\n",
    "                loaded with the Loader class, loader.predictand!!\n",
    "            sites_to_analyze (list, optional): This is the list with all the moana v2\n",
    "                hindcast locations to analyze. Defaults to random locations.\n",
    "            pca_attrs (dict, optional): PCA dictionary with all the parameters to use in pca.\n",
    "                Defaults to pca_attrs_default.\n",
    "            pca_folder (string, optionl): The folder from/in which the PCs should be loaded/saved.\n",
    "        \"\"\"\n",
    "\n",
    "        self.slp_data = slp_data\n",
    "        self.wind_data = wind_data\n",
    "        self.ss_data = ss_data\n",
    "        self.ss_sites = sites_to_analyze\n",
    "        self.pca_attrs = pca_attrs\n",
    "        self.pcs_folder = pcs_folder\n",
    "    \n",
    "\n",
    "    def compute_pcas(self):\n",
    "        \"\"\"\n",
    "        This function goes over all the sites/configurations and computes\n",
    "        and stores the results of the PCA analysis if not already available.\n",
    "\n",
    "        Args:\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        for isite,site in enumerate(self.ss_sites):\n",
    "\n",
    "            # we first load in memmory the selected site\n",
    "            ss_site = self.ss_data.isel(site=site)[[\n",
    "                'ss','lat','lon'\n",
    "            ]].load() # load the ss and the location of the site\n",
    "            site_location = (ss_site.lon.values,ss_site.lat.values)\n",
    "\n",
    "            # lets iterate over all the pca_attrs + model_attrs\n",
    "            counter = 0\n",
    "            for i_parameters,parameters in zip(\n",
    "                [(icg,iw,itl,itr,ir) \\\n",
    "                    for icg in [i for i in range(len(list(self.pca_attrs.values())[0]))] \\\n",
    "                    for iw in [i for i in range(len(list(self.pca_attrs.values())[1]))] \\\n",
    "                    for itl in [i for i in range(len(list(self.pca_attrs.values())[2]))] \\\n",
    "                    for itr in [i for i in range(len(list(self.pca_attrs.values())[3]))] \\\n",
    "                    for ir in [i for i in range(len(list(self.pca_attrs.values())[4]))]]\n",
    "                , [(cg,w,tl,tr,r) \\\n",
    "                    for cg in list(self.pca_attrs.values())[0] \\\n",
    "                    for w in list(self.pca_attrs.values())[1] \\\n",
    "                    for tl in list(self.pca_attrs.values())[2] \\\n",
    "                    for tr in list(self.pca_attrs.values())[3] \\\n",
    "                    for r in list(self.pca_attrs.values())[4]]\n",
    "            ):  \n",
    "            \n",
    "                # Some info\n",
    "                print(\n",
    "                    '\\n --------------------------------------------------------- \\\n",
    "                    \\n\\n Experiment {} in site {}, coords = {} ...... \\\n",
    "                    \\n\\n pca_params = {} \\n\\n \\\n",
    "                    \\n\\n and iteration with indexes = {} \\\n",
    "                    \\n\\n ---------------------------------------------------------'.format(\n",
    "                        counter+1, # this is just the counter\n",
    "                        site, # site to analyze in this loop\n",
    "                        site_location, # site coordinates\n",
    "                        dict(zip(self.pca_attrs.keys(),parameters[:5])),\n",
    "                        i_parameters # this are the parameters indexes\n",
    "                    ), end='\\r'\n",
    "                )\n",
    "\n",
    "                # perform the experiment\n",
    "                dict_to_pca = dict(zip(list(self.pca_attrs.keys()),parameters[:5]))\n",
    "                trash = dict_to_pca.pop('winds')\n",
    "                \n",
    "                # change region parameter if local area is required\n",
    "                if parameters[4][0]=='local':\n",
    "                    local_region = (True,(\n",
    "                        site_location[0]-parameters[4][1][0], # new lon / lat region\n",
    "                        site_location[0]+parameters[4][1][0],\n",
    "                        site_location[1]+parameters[4][1][1],\n",
    "                        site_location[1]-parameters[4][1][1]\n",
    "                    ))\n",
    "                    dict_to_pca['region'] = local_region\n",
    "\n",
    "                pca_data, pca_scaler = PCA_DynamicPred(\n",
    "                    self.slp_data,\n",
    "                    pres_vars=('SLP','longitude','latitude'),\n",
    "                    wind=self.wind_data if parameters[1] else None,\n",
    "                    wind_vars=('wind_proj_mask','lon','lat','U_GRD_L103','V_GRD_L103'),\n",
    "                    pca_plot=(True,False,1),\n",
    "                    verbose=True,\n",
    "                    pcs_folder=self.pcs_folder,\n",
    "                    site_id=site,\n",
    "                    site_location=site_location,\n",
    "                    pca_percent=0.99,\n",
    "                    pca_method='cpu',\n",
    "                    **dict_to_pca # extra arguments without the winds\n",
    "                ).pcs_get()\n",
    "                \n",
    "                # this seems necessary to keep memory under control    \n",
    "                del pca_data\n",
    "                del pca_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6b3f33",
   "metadata": {},
   "source": [
    "Here we define all the configurations to run and precompute and store the coreesponding principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_attrs = {\n",
    "    'calculate_gradient': [True, False],\n",
    "    'winds': [True, False],\n",
    "    'time_lapse': [1,2,3], # 1 equals to NO time delay \n",
    "    'time_resample': ['1D', '12H', '6H'], # 1D and 12H, 6H available...\n",
    "    'region': [('local', (1.5, 1.5)), ('local',(2.5,2.5)), (True,default_region_reduced)]\n",
    "}\n",
    "\n",
    "sites_to_analyze = np.unique( # closest Moana v2 Hindcast to tidal gauges\n",
    "    [  689,328,393,1327,393,480,999,116,224,1124,949,708, # UHSLC\n",
    "       1296,378,1124,780,613,488,1442,1217,578,200,1177,1025,689,949,224,1146, # LINZ\n",
    "       1174,1260,1217,744,1064,1214,803,999 # OTHER (ports...)\n",
    "    ]   \n",
    ")\n",
    "\n",
    "PCA_precomputer = PCA_PreComputer(\n",
    "    load_cfsr_moana_uhslc.predictor_slp,\n",
    "    load_cfsr_moana_uhslc.predictor_wind,\n",
    "    load_cfsr_moana_uhslc.predictand, # all the sites are passed to exp at first\n",
    "    sites_to_analyze=sites_to_analyze, \n",
    "    pca_attrs=pca_attrs,\n",
    "    pcs_folder='/home/javitausia/Documentos/geocean-nz-ss/data/pcs/'\n",
    ")\n",
    "\n",
    "PCA_precomputer.compute_pcas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d779059",
   "metadata": {},
   "source": [
    "Now that the principal components are available on drive, we can release the memory occupied by the different data and objects used in the precomputing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0168a66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA_precomputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-60ba38f9aae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mPCA_precomputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mload_cfsr_moana_uhslc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA_precomputer' is not defined"
     ]
    }
   ],
   "source": [
    "del PCA_precomputer\n",
    "del load_cfsr_moana_uhslc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a07506",
   "metadata": {},
   "source": [
    "Unlike the PCA algorithms the regression experiments have a small memory footprint and run on a single core. Hence to maximise ressource usage we run multiple experiments in parallel. We parallelise the experiments run in a way that all predictor options for a single site are run in a separate process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fefd0f",
   "metadata": {},
   "source": [
    "First we create a function that can run all perdictor and all algorithm configuration for a single site location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2866cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost_experiment(site_num):\n",
    "    \n",
    "    import os, sys\n",
    "    # append sscode to path\n",
    "    sys.path.insert(0, '/home/metocean/geocean-nz-ss')\n",
    "    data_path = '/data' #'/data/storm_surge_data/'\n",
    "    os.environ[\"SSURGE_DATA_PATH\"] = data_path\n",
    "    \n",
    "    from sscode.data import load_moana_hindcast\n",
    "    from sscode.experiment_new import Experiment\n",
    "    from sscode.config import default_region_reduced\n",
    "    \n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import dill\n",
    "    \n",
    "    # experiment attributes\n",
    "    # ---------------------\n",
    "    sites_to_analyze = [site_num]\n",
    "    \n",
    "    # Predictor configurations to iterate through\n",
    "    pca_attrs_exp = {\n",
    "            'calculate_gradient': [False,True],\n",
    "            'winds': [False,True],\n",
    "            'time_lapse': [1,2,3], # 1 equals to NO time delay \n",
    "            'time_resample': ['6H','12H','1D'], # 6H and 12H available...\n",
    "            'region': [('local',(1.5,1.5)),('local',(2.5,2.5)),(True,default_region_reduced)]\n",
    "    }\n",
    "    \n",
    "    # Labels to use for the different predictor domains\n",
    "    region_labels = ['local_1.5_1.5', 'local_2.5_2.5', 'default_region_reduced']\n",
    "    \n",
    "    \n",
    "    # Configuration of the xgboost regression algorithm to iterate through\n",
    "    xgboost_attrs_exp = {\n",
    "        'train_size': [0.7], 'percentage_PCs': [0.98],\n",
    "        'n_estimators': [50], 'max_depth': [6,12,18],\n",
    "        'min_samples_split': [0.02,0.06,0.1],\n",
    "        'learning_rate': [0.1], 'loss': ['ls'] # more could be added\n",
    "    }\n",
    "    \n",
    "    # Loading predictand data\n",
    "    predictand = load_moana_hindcast(plot=False)\n",
    "    \n",
    "    # create the experiment\n",
    "    experiment = Experiment(None, # No need for pressure data we expect PCs to be already available\n",
    "                            None, # No need for wind data we expect PCs to be already available\n",
    "                            predictand,\n",
    "                            sites_to_analyze=sites_to_analyze, \n",
    "                            model='xgboost', # model that will be used to predict\n",
    "                            pca_attrs=pca_attrs_exp,\n",
    "                            model_attrs=xgboost_attrs_exp,\n",
    "                            pcs_folder='/home/metocean/pcs/')\n",
    "    \n",
    "    # Run experiment\n",
    "    exp_parameters, exp_mean_parameters, stats_keys =\\\n",
    "        experiment.execute_cross_model_calculations(verbose=False,\n",
    "                                                    plot=False # plot logs when computing the models\n",
    "                                                    )\n",
    "    \n",
    "    # Pack results\n",
    "    coords_dict = {'grad': pca_attrs_exp['calculate_gradient'],\n",
    "                   'winds': pca_attrs_exp['winds'],\n",
    "                   'tlapse': pca_attrs_exp['time_lapse'],\n",
    "                   'tresample': pca_attrs_exp['time_resample'],\n",
    "                   'region': region_labels,\n",
    "                   'tsize': xgboost_attrs_exp['train_size'],\n",
    "                   'perpcs': xgboost_attrs_exp['percentage_PCs'],\n",
    "                   'n_estimators': xgboost_attrs_exp['n_estimators'],\n",
    "                   'max_depth': xgboost_attrs_exp['max_depth'],\n",
    "                   'min_samples_split': xgboost_attrs_exp['min_samples_split'],\n",
    "                   'learning_rate': xgboost_attrs_exp['learning_rate'],\n",
    "                   'loss': xgboost_attrs_exp['loss']\n",
    "                  }\n",
    "    \n",
    "    site_metrics = {}\n",
    "    for im,metric in enumerate(stats_keys):\n",
    "        site_metrics[metric] = (\n",
    "            ('grad','winds','tlapse','tresample','region','tsize','perpcs',\n",
    "             'n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss'),\n",
    "             exp_parameters[0][:,:,:,:,:,:,:,:,:,:,:,:,im])\n",
    "    experiment_metrics = xr.Dataset(site_metrics).assign(coords_dict).expand_dims({'site':[experiment.ss_sites[0]]})\n",
    "  \n",
    "    # Return results as pickle object\n",
    "    return dill.dumps(experiment_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fac1ee",
   "metadata": {},
   "source": [
    "Given a list of site ids we want to run all the experiments for, we create a pool of size 8 which will allow us to keep one job running on each of the 8 cores available on our server and run all the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_to_analyze = np.unique( # closest Moana v2 Hindcast to tidal gauges\n",
    "    [ 689,328,393,1327,393,480,999,116,224,1124,949,708, # UHSLC\n",
    "      1296,378,1124,780,613,488,1442,1217,578,  200,1177,1025,689,949,224,1146, # LINZ\n",
    "      1174,1260,1217,744,1064,1214,803,999 # OTHER (ports...)\n",
    "    ]\n",
    ")\n",
    "    \n",
    "with Pool(8) as p:\n",
    "    all_results = p.map(run_xgboost_experiment, sites_to_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49480cec",
   "metadata": {},
   "source": [
    "Once all the experiments are run, the results can be put together into an xarray dataset and stored to drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba142118",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_results = xr.concat([dill.loads(r) for r in all_results],\n",
    "                             dim='site')\n",
    "xgboost_results.to_netcdf('xgboost_results.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809b1d5",
   "metadata": {},
   "source": [
    "We repeat the operation for the multilinear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c057f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_experiment(site_num):\n",
    "    \n",
    "    import os, sys\n",
    "    # append sscode to path\n",
    "    sys.path.insert(0, '/home/metocean/geocean-nz-ss')\n",
    "    data_path = '/data' #'/data/storm_surge_data/'\n",
    "    os.environ[\"SSURGE_DATA_PATH\"] = data_path\n",
    "    \n",
    "    from sscode.data import load_moana_hindcast\n",
    "    from sscode.experiment_new import Experiment\n",
    "    from sscode.config import default_region_reduced\n",
    "    \n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import dill\n",
    "    \n",
    "    # experiment attributes\n",
    "    # ---------------------\n",
    "    \n",
    "    sites_to_analyze = [site_num]\n",
    "\n",
    "    # Predictor configurations to iterate through\n",
    "    pca_attrs_exp = {\n",
    "        'calculate_gradient': [False,True],\n",
    "        'winds': [False,True],\n",
    "        'time_lapse': [1,2,3], # 1 equals to NO time delay \n",
    "        'time_resample': ['6H','12H','1D'], # 6H and 12H available...\n",
    "        'region': [('local',(1.5,1.5)),('local',(2.5,2.5)),(True,default_region_reduced)]\n",
    "    }\n",
    "        \n",
    "    # Labels to use for the different predictor domains\n",
    "    region_labels = ['local_1.5_1.5', 'local_2.5_2.5', 'default_region_reduced']\n",
    "    \n",
    "    # Configuration of the linear regression algorithm to iterate through\n",
    "    linear_attrs_exp = {\n",
    "        'train_size': [0.7], 'percentage_PCs': [0.98]\n",
    "    }\n",
    "    \n",
    "    # Loading predictand data\n",
    "    predictand = load_moana_hindcast(plot=False)\n",
    "    \n",
    "    # Create the experiment\n",
    "    experiment = Experiment(None,\n",
    "                            None,\n",
    "                            predictand, # all the sites are passed to exp at first\n",
    "                            sites_to_analyze=sites_to_analyze, \n",
    "                            model='linear', # model that will be used to predict\n",
    "                            pca_attrs=pca_attrs_exp,\n",
    "                            model_attrs=linear_attrs_exp,\n",
    "                            pcs_folder='/home/metocean/pcs/')\n",
    "    \n",
    "    # Run the experiment\n",
    "    exp_parameters, exp_mean_parameters, stats_keys =\\\n",
    "        experiment.execute_cross_model_calculations(verbose=False,\n",
    "                                                    plot=False # plot logs when computing the models\n",
    "                                                    )\n",
    "    \n",
    "    coords_dict = {'grad': pca_attrs_exp['calculate_gradient'],\n",
    "                   'winds': pca_attrs_exp['winds'],\n",
    "                   'tlapse': pca_attrs_exp['time_lapse'],\n",
    "                   'tresample': pca_attrs_exp['time_resample'],\n",
    "                   'region': region_labels,\n",
    "                   'tsize': linear_attrs_exp['train_size'],\n",
    "                   'perpcs': linear_attrs_exp['percentage_PCs']}\n",
    "    \n",
    "    site_metrics = {}\n",
    "    for im,metric in enumerate(stats_keys):\n",
    "        site_metrics[metric] = (\n",
    "            ('grad','winds','tlapse','tresample','region','tsize','perpcs'),\n",
    "             exp_parameters[0][:,:,:,:,:,:,:,im])\n",
    "    experiment_metrics = xr.Dataset(site_metrics).assign(coords_dict).expand_dims({'site':[experiment.ss_sites[0]]})\n",
    "    \n",
    "    return dill.dumps(experiment_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a64d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(8) as p:\n",
    "    all_results = p.map(run_linear_experiment, sites_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_results = xr.concat([dill.loads(r) for r in all_results],\n",
    "                            dim='site')\n",
    "linear_results.to_netcdf('linear_results.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc2dbe7",
   "metadata": {},
   "source": [
    "And finally we run the knn experiments. This time we choose to run one process per number of neignour value instead of per site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_experiment(k_neighbors):\n",
    "    \n",
    "    import os, sys\n",
    "    # append sscode to path\n",
    "    sys.path.insert(0, '/home/metocean/geocean-nz-ss')\n",
    "    data_path = '/data' #'/data/storm_surge_data/'\n",
    "    os.environ[\"SSURGE_DATA_PATH\"] = data_path\n",
    "    \n",
    "    from sscode.data import load_moana_hindcast\n",
    "    from sscode.experiment_new import Experiment\n",
    "    # custom\n",
    "    from sscode.config import default_region_reduced\n",
    "    \n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    import dill\n",
    "    \n",
    "    # experiment attributes\n",
    "    # ---------------------\n",
    "    \n",
    "    # sites to iterated through\n",
    "    sites_to_analyze = np.unique( # closest Moana v2 Hindcast to tidal gauges\n",
    "        [ 689,328,393,1327,393,480,999,116,224,1124,949,708, # UHSLC\n",
    "          1296,378,1124,780,613,488,1442,1217,578,200,1177,1025,689,949,224,1146, # LINZ\n",
    "          1174,1260,1217,744,1064,1214,803,999 # OTHER (ports...)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Predictor configurations to iterate through\n",
    "    pca_attrs_exp = {\n",
    "        'calculate_gradient': [False,True],\n",
    "        'winds': [False,True],\n",
    "        'time_lapse': [1,2,3], # 1 equals to NO time delay \n",
    "        'time_resample': ['6H','12H','1D'], # 6H and 12H available...\n",
    "        'region': [('local',(1.5,1.5)),('local',(2.5,2.5)),(True,default_region_reduced)]\n",
    "    }\n",
    "    \n",
    "    # Labels to use for the different predictor domains\n",
    "    region_labels = ['local_1.5_1.5', 'local_2.5_2.5', 'default_region_reduced']\n",
    "    \n",
    "    # Configuration of the knn regression algorithm to iterate through\n",
    "    knn_attrs_exp = {\n",
    "        'train_size': [0.7],\n",
    "        'percentage_PCs': [0.98],\n",
    "        'k_neighbors': [None if k_neighbors == 0 else k_neighbors] # None calculates the optimum k-neighs\n",
    "    }\n",
    "    \n",
    "    predictand = load_moana_hindcast(plot=False)\n",
    "    \n",
    "    # create the experiment\n",
    "    experiment = Experiment(None,\n",
    "                            None,\n",
    "                            predictand, # all the sites are passed to exp at first\n",
    "                            sites_to_analyze=sites_to_analyze, \n",
    "                            model='knn', # model that will be used to predict\n",
    "                            pca_attrs=pca_attrs_exp,\n",
    "                            model_attrs=knn_attrs_exp,\n",
    "                            pcs_folder='/home/metocean/pcs/')\n",
    "    \n",
    "    # Run the experiment\n",
    "    exp_parameters, exp_mean_parameters, stats_keys =\\\n",
    "        experiment.execute_cross_model_calculations(verbose=True,\n",
    "                                                    plot=False # plot logs when computing the models\n",
    "                                                    )\n",
    "    # Pack results\n",
    "    coords_dict = {'site': experiment.ss_sites,\n",
    "                   'grad': pca_attrs_exp['calculate_gradient'],\n",
    "                   'winds': pca_attrs_exp['winds'],\n",
    "                   'tlapse': pca_attrs_exp['time_lapse'],\n",
    "                   'tresample': pca_attrs_exp['time_resample'],\n",
    "                   'region': region_labels,\n",
    "                   'tsize': knn_attrs_exp['train_size'],\n",
    "                   'perpcs': knn_attrs_exp['percentage_PCs'],\n",
    "                   'k_neighbors': [k_neighbors]}\n",
    "    \n",
    "    site_metrics = {}\n",
    "    for im,metric in enumerate(stats_keys):\n",
    "        site_metrics[metric] = (\n",
    "            ('site', 'grad','winds','tlapse','tresample','region','tsize','perpcs','k_neighbors'),\n",
    "            np.concatenate([np.expand_dims(exp_param[:,:,:,:,:,:,:,:,im],0)\n",
    "                             for exp_param in exp_parameters], 0))\n",
    "    experiment_metrics = xr.Dataset(site_metrics).assign(coords_dict)\n",
    "\n",
    "    return dill.dumps(experiment_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4140d9",
   "metadata": {},
   "source": [
    "Run the knn experiments for number of neibours ranging from 0 to 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047eb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(8) as p:\n",
    "    all_results = p.map(run_knn_experiment, range(0, 51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = xr.concat([dill.loads(r) for r in all_results],\n",
    "                         dim='k_neighbors')\n",
    "knn_results.to_netcdf('knn_results.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ssnz]",
   "language": "python",
   "name": "conda-env-.conda-ssnz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
