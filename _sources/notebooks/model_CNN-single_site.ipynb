{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad90411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PATH /data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basics\n",
    "import os, sys\n",
    "\n",
    "# arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import (\n",
    "    datetime,\n",
    "    timedelta\n",
    ")\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# append sscode to path\n",
    "sys.path.insert(0, '/home/metocean/geocean-nz-ss')\n",
    "data_path = '/data' #'/data/storm_surge_data/'\n",
    "os.environ[\"SSURGE_DATA_PATH\"] = data_path\n",
    "\n",
    "# custom\n",
    "from sscode.config import data_path, default_region_reduced, default_evaluation_metrics, default_region\n",
    "from sscode.utils import (\n",
    "    calculate_relative_winds,\n",
    "    spatial_gradient\n",
    ")\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# this is to allow plots to be centered\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5791daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset attrs\n",
    "datasets_attrs = {\n",
    "    'era5': ('longitude','latitude',None,'ERA 5 reanalysis','u10','v10'),\n",
    "    #'cfsr': ('lon','lat',None,'CFSR reanalysis','U_GRD_L103','V_GRD_L103'),\n",
    "     'cfsr': ('longitude','latitude',None,'CFSR reanalysis','ugrd10m','vgrd10m'),\n",
    "    'dac': ('longitude','latitude',None,'DAC global reanalysis'),\n",
    "    'moana': ('lon','lat','site','Moana v2 hindcast'),\n",
    "    'codec': ('codec_coords_lon','codec_coords_lat','name','CoDEC reanalysis'),\n",
    "    'uhslc': ('longitude','latitude','name','UHSLC tgs'),\n",
    "    'linz': ('longitude','latitude','name','LINZ tgs'),\n",
    "    'other': ('longitude','latitude','name','OTHER tgs'),\n",
    "    'privtgs': ('longitude','latitude','name','Private tgs')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1ffe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_dset = xr.open_zarr(os.path.join(data_path, 'storm_surge_data/moana_hindcast_v2/moana_coast.zarr/'))\n",
    "ss_dset = ss_dset.isel(site=np.logical_and(ss_dset.lat>-50, ss_dset.lon < 180))\n",
    "predictand = ss_dset.ss\\\n",
    "                    .sel(time=slice(datetime(1994,12,1), datetime(2017,2,1,1)))\\\n",
    "                    .drop_duplicates('time')\\\n",
    "                    .chunk(dict(time=-1))\\\n",
    "                    .interpolate_na(dim='time')\\\n",
    "                    .transpose()\\\n",
    "                    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c78ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_predictor_for_site(location,\n",
    "                                dx=2.5,\n",
    "                                region=None,\n",
    "                                normalised=True):\n",
    "    \n",
    "    from sscode.utils import spatial_gradient\n",
    "    \n",
    "    pres_vars = ('SLP','longitude','latitude')\n",
    "    wind_vars = ('wind_proj_mask','longitude','latitude','U_GRD_L103','V_GRD_L103')\n",
    "    \n",
    "    if region is None:\n",
    "        region = (\n",
    "                  location[0]-dx,\n",
    "                  location[0]+dx,\n",
    "                  location[1]-dx,\n",
    "                  location[1]+dx\n",
    "                )\n",
    "    \n",
    "    region_large = (\n",
    "              region[0]-1,\n",
    "              region[1]+1,\n",
    "              region[2]-1,\n",
    "              region[3]+1\n",
    "            )\n",
    "\n",
    "    print(\"Loading U\")\n",
    "    uw = xr.open_dataset(os.path.join(data_path, 'cfsr',\n",
    "                                      'wnd10m/cfsr_wnd_1979_2021.nc'))[datasets_attrs['cfsr'][4]]\\\n",
    "                    .sel(time=slice(datetime(1994,11,1), datetime(2017,3,1)))\\\n",
    "                    .sel({\n",
    "                      wind_vars[1]:slice(region_large[0],region_large[1]),\n",
    "                      wind_vars[2]:slice(region_large[2],region_large[3])\n",
    "                    })\\\n",
    "                    .sortby(datasets_attrs['cfsr'][0],ascending=True)\\\n",
    "                    .sortby(datasets_attrs['cfsr'][1],ascending=True)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Loading V\")\n",
    "    vw = xr.open_dataset(os.path.join(data_path, 'cfsr',\n",
    "                                                  'wnd10m/cfsr_wnd_1979_2021.nc'))[datasets_attrs['cfsr'][5]]\\\n",
    "                    .sel(time=slice(datetime(1994,11,1), datetime(2017,3,1)))\\\n",
    "                    .sel({\n",
    "                      wind_vars[1]:slice(region_large[0],region_large[1]),\n",
    "                      wind_vars[2]:slice(region_large[2],region_large[3])\n",
    "                    })\\\n",
    "                    .sortby(datasets_attrs['cfsr'][0],ascending=True)\\\n",
    "                    .sortby(datasets_attrs['cfsr'][1],ascending=True)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Calculating relative winds\")\n",
    "    wind = calculate_relative_winds(location=location, # load_winds[1],\n",
    "                                    lat_name=datasets_attrs['cfsr'][1],\n",
    "                                    lon_name=datasets_attrs['cfsr'][0],\n",
    "                                    uw=uw,vw=vw)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Clearing U,V\")\n",
    "    del uw\n",
    "    del vw\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"Loading MSLP\")\n",
    "    pres = xr.open_dataarray(os.path.join(data_path, 'cfsr',\n",
    "                                          'CFSR_MSLP_1H_1990_2021.nc'))\\\n",
    "             .sel(time=slice(datetime(1994,11,1), datetime(2017,3,1)))\\\n",
    "             .sel({\n",
    "                    pres_vars[1]:slice(region[0],region[1]),\n",
    "                    pres_vars[2]:slice(region[2],region[3])\n",
    "                    })\\\n",
    "             .sortby('longitude',ascending=True)\\\n",
    "             .sortby('latitude',ascending=True)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    if pres_vars[0]=='wind_proj' or pres_vars[0]=='wind_proj_mask': # when just winds are loaded                                                                               \n",
    "        pres = pres.fillna(0.0)\n",
    "    else:\n",
    "        pres = pres.dropna(dim='time',how='all')\n",
    "    \n",
    "    wind = wind[wind_vars[0]].fillna(0.0)\\\n",
    "                             .interp(coords={wind_vars[1]:pres[pres_vars[1]],\n",
    "                                             wind_vars[2]:pres[pres_vars[2]]})\\\n",
    "                             .sel(time=pres.time) # interp to pressure coords                            \n",
    "    \n",
    "    # calculate the gradient                                                            \n",
    "    print('\\n calculating the gradient of the sea-level-pressure fields... \\n')\n",
    "    pres = spatial_gradient(pres,pres_vars[0]) # from utils.py                      \n",
    "    print('\\n pressure/gradient predictor both with shape: \\n {} \\n'\\\n",
    "            .format(pres[pres_vars[0]].shape))\n",
    "        \n",
    "    # Normalising\n",
    "    if normalised:\n",
    "        all_predictors =\\\n",
    "           xr.concat([\n",
    "                      (pres.SLP.expand_dims(\"channel\", -1)-pres.SLP.min())/(pres.SLP.max()-pres.SLP.min()),\n",
    "                      (pres.SLP_gradient.expand_dims(\"channel\", -1)-pres.SLP_gradient.min())/(pres.SLP_gradient.max()-pres.SLP.min()),\n",
    "                      (wind.expand_dims(\"channel\", -1)-wind.min())/(wind.max()-wind.min())\n",
    "                     ],\n",
    "                     \"channel\").fillna(0)\n",
    "    else:\n",
    "        all_predictors =\\\n",
    "           xr.concat([\n",
    "                      pres.SLP.expand_dims(\"channel\", -1),\n",
    "                      pres.SLP_gradient.expand_dims(\"channel\", -1),\n",
    "                      wind.expand_dims(\"channel\", -1)\n",
    "                     ],\n",
    "                     \"channel\").fillna(0)\n",
    "    \n",
    "    return all_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c75ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2 as tfv2\n",
    "# pylint: disable=g-classes-have-attributes\n",
    "\n",
    "# These functions are adapted from\n",
    "# https://github.com/keras-team/keras/blob/06ba37b8662dea768b3bc8201942f1eb877708e8/keras/preprocessing/timeseries.py\n",
    "# The main addition is that they targets have been modified to contain both the input grid and the ss output\n",
    "# That way it is possible to use the dataset to train both heads of the network\n",
    "    \n",
    "def sequences_from_indices(array, indices_ds, start_index, end_index):\n",
    "  dataset = tfv2.data.Dataset.from_tensors(array[start_index : end_index])\n",
    "  dataset = tfv2.data.Dataset.zip((dataset.repeat(), indices_ds)).map(\n",
    "      lambda steps, inds: tfv2.gather(steps, inds),  # pylint: disable=unnecessary-lambda\n",
    "      num_parallel_calls=tfv2.data.AUTOTUNE)\n",
    "  return dataset\n",
    "\n",
    "def timeseries_dataset_from_array_seb(\n",
    "    data,\n",
    "    targets,\n",
    "    targets_2,\n",
    "    sequence_length,\n",
    "    weights=None,\n",
    "    sequence_stride=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    start_index=None,\n",
    "    end_index=None):\n",
    "  \"\"\"Creates a dataset of sliding windows over a timeseries provided as array.\n",
    "  This function takes in a sequence of data-points gathered at\n",
    "  equal intervals, along with time series parameters such as\n",
    "  length of the sequences/windows, spacing between two sequence/windows, etc.,\n",
    "  to produce batches of timeseries inputs and targets.\n",
    "  Args:\n",
    "    data: Numpy array or eager tensor\n",
    "      containing consecutive data points (timesteps).\n",
    "      Axis 0 is expected to be the time dimension.\n",
    "    targets: Targets corresponding to timesteps in `data`.\n",
    "      `targets[i]` should be the target\n",
    "      corresponding to the window that starts at index `i`\n",
    "      (see example 2 below).\n",
    "      Pass None if you don't have target data (in this case the dataset will\n",
    "      only yield the input data).\n",
    "    sequence_length: Length of the output sequences (in number of timesteps).\n",
    "    sequence_stride: Period between successive output sequences.\n",
    "      For stride `s`, output samples would\n",
    "      start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.\n",
    "    sampling_rate: Period between successive individual timesteps\n",
    "      within sequences. For rate `r`, timesteps\n",
    "      `data[i], data[i + r], ... data[i + sequence_length]`\n",
    "      are used for create a sample sequence.\n",
    "    batch_size: Number of timeseries samples in each batch\n",
    "      (except maybe the last one).\n",
    "    shuffle: Whether to shuffle output samples,\n",
    "      or instead draw them in chronological order.\n",
    "    seed: Optional int; random seed for shuffling.\n",
    "    start_index: Optional int; data points earlier (exclusive)\n",
    "      than `start_index` will not be used\n",
    "      in the output sequences. This is useful to reserve part of the\n",
    "      data for test or validation.\n",
    "    end_index: Optional int; data points later (exclusive) than `end_index`\n",
    "      will not be used in the output sequences.\n",
    "      This is useful to reserve part of the data for test or validation.\n",
    "  Returns:\n",
    "    A tfv2.data.Dataset instance. If `targets` was passed, the dataset yields\n",
    "    tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields\n",
    "    only `batch_of_sequences`.\n",
    "  Example 1:\n",
    "  Consider indices `[0, 1, ... 99]`.\n",
    "  With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,\n",
    "  `shuffle=False`, the dataset will yield batches of sequences\n",
    "  composed of the following indices:\n",
    "  ```\n",
    "  First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
    "  Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
    "  Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
    "  ...\n",
    "  Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
    "  ```\n",
    "  In this case the last 3 data points are discarded since no full sequence\n",
    "  can be generated to include them (the next sequence would have started\n",
    "  at index 81, and thus its last step would have gone over 99).\n",
    "  Example 2: Temporal regression.\n",
    "  Consider an array `data` of scalar values, of shape `(steps,)`.\n",
    "  To generate a dataset that uses the past 10\n",
    "  timesteps to predict the next timestep, you would use:\n",
    "  ```python\n",
    "  input_data = data[:-10]\n",
    "  targets = data[10:]\n",
    "  dataset = tfv2.keras.preprocessing.timeseries_dataset_from_array(\n",
    "      input_data, targets, sequence_length=10)\n",
    "  for batch in dataset:\n",
    "    inputs, targets = batch\n",
    "    assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]\n",
    "    assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10\n",
    "    break\n",
    "  ```\n",
    "  Example 3: Temporal regression for many-to-many architectures.\n",
    "  Consider two arrays of scalar values `X` and `Y`,\n",
    "  both of shape `(100,)`. The resulting dataset should consist samples with\n",
    "  20 timestamps each. The samples should not overlap.\n",
    "  To generate a dataset that uses the current timestamp\n",
    "  to predict the corresponding target timestep, you would use:\n",
    "  ```python\n",
    "  X = np.arange(100)\n",
    "  Y = X*2\n",
    "  sample_length = 20\n",
    "  input_dataset = tfv2.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    X, None, sequence_length=sample_length, sequence_stride=sample_length)\n",
    "  target_dataset = tfv2.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    Y, None, sequence_length=sample_length, sequence_stride=sample_length)\n",
    "  for batch in zip(input_dataset, target_dataset):\n",
    "    inputs, targets = batch\n",
    "    assert np.array_equal(inputs[0], X[:sample_length])\n",
    "    # second sample equals output timestamps 20-40\n",
    "    assert np.array_equal(targets[1], Y[sample_length:2*sample_length])\n",
    "    break\n",
    "  ```\n",
    "  \"\"\"\n",
    "  if start_index:\n",
    "    if start_index < 0:\n",
    "      raise ValueError(f'`start_index` must be 0 or greater. Received: '\n",
    "                       f'start_index={start_index}')\n",
    "    if start_index >= len(data):\n",
    "      raise ValueError(f'`start_index` must be lower than the length of the '\n",
    "                       f'data. Received: start_index={start_index}, for data '\n",
    "                       f'of length {len(data)}')\n",
    "  if end_index:\n",
    "    if start_index and end_index <= start_index:\n",
    "      raise ValueError(f'`end_index` must be higher than `start_index`. '\n",
    "                       f'Received: start_index={start_index}, and '\n",
    "                       f'end_index={end_index} ')\n",
    "    if end_index >= len(data):\n",
    "      raise ValueError(f'`end_index` must be lower than the length of the '\n",
    "                       f'data. Received: end_index={end_index}, for data of '\n",
    "                       f'length {len(data)}')\n",
    "    if end_index <= 0:\n",
    "      raise ValueError('`end_index` must be higher than 0. '\n",
    "                       f'Received: end_index={end_index}')\n",
    "\n",
    "  # Validate strides\n",
    "  if sampling_rate <= 0:\n",
    "    raise ValueError(f'`sampling_rate` must be higher than 0. Received: '\n",
    "                     f'sampling_rate={sampling_rate}')\n",
    "  if sampling_rate >= len(data):\n",
    "    raise ValueError(f'`sampling_rate` must be lower than the length of the '\n",
    "                     f'data. Received: sampling_rate={sampling_rate}, for data '\n",
    "                     f'of length {len(data)}')\n",
    "  if sequence_stride <= 0:\n",
    "    raise ValueError(f'`sequence_stride` must be higher than 0. Received: '\n",
    "                     f'sequence_stride={sequence_stride}')\n",
    "  if sequence_stride >= len(data):\n",
    "    raise ValueError(f'`sequence_stride` must be lower than the length of the '\n",
    "                     f'data. Received: sequence_stride={sequence_stride}, for '\n",
    "                     f'data of length {len(data)}')\n",
    "\n",
    "  if start_index is None:\n",
    "    start_index = 0\n",
    "  if end_index is None:\n",
    "    end_index = len(data)\n",
    "\n",
    "  # Determine the lowest dtype to store start positions (to lower memory usage).\n",
    "  num_seqs = end_index - start_index - (sequence_length * sampling_rate) + 1\n",
    "  if targets is not None:\n",
    "    num_seqs = min(num_seqs, len(targets))\n",
    "  if num_seqs < 2147483647:\n",
    "    index_dtype = 'int32'\n",
    "  else:\n",
    "    index_dtype = 'int64'\n",
    "\n",
    "  # Generate start positions\n",
    "  start_positions = np.arange(0, num_seqs, sequence_stride, dtype=index_dtype)\n",
    "  if shuffle:\n",
    "    if seed is None:\n",
    "      seed = np.random.randint(1e6)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(start_positions)\n",
    "\n",
    "  sequence_length = tfv2.cast(sequence_length, dtype=index_dtype)\n",
    "  sampling_rate = tfv2.cast(sampling_rate, dtype=index_dtype)\n",
    "\n",
    "  positions_ds = tfv2.data.Dataset.from_tensors(start_positions).repeat()\n",
    "\n",
    "  # For each initial window position, generates indices of the window elements\n",
    "  indices = tfv2.data.Dataset.zip(\n",
    "      (tfv2.data.Dataset.range(len(start_positions)), positions_ds)).map(\n",
    "          lambda i, positions: tfv2.range(  # pylint: disable=g-long-lambda\n",
    "              positions[i],\n",
    "              positions[i] + sequence_length * sampling_rate,\n",
    "              sampling_rate),\n",
    "          num_parallel_calls=tfv2.data.AUTOTUNE)\n",
    "\n",
    "  dataset = sequences_from_indices(data, indices, start_index, end_index)\n",
    "  outputs = [dataset]\n",
    "  if targets is not None:\n",
    "    indices = tfv2.data.Dataset.zip(\n",
    "        (tfv2.data.Dataset.range(len(start_positions)), positions_ds)).map(\n",
    "            lambda i, positions: positions[i],\n",
    "            num_parallel_calls=tfv2.data.AUTOTUNE)\n",
    "    print(targets.__class__)\n",
    "    target_ds = sequences_from_indices(\n",
    "        targets, indices, start_index, end_index)\n",
    "    \n",
    "    if targets_2 is not None:\n",
    "        target_ds_2 = sequences_from_indices(\n",
    "            targets_2, indices, start_index, end_index)\n",
    "        outputs.append((target_ds, target_ds_2))\n",
    "    else:\n",
    "    #outputs.append((target_ds, dataset))\n",
    "        outputs.append(target_ds)\n",
    "    ##dataset = tfv2.data.Dataset.zip((dataset, (target_ds, dataset)))\n",
    "    \n",
    "    #outputs.append((target_ds, dataset))\n",
    "    #outputs.append(target_ds)\n",
    "\n",
    "  if weights is not None:\n",
    "    indices = tfv2.data.Dataset.zip(\n",
    "        (tfv2.data.Dataset.range(len(start_positions)), positions_ds)).map(\n",
    "            lambda i, positions: positions[i],\n",
    "            num_parallel_calls=tfv2.data.AUTOTUNE)\n",
    "    print(weights.__class__)\n",
    "    target_weights_dset = sequences_from_indices(\n",
    "            weights, indices, start_index, end_index)\n",
    "    \n",
    "    indices = tfv2.data.Dataset.zip(\n",
    "      (tfv2.data.Dataset.range(len(start_positions)), positions_ds)).map(\n",
    "          lambda i, positions: tfv2.range(  # pylint: disable=g-long-lambda\n",
    "              positions[i],\n",
    "              positions[i] + sequence_length * sampling_rate,\n",
    "              sampling_rate),\n",
    "          num_parallel_calls=tfv2.data.AUTOTUNE)\n",
    "\n",
    "    ae_weights_dset = sequences_from_indices(np.ones(data.shape[:-1]+(1,)), indices, start_index, end_index)\n",
    "    outputs.append((target_weights_dset, ae_weights_dset))\n",
    "    #dataset = tfv2.data.Dataset.zip((dataset, (target_ds, dataset)))\n",
    "  dataset = tfv2.data.Dataset.zip(tuple(outputs))\n",
    "  if shuffle:\n",
    "    # Shuffle locally at each iteration\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "  dataset = dataset.prefetch(tfv2.data.AUTOTUNE).batch(batch_size)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4850a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sscode.validation import generate_stats\n",
    "from sscode.config import (\n",
    "    default_evaluation_metrics,\n",
    "    default_ext_quantile\n",
    ")\n",
    "\n",
    "def calculate_stats(ts1, ts2):\n",
    "    title, stats = generate_stats(ts1,\n",
    "                              ts2,\n",
    "                              metrics=default_evaluation_metrics,\n",
    "                              ext_quantile=default_ext_quantile)\n",
    "    for metric in ['bias', 'si',\n",
    "                   'rmse', 'kgeprime', 'rmse_95', 'rmse_99',\n",
    "                   'pearson', 'pearson_95', 'pearson_99',\n",
    "                   'rscore', 'rscore_95', 'rscore_99',\n",
    "                   'nse', 'nse_95', 'nse_99',\n",
    "                   'kge', 'ext_kge_95', 'ext_kge_99']:\n",
    "        print(metric, stats[metric])\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8d0318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Conv3D,\n",
    "    BatchNormalization,\n",
    "    MaxPool2D,\n",
    "    MaxPool3D,\n",
    "    ConvLSTM2D,\n",
    "    GlobalMaxPool2D,\n",
    "    Flatten, Dropout,\n",
    "    TimeDistributed,\n",
    "    GRU,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Conv1D,\n",
    "    LSTM,\n",
    "    Conv2DTranspose,\n",
    "    Reshape,\n",
    "    Cropping2D,\n",
    "    Cropping1D,\n",
    "    Activation,\n",
    "    Lambda,\n",
    "    Concatenate,\n",
    "    TimeDistributed,\n",
    "    Flatten,\n",
    "    Reshape\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64735d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one works!!!\n",
    "def build_model_cnn(shape_in,\n",
    "                    nbout=1):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=shape_in)\n",
    "    \n",
    "    conv_1 = TimeDistributed(Conv2D(24, (3,3), padding='same', activation='relu'))(inputs)\n",
    "    cnn_outputs = TimeDistributed(MaxPool2D((2,2)))(conv_1)     \n",
    "    \n",
    "    flat = Flatten()(cnn_outputs)\n",
    "\n",
    "    dense_1 = Dense(48,\n",
    "                    activation='relu',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                    )(flat)\n",
    "    \n",
    "    dropout_1 = Dropout(0.1)(dense_1)\n",
    "    \n",
    "    out = Dense(nbout,\n",
    "                activation='linear',\n",
    "                use_bias=True)(dropout_1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b913d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "to_use=1\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "try:\n",
    "    tf.config.set_visible_devices(gpus[to_use], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print(\"Failed to select GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "66c793de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_and_validation_sets(predictors,\n",
    "                                     predictand,\n",
    "                                     input_sequence_length,\n",
    "                                     input_sequence_frequency,\n",
    "                                     lead_time,\n",
    "                                     target_sequence_frequency,\n",
    "                                     fold=0,\n",
    "                                     n_folds=5,\n",
    "                                     batch_size=32,\n",
    "                                     print_test=False):\n",
    "    \n",
    "    if ( not (predictand.time.values[1:]-predictand.time.values[:-1]).max() ==\\\n",
    "        (predictand.time.values[1:]-predictand.time.values[:-1]).min() ):\n",
    "        print(\"Time spacing is not constant in predictand dataset\")\n",
    "        raise\n",
    "    \n",
    "    if ( not (predictors.time.values[1:]-predictors.time.values[:-1]).max() ==\\\n",
    "        (predictors.time.values[1:]-predictors.time.values[:-1]).min() ):\n",
    "            print(\"Time spacing is not constant in predictor dataset\")\n",
    "            raise\n",
    "    \n",
    "    # Find start, end and extent of dataset\n",
    "    [tstart_predictor, tend_predictor] = predictors.time.values[[0,-1]].astype('datetime64[s]').tolist()\n",
    "    [tstart_predictand, tend_predictand] = predictand.time.values[[0,-1]].astype('datetime64[s]').tolist()\n",
    "    tstart = max(tstart_predictand, tstart_predictor)\n",
    "    tend = min(tend_predictand, tend_predictor)\n",
    "    time_extent = tend - tstart\n",
    "\n",
    "    print(\"Returning fold \",fold,\" of \", n_folds, \" e.g. %2.1f percent training data\"%((n_folds-1)/n_folds*100.))\n",
    "\n",
    "    # Finding time bounds of training data segments\n",
    "    tstart_1 = tstart\n",
    "    tend_1 = (tstart + time_extent*((n_folds-1-fold)/n_folds)).replace(second=0, microsecond=0, minute=0)\n",
    "\n",
    "    tstart_2 = (tstart + time_extent*((n_folds-fold)/n_folds)).replace(second=0, microsecond=0, minute=0)\n",
    "    tend_2 = tend\n",
    "    print(\"\")\n",
    "    \n",
    "    train_dset = []\n",
    "    for tstart_train, tend_train in zip([tstart_1, tstart_2],[tend_1, tend_2]):\n",
    "        if tstart_train != tend_train:\n",
    "            print(tstart_train, tend_train)\n",
    "\n",
    "            predictor_train =\\\n",
    "                predictors.sel(time=slice(tstart_train,\n",
    "                                          tend_train-timedelta(hours=(lead_time-target_sequence_frequency))))\\\n",
    "                          .rolling(time=input_sequence_frequency).mean()\\\n",
    "                          .shift(time=-(input_sequence_frequency-1))\\\n",
    "                          .isel(time=slice(0,-(input_sequence_frequency-1)))\\\n",
    "                          .fillna(0)\n",
    "\n",
    "            ss_train = predictand.sel(time=slice(tstart_train+timedelta(hours=input_sequence_length+lead_time-target_sequence_frequency),\n",
    "                                      tend_train))\\\n",
    "                                 .chunk(dict(time=-1))\\\n",
    "                                 .interpolate_na(dim='time')\\\n",
    "                                 .rolling(time=target_sequence_frequency).max()\\\n",
    "                                 .shift(time=-(target_sequence_frequency-1))\\\n",
    "                                 .isel(time=slice(0,-(target_sequence_frequency-1)))\\\n",
    "                                 .expand_dims(\"channel\", -1)*100\n",
    "\n",
    "            if print_test: # To check indices are fine\n",
    "                test_dset = timeseries_dataset_from_array_seb(\n",
    "                    (predictor_train.time.values.astype(np.int)-786412800000000000)//3600000000000,\n",
    "                    (ss_train.time.values.astype(np.int)-786412800000000000)//3600000000000,\n",
    "                    targets_2=None,\n",
    "                    weights=None,\n",
    "                    sequence_length=int(input_sequence_length/input_sequence_frequency),\n",
    "                    sequence_stride=int(target_sequence_frequency/input_sequence_frequency),\n",
    "                    sampling_rate=input_sequence_frequency,\n",
    "                    batch_size=4, shuffle=True, seed=None, start_index=None, end_index=None\n",
    "                    )\n",
    "        \n",
    "                print(\"All integer correspond to number of hours with respect to reference date\")\n",
    "                for batch in test_dset:\n",
    "                    inputs, targets = batch\n",
    "                    for v1, v2 in zip(inputs, targets):\n",
    "                        print(\"Times in:\", v1, \"Times out:\", v2)\n",
    "                    print_test = False\n",
    "                    break\n",
    "                print(\"\")\n",
    "\n",
    "            train_dset.append(timeseries_dataset_from_array_seb(\n",
    "                    predictor_train,\n",
    "                    ss_train,\n",
    "                    targets_2=None,\n",
    "                weights=None,\n",
    "                sequence_length=int(input_sequence_length/input_sequence_frequency),\n",
    "                sequence_stride=int(target_sequence_frequency/input_sequence_frequency),\n",
    "                sampling_rate=input_sequence_frequency,\n",
    "                batch_size=batch_size, shuffle=True, seed=None, start_index=None, end_index=None\n",
    "                ))\n",
    "            \n",
    "    # If multiple segments concatenate them into a single dataset\n",
    "    if len(train_dset) == 1:\n",
    "        train_dset = train_dset[0]\n",
    "    else:\n",
    "        assert len(train_dset) == 2\n",
    "        train_dset = train_dset[0].concatenate(train_dset[1])\n",
    "\n",
    "\n",
    "    # Validation data\n",
    "    predictor_val = predictors.sel(time=slice(tend_1,\n",
    "                                              tstart_2-timedelta(hours=(lead_time-target_sequence_frequency))))\\\n",
    "                              .rolling(time=input_sequence_frequency).mean()\\\n",
    "                              .shift(time=-(input_sequence_frequency-1))\\\n",
    "                              .isel(time=slice(0,-(input_sequence_frequency-1)))\\\n",
    "                              .fillna(0)\n",
    "\n",
    "    ss_val = predictand.sel(time=slice(tend_1+timedelta(hours=input_sequence_length+lead_time-target_sequence_frequency),\n",
    "                                       tstart_2))\\\n",
    "                       .chunk(dict(time=-1))\\\n",
    "                       .interpolate_na(dim='time')\\\n",
    "                       .rolling(time=target_sequence_frequency).max()\\\n",
    "                       .shift(time=-(target_sequence_frequency-1))\\\n",
    "                       .isel(time=slice(0,-(target_sequence_frequency-1)))\\\n",
    "                       .expand_dims(\"channel\", -1)*100\n",
    "\n",
    "\n",
    "    val_dset = timeseries_dataset_from_array_seb(\n",
    "        predictor_val,\n",
    "        ss_val,\n",
    "        targets_2=None,\n",
    "        weights=None,\n",
    "        sequence_length=int(input_sequence_length/input_sequence_frequency),\n",
    "        sequence_stride=int(target_sequence_frequency/input_sequence_frequency),\n",
    "        sampling_rate=input_sequence_frequency,\n",
    "        batch_size=batch_size, shuffle=False, seed=None, start_index=None, end_index=None\n",
    "    )\n",
    "\n",
    "    return train_dset, val_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1047a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -50.11 -49.8 -49.49 ... -43.56 -43.24\n",
      "  * longitude       (longitude) float32 165.0 165.3 165.6 ... 170.9 171.2 171.6\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -9.86 -9.789 ... -2.205\n",
      "    vgrd10m         (time, latitude, longitude) float32 11.66 11.62 ... 2.572\n",
      "    uw2             (time, latitude, longitude) float32 97.21 95.83 ... 4.861\n",
      "    vw2             (time, latitude, longitude) float32 135.9 135.0 ... 6.617\n",
      "    wind_magnitude  (time, latitude, longitude) float32 15.27 15.19 ... 3.388\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([119787 119811 119835], shape=(3,), dtype=int64) Times out: tf.Tensor(119835, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([127035 127059 127083], shape=(3,), dtype=int64) Times out: tf.Tensor(127083, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([81373 81397 81421], shape=(3,), dtype=int64) Times out: tf.Tensor(81421, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([97244 97268 97292], shape=(3,), dtype=int64) Times out: tf.Tensor(97292, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_253\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_254 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_506 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_507 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_253 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_506 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_253 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_507 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 51.1239 - mse: 51.0650 - mae: 5.5192 - val_loss: 36.9328 - val_mse: 36.8598 - val_mae: 4.8413\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 39.6420 - mse: 39.5596 - mae: 4.9129 - val_loss: 35.6100 - val_mse: 35.5196 - val_mae: 4.7599\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.6498 - mse: 38.5524 - mae: 4.8520 - val_loss: 35.1465 - val_mse: 35.0425 - val_mae: 4.7257\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.2969 - mse: 38.1876 - mae: 4.8255 - val_loss: 34.8703 - val_mse: 34.7554 - val_mae: 4.7053\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.7381 - mse: 37.6180 - mae: 4.7915 - val_loss: 34.5444 - val_mse: 34.4189 - val_mae: 4.6911\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 9s 2ms/step - loss: 37.5160 - mse: 37.3855 - mae: 4.7751 - val_loss: 34.5018 - val_mse: 34.3661 - val_mae: 4.6925\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 37.3241 - mse: 37.1834 - mae: 4.7615 - val_loss: 34.1152 - val_mse: 33.9697 - val_mae: 4.6666\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 36.8714 - mse: 36.7206 - mae: 4.7374 - val_loss: 34.4347 - val_mse: 34.2790 - val_mae: 4.6901\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 36.5764 - mse: 36.4161 - mae: 4.7153 - val_loss: 35.1946 - val_mse: 35.0297 - val_mae: 4.7414\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 12s 2ms/step - loss: 36.3419 - mse: 36.1726 - mae: 4.6975 - val_loss: 34.1961 - val_mse: 34.0229 - val_mae: 4.6800\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 12s 2ms/step - loss: 36.0694 - mse: 35.8927 - mae: 4.6839 - val_loss: 35.7373 - val_mse: 35.5569 - val_mae: 4.7831\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 9s 2ms/step - loss: 35.9318 - mse: 35.7479 - mae: 4.6754 - val_loss: 34.2336 - val_mse: 34.0466 - val_mae: 4.6844\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 12s 2ms/step - loss: 35.5747 - mse: 35.3847 - mae: 4.6468 - val_loss: 34.7408 - val_mse: 34.5479 - val_mae: 4.7187\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 35.5102 - mse: 35.3144 - mae: 4.6430 - val_loss: 34.6903 - val_mse: 34.4919 - val_mae: 4.7160\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 35.4207 - mse: 35.2199 - mae: 4.6321 - val_loss: 34.6018 - val_mse: 34.3986 - val_mae: 4.7106\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 12s 2ms/step - loss: 34.9937 - mse: 34.7889 - mae: 4.6092 - val_loss: 34.1244 - val_mse: 33.9179 - val_mae: 4.6772\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 12s 2ms/step - loss: 35.0886 - mse: 34.8801 - mae: 4.6149 - val_loss: 34.1419 - val_mse: 33.9315 - val_mae: 4.6776\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 12s 2ms/step - loss: 34.8675 - mse: 34.6554 - mae: 4.6007 - val_loss: 33.6123 - val_mse: 33.3987 - val_mae: 4.6363\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 34.6063 - mse: 34.3916 - mae: 4.5815 - val_loss: 33.7704 - val_mse: 33.5542 - val_mae: 4.6505\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 34.3471 - mse: 34.1300 - mae: 4.5606 - val_loss: 33.2230 - val_mse: 33.0047 - val_mae: 4.6119\n",
      "bias -0.012828387\n",
      "si 0.48975155\n",
      "rmse 0.057449713\n",
      "kgeprime [0.53193962]\n",
      "rmse_95 0.0777748\n",
      "rmse_99 0.09913381\n",
      "pearson 0.848115183006452\n",
      "pearson_95 0.6958499914095382\n",
      "pearson_99 0.7086672561791206\n",
      "rscore 0.7042894573040572\n",
      "rscore_95 -1.0448784778618356\n",
      "rscore_99 -9.856662702087696\n",
      "nse [0.70428946]\n",
      "nse_95 [-1.04487848]\n",
      "nse_99 [-9.8566627]\n",
      "kge [0.64294887]\n",
      "ext_kge_95 [0.60871329]\n",
      "ext_kge_99 [-0.19349587]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([25860 25884 25908], shape=(3,), dtype=int64) Times out: tf.Tensor(25908, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([2205 2229 2253], shape=(3,), dtype=int64) Times out: tf.Tensor(2253, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([71341 71365 71389], shape=(3,), dtype=int64) Times out: tf.Tensor(71389, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([53354 53378 53402], shape=(3,), dtype=int64) Times out: tf.Tensor(53402, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_254\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_255 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_508 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_509 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_254 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_508 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_254 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_509 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 50.1068 - mse: 50.0477 - mae: 5.4760 - val_loss: 39.5802 - val_mse: 39.5098 - val_mae: 4.9657\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.5805 - mse: 38.5020 - mae: 4.8617 - val_loss: 37.8939 - val_mse: 37.8067 - val_mae: 4.8536\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.3281 - mse: 37.2339 - mae: 4.7736 - val_loss: 37.1518 - val_mse: 37.0503 - val_mae: 4.7951\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.7407 - mse: 36.6331 - mae: 4.7331 - val_loss: 36.7433 - val_mse: 36.6296 - val_mae: 4.7623\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.3999 - mse: 36.2810 - mae: 4.7119 - val_loss: 37.0393 - val_mse: 36.9157 - val_mae: 4.7732\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.9153 - mse: 35.7862 - mae: 4.6805 - val_loss: 36.7856 - val_mse: 36.6517 - val_mae: 4.7603\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.5296 - mse: 35.3900 - mae: 4.6489 - val_loss: 35.9727 - val_mse: 35.8286 - val_mae: 4.7049\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 10s 2ms/step - loss: 34.7924 - mse: 34.6431 - mae: 4.6040 - val_loss: 35.3097 - val_mse: 35.1569 - val_mae: 4.6591\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 34.3433 - mse: 34.1857 - mae: 4.5735 - val_loss: 34.3980 - val_mse: 34.2375 - val_mae: 4.6004\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 34.0461 - mse: 33.8814 - mae: 4.5491 - val_loss: 34.3086 - val_mse: 34.1416 - val_mae: 4.5916\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 33.5625 - mse: 33.3917 - mae: 4.5131 - val_loss: 33.8567 - val_mse: 33.6835 - val_mae: 4.5603\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 33.2367 - mse: 33.0593 - mae: 4.4917 - val_loss: 33.4962 - val_mse: 33.3167 - val_mae: 4.5340\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 33.0959 - mse: 32.9130 - mae: 4.4758 - val_loss: 33.5662 - val_mse: 33.3817 - val_mae: 4.5325\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 32.7274 - mse: 32.5398 - mae: 4.4549 - val_loss: 32.9034 - val_mse: 32.7142 - val_mae: 4.4895\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 13s 3ms/step - loss: 32.6067 - mse: 32.4148 - mae: 4.4470 - val_loss: 32.4881 - val_mse: 32.2948 - val_mae: 4.4682\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 9s 2ms/step - loss: 32.3060 - mse: 32.1100 - mae: 4.4224 - val_loss: 32.9922 - val_mse: 32.7949 - val_mae: 4.4916\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 32.2955 - mse: 32.0958 - mae: 4.4233 - val_loss: 32.5589 - val_mse: 32.3575 - val_mae: 4.4639\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 32.1411 - mse: 31.9379 - mae: 4.4114 - val_loss: 32.2431 - val_mse: 32.0388 - val_mae: 4.4405\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 32.0097 - mse: 31.8036 - mae: 4.4018 - val_loss: 32.5399 - val_mse: 32.3327 - val_mae: 4.4582\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 32.0485 - mse: 31.8390 - mae: 4.4039 - val_loss: 32.5531 - val_mse: 32.3427 - val_mae: 4.4589\n",
      "bias 0.0003243396\n",
      "si 0.4709503\n",
      "rmse 0.05687064\n",
      "kgeprime [0.74837427]\n",
      "rmse_95 0.083566666\n",
      "rmse_99 0.08912001\n",
      "pearson 0.8690262843176926\n",
      "pearson_95 0.5139396821609185\n",
      "pearson_99 0.17517287581961338\n",
      "rscore 0.7471416030904823\n",
      "rscore_95 -4.057378425888483\n",
      "rscore_99 -17.046814495298133\n",
      "nse [0.7471416]\n",
      "nse_95 [-4.05737843]\n",
      "nse_99 [-17.0468145]\n",
      "kge [0.74321952]\n",
      "ext_kge_95 [0.06497173]\n",
      "ext_kge_99 [-0.9379045]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([68495 68519 68543], shape=(3,), dtype=int64) Times out: tf.Tensor(68543, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([27124 27148 27172], shape=(3,), dtype=int64) Times out: tf.Tensor(27172, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([58291 58315 58339], shape=(3,), dtype=int64) Times out: tf.Tensor(58339, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([15443 15467 15491], shape=(3,), dtype=int64) Times out: tf.Tensor(15491, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_255\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_256 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_510 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_511 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_255 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_510 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_255 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_511 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 10s 2ms/step - loss: 53.8704 - mse: 53.8174 - mae: 5.7109 - val_loss: 37.5125 - val_mse: 37.4468 - val_mae: 4.7226\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 40.0296 - mse: 39.9587 - mae: 4.9690 - val_loss: 35.8804 - val_mse: 35.8041 - val_mae: 4.6099\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 38.9160 - mse: 38.8367 - mae: 4.8968 - val_loss: 35.7338 - val_mse: 35.6497 - val_mae: 4.5912\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 38.6690 - mse: 38.5820 - mae: 4.8772 - val_loss: 35.2386 - val_mse: 35.1479 - val_mae: 4.5657\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 38.3990 - mse: 38.3052 - mae: 4.8582 - val_loss: 35.1101 - val_mse: 35.0126 - val_mae: 4.5523\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 38.0964 - mse: 37.9962 - mae: 4.8379 - val_loss: 35.2502 - val_mse: 35.1463 - val_mae: 4.5559\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 37.9485 - mse: 37.8417 - mae: 4.8337 - val_loss: 35.1677 - val_mse: 35.0571 - val_mae: 4.5522\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 37.7268 - mse: 37.6131 - mae: 4.8192 - val_loss: 34.6922 - val_mse: 34.5747 - val_mae: 4.5278\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 37.3809 - mse: 37.2603 - mae: 4.7943 - val_loss: 34.6338 - val_mse: 34.5094 - val_mae: 4.5244\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 37.2897 - mse: 37.1622 - mae: 4.7884 - val_loss: 34.4087 - val_mse: 34.2774 - val_mae: 4.5056\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 37.0989 - mse: 36.9647 - mae: 4.7750 - val_loss: 34.7258 - val_mse: 34.5881 - val_mae: 4.5195\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 10s 2ms/step - loss: 36.9225 - mse: 36.7819 - mae: 4.7611 - val_loss: 34.4581 - val_mse: 34.3142 - val_mae: 4.5026\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 12s 3ms/step - loss: 36.7424 - mse: 36.5961 - mae: 4.7491 - val_loss: 34.4542 - val_mse: 34.3047 - val_mae: 4.4976\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 13s 3ms/step - loss: 36.3476 - mse: 36.1953 - mae: 4.7247 - val_loss: 33.5487 - val_mse: 33.3931 - val_mae: 4.4427\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 12s 3ms/step - loss: 36.1282 - mse: 35.9700 - mae: 4.7075 - val_loss: 33.1661 - val_mse: 33.0048 - val_mae: 4.4168\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 12s 3ms/step - loss: 35.8398 - mse: 35.6764 - mae: 4.6847 - val_loss: 32.4633 - val_mse: 32.2975 - val_mae: 4.3711\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 12s 2ms/step - loss: 35.5148 - mse: 35.3475 - mae: 4.6681 - val_loss: 32.3761 - val_mse: 32.2069 - val_mae: 4.3644\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 12s 2ms/step - loss: 35.2685 - mse: 35.0976 - mae: 4.6510 - val_loss: 32.6636 - val_mse: 32.4911 - val_mae: 4.3821\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 13s 3ms/step - loss: 35.2208 - mse: 35.0471 - mae: 4.6396 - val_loss: 31.7964 - val_mse: 31.6216 - val_mae: 4.3181\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 12s 2ms/step - loss: 35.0322 - mse: 34.8562 - mae: 4.6324 - val_loss: 31.5820 - val_mse: 31.4047 - val_mae: 4.3044\n",
      "bias 0.0037284107\n",
      "si 0.47071767\n",
      "rmse 0.05603989\n",
      "kgeprime [0.79769894]\n",
      "rmse_95 0.11709115\n",
      "rmse_99 0.21082576\n",
      "pearson 0.8652860728277744\n",
      "pearson_95 0.16752426359350425\n",
      "pearson_99 -0.2444489950795243\n",
      "rscore 0.7433998721564699\n",
      "rscore_95 -1.9978625637075198\n",
      "rscore_99 -6.262203010873385\n",
      "nse [0.74339987]\n",
      "nse_95 [-1.99786256]\n",
      "nse_99 [-6.26220301]\n",
      "kge [0.74439394]\n",
      "ext_kge_95 [0.10376966]\n",
      "ext_kge_99 [-0.31495991]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([ 9988 10012 10036], shape=(3,), dtype=int64) Times out: tf.Tensor(10036, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([14472 14496 14520], shape=(3,), dtype=int64) Times out: tf.Tensor(14520, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([37146 37170 37194], shape=(3,), dtype=int64) Times out: tf.Tensor(37194, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([38629 38653 38677], shape=(3,), dtype=int64) Times out: tf.Tensor(38677, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_256\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_257 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_512 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_513 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_256 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_512 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_256 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_513 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 13s 3ms/step - loss: 53.3961 - mse: 53.3523 - mae: 5.6642 - val_loss: 37.3595 - val_mse: 37.3064 - val_mae: 4.7539\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 40.5974 - mse: 40.5401 - mae: 4.9899 - val_loss: 36.1805 - val_mse: 36.1184 - val_mae: 4.6757\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 39.4660 - mse: 39.4007 - mae: 4.9121 - val_loss: 36.1941 - val_mse: 36.1250 - val_mae: 4.6791\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 11s 2ms/step - loss: 39.0548 - mse: 38.9828 - mae: 4.8918 - val_loss: 35.4298 - val_mse: 35.3537 - val_mae: 4.6300\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.9855 - mse: 38.9063 - mae: 4.8815 - val_loss: 35.4634 - val_mse: 35.3801 - val_mae: 4.6285\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.6684 - mse: 38.5814 - mae: 4.8588 - val_loss: 35.3781 - val_mse: 35.2867 - val_mae: 4.6330\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.3706 - mse: 38.2754 - mae: 4.8437 - val_loss: 35.3558 - val_mse: 35.2559 - val_mae: 4.6257\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.1581 - mse: 38.0540 - mae: 4.8332 - val_loss: 35.0278 - val_mse: 34.9189 - val_mae: 4.6038\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.9334 - mse: 37.8203 - mae: 4.8163 - val_loss: 35.0923 - val_mse: 34.9744 - val_mae: 4.6022\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.5529 - mse: 37.4306 - mae: 4.7914 - val_loss: 34.3659 - val_mse: 34.2389 - val_mae: 4.5504\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.4907 - mse: 37.3597 - mae: 4.7865 - val_loss: 34.2604 - val_mse: 34.1250 - val_mae: 4.5524\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.2098 - mse: 37.0700 - mae: 4.7685 - val_loss: 34.0369 - val_mse: 33.8925 - val_mae: 4.5461\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.1423 - mse: 36.9941 - mae: 4.7627 - val_loss: 34.0248 - val_mse: 33.8726 - val_mae: 4.5416\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.7634 - mse: 36.6073 - mae: 4.7398 - val_loss: 33.5893 - val_mse: 33.4294 - val_mae: 4.5141\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 10s 2ms/step - loss: 36.7425 - mse: 36.5786 - mae: 4.7347 - val_loss: 33.4538 - val_mse: 33.2863 - val_mae: 4.4938\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 36.4776 - mse: 36.3063 - mae: 4.7102 - val_loss: 33.0967 - val_mse: 32.9218 - val_mae: 4.4665\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 36.2254 - mse: 36.0471 - mae: 4.6990 - val_loss: 32.4824 - val_mse: 32.3011 - val_mae: 4.4250\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 35.8663 - mse: 35.6819 - mae: 4.6704 - val_loss: 32.3135 - val_mse: 32.1264 - val_mae: 4.4197\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 35.4623 - mse: 35.2727 - mae: 4.6405 - val_loss: 32.5431 - val_mse: 32.3518 - val_mae: 4.4516\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 13s 3ms/step - loss: 35.2429 - mse: 35.0491 - mae: 4.6287 - val_loss: 32.1386 - val_mse: 31.9432 - val_mae: 4.4198\n",
      "bias 0.009248525\n",
      "si 0.48798564\n",
      "rmse 0.056518342\n",
      "kgeprime [0.71679296]\n",
      "rmse_95 0.09571484\n",
      "rmse_99 0.10896053\n",
      "pearson 0.8522727823640246\n",
      "pearson_95 0.6027665753110135\n",
      "pearson_99 0.5184291386065779\n",
      "rscore 0.7187797857486767\n",
      "rscore_95 -2.177985179644043\n",
      "rscore_99 -7.169626673111191\n",
      "nse [0.71877979]\n",
      "nse_95 [-2.17798518]\n",
      "nse_99 [-7.16962667]\n",
      "kge [0.68951903]\n",
      "ext_kge_95 [0.43605505]\n",
      "ext_kge_99 [-0.11137088]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([134697 134721 134745], shape=(3,), dtype=int64) Times out: tf.Tensor(134745, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([182155 182179 182203], shape=(3,), dtype=int64) Times out: tf.Tensor(182203, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([82455 82479 82503], shape=(3,), dtype=int64) Times out: tf.Tensor(82503, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([151989 152013 152037], shape=(3,), dtype=int64) Times out: tf.Tensor(152037, shape=(), dtype=int64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_257\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_258 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_514 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_515 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_257 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_514 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_257 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_515 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 14s 3ms/step - loss: 56.2804 - mse: 56.2338 - mae: 5.7782 - val_loss: 36.2174 - val_mse: 36.1610 - val_mae: 4.7293\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 41.3729 - mse: 41.3108 - mae: 5.0311 - val_loss: 34.1141 - val_mse: 34.0468 - val_mae: 4.5778\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 39.9789 - mse: 39.9071 - mae: 4.9438 - val_loss: 33.5804 - val_mse: 33.5043 - val_mae: 4.5339\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 39.5650 - mse: 39.4856 - mae: 4.9173 - val_loss: 33.3246 - val_mse: 33.2416 - val_mae: 4.5164\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 39.1687 - mse: 39.0825 - mae: 4.8867 - val_loss: 32.9475 - val_mse: 32.8586 - val_mae: 4.4953\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 39.2075 - mse: 39.1159 - mae: 4.8932 - val_loss: 32.8618 - val_mse: 32.7672 - val_mae: 4.4894\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 11s 2ms/step - loss: 38.6866 - mse: 38.5894 - mae: 4.8574 - val_loss: 32.4734 - val_mse: 32.3735 - val_mae: 4.4630\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.5513 - mse: 38.4492 - mae: 4.8495 - val_loss: 32.2570 - val_mse: 32.1523 - val_mae: 4.4432\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.3259 - mse: 38.2189 - mae: 4.8422 - val_loss: 32.3825 - val_mse: 32.2731 - val_mae: 4.4533\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.3073 - mse: 38.1955 - mae: 4.8344 - val_loss: 32.0315 - val_mse: 31.9173 - val_mae: 4.4235\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.2155 - mse: 38.0990 - mae: 4.8300 - val_loss: 31.8955 - val_mse: 31.7766 - val_mae: 4.4152\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.0641 - mse: 37.9431 - mae: 4.8172 - val_loss: 31.8595 - val_mse: 31.7362 - val_mae: 4.4122\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.8649 - mse: 37.7392 - mae: 4.8025 - val_loss: 32.0530 - val_mse: 31.9249 - val_mae: 4.4283\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.7387 - mse: 37.6083 - mae: 4.7927 - val_loss: 31.3633 - val_mse: 31.2305 - val_mae: 4.3730\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.5422 - mse: 37.4074 - mae: 4.7860 - val_loss: 31.2334 - val_mse: 31.0962 - val_mae: 4.3610\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.4173 - mse: 37.2778 - mae: 4.7755 - val_loss: 30.9766 - val_mse: 30.8347 - val_mae: 4.3422\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.1368 - mse: 36.9926 - mae: 4.7558 - val_loss: 30.4667 - val_mse: 30.3202 - val_mae: 4.3028\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 10s 2ms/step - loss: 36.8029 - mse: 36.6547 - mae: 4.7354 - val_loss: 30.2950 - val_mse: 30.1447 - val_mae: 4.2916\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 36.6067 - mse: 36.4546 - mae: 4.7214 - val_loss: 29.7722 - val_mse: 29.6184 - val_mae: 4.2533\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 36.3270 - mse: 36.1719 - mae: 4.7009 - val_loss: 29.8026 - val_mse: 29.6462 - val_mae: 4.2542\n",
      "bias 0.0039751614\n",
      "si 0.4693175\n",
      "rmse 0.054448348\n",
      "kgeprime [0.8021737]\n",
      "rmse_95 0.08910656\n",
      "rmse_99 0.11739118\n",
      "pearson 0.8643396181923318\n",
      "pearson_95 0.5496398954831371\n",
      "pearson_99 0.41523521091579724\n",
      "rscore 0.7424751824238571\n",
      "rscore_95 -2.3316744418759336\n",
      "rscore_99 -3.302179584012457\n",
      "nse [0.74247518]\n",
      "nse_95 [-2.33167444]\n",
      "nse_99 [-3.30217958]\n",
      "kge [0.74721921]\n",
      "ext_kge_95 [0.47215048]\n",
      "ext_kge_99 [0.29261173]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -49.49 -49.18 -48.86 ... -42.93 -42.62\n",
      "  * longitude       (longitude) float32 163.1 163.4 163.8 ... 169.4 169.7 170.0\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -10.61 -10.4 ... -4.54\n",
      "    vgrd10m         (time, latitude, longitude) float32 10.44 10.63 ... 0.07264\n",
      "    uw2             (time, latitude, longitude) float32 112.6 108.1 ... 20.61\n",
      "    vw2             (time, latitude, longitude) float32 109.0 113.0 ... 0.005276\n",
      "    wind_magnitude  (time, latitude, longitude) float32 14.88 14.87 ... 4.54\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([53516 53540 53564], shape=(3,), dtype=int64) Times out: tf.Tensor(53564, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([134701 134725 134749], shape=(3,), dtype=int64) Times out: tf.Tensor(134749, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([26382 26406 26430], shape=(3,), dtype=int64) Times out: tf.Tensor(26430, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34133 34157 34181], shape=(3,), dtype=int64) Times out: tf.Tensor(34181, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_258\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_259 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_516 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_517 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_258 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_516 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_258 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_517 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 43.2141 - mse: 43.1664 - mae: 5.0643 - val_loss: 27.2060 - val_mse: 27.1493 - val_mae: 4.1810\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 31.4710 - mse: 31.4118 - mae: 4.3799 - val_loss: 26.2510 - val_mse: 26.1898 - val_mae: 4.1133\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.6537 - mse: 30.5904 - mae: 4.3199 - val_loss: 25.4965 - val_mse: 25.4310 - val_mae: 4.0545\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.3571 - mse: 30.2898 - mae: 4.2964 - val_loss: 25.4019 - val_mse: 25.3333 - val_mae: 4.0459\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.0425 - mse: 29.9727 - mae: 4.2739 - val_loss: 24.9439 - val_mse: 24.8732 - val_mae: 4.0026\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.9871 - mse: 29.9155 - mae: 4.2660 - val_loss: 25.1202 - val_mse: 25.0477 - val_mae: 4.0216\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.8446 - mse: 29.7714 - mae: 4.2545 - val_loss: 24.9111 - val_mse: 24.8369 - val_mae: 4.0082\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.7270 - mse: 29.6521 - mae: 4.2451 - val_loss: 24.9292 - val_mse: 24.8533 - val_mae: 4.0096\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 9s 2ms/step - loss: 29.4793 - mse: 29.4028 - mae: 4.2261 - val_loss: 24.4896 - val_mse: 24.4120 - val_mae: 3.9675\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 29.2281 - mse: 29.1496 - mae: 4.2071 - val_loss: 24.2624 - val_mse: 24.1828 - val_mae: 3.9495\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 29.0123 - mse: 28.9317 - mae: 4.1908 - val_loss: 24.3077 - val_mse: 24.2261 - val_mae: 3.9578\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.8060 - mse: 28.7235 - mae: 4.1720 - val_loss: 23.9386 - val_mse: 23.8551 - val_mae: 3.9264\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.6288 - mse: 28.5443 - mae: 4.1576 - val_loss: 23.5875 - val_mse: 23.5022 - val_mae: 3.8956\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.4434 - mse: 28.3573 - mae: 4.1443 - val_loss: 23.1946 - val_mse: 23.1077 - val_mae: 3.8599\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.3421 - mse: 28.2545 - mae: 4.1398 - val_loss: 23.1205 - val_mse: 23.0324 - val_mae: 3.8553\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.2694 - mse: 28.1806 - mae: 4.1272 - val_loss: 22.9981 - val_mse: 22.9089 - val_mae: 3.8444\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.1587 - mse: 28.0686 - mae: 4.1234 - val_loss: 22.9886 - val_mse: 22.8980 - val_mae: 3.8472\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.0162 - mse: 27.9250 - mae: 4.1053 - val_loss: 22.8795 - val_mse: 22.7874 - val_mae: 3.8363\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 28.0545 - mse: 27.9619 - mae: 4.1086 - val_loss: 23.0621 - val_mse: 22.9688 - val_mae: 3.8570\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 12s 3ms/step - loss: 27.9049 - mse: 27.8110 - mae: 4.1004 - val_loss: 22.7340 - val_mse: 22.6394 - val_mae: 3.8220\n",
      "bias -0.0050381855\n",
      "si 0.47463933\n",
      "rmse 0.047580894\n",
      "kgeprime [0.66648569]\n",
      "rmse_95 0.063250236\n",
      "rmse_99 0.073814206\n",
      "pearson 0.8610595699863389\n",
      "pearson_95 0.6214049089070305\n",
      "pearson_99 0.4895707539131495\n",
      "rscore 0.7376725464204983\n",
      "rscore_95 -1.7953555529924108\n",
      "rscore_99 -9.70841559764021\n",
      "nse [0.73767255]\n",
      "nse_95 [-1.79535555]\n",
      "nse_99 [-9.7084156]\n",
      "kge [0.74162906]\n",
      "ext_kge_95 [0.46318088]\n",
      "ext_kge_99 [-0.21566854]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([64217 64241 64265], shape=(3,), dtype=int64) Times out: tf.Tensor(64265, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([6071 6095 6119], shape=(3,), dtype=int64) Times out: tf.Tensor(6119, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([10333 10357 10381], shape=(3,), dtype=int64) Times out: tf.Tensor(10381, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([38423 38447 38471], shape=(3,), dtype=int64) Times out: tf.Tensor(38471, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_259\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_260 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_518 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_519 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_259 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_518 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_259 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_519 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 13s 3ms/step - loss: 43.4731 - mse: 43.4251 - mae: 5.0985 - val_loss: 32.3602 - val_mse: 32.3044 - val_mae: 4.5218\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 10s 2ms/step - loss: 32.2581 - mse: 32.1996 - mae: 4.4257 - val_loss: 27.9250 - val_mse: 27.8635 - val_mae: 4.2100\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 30.6245 - mse: 30.5590 - mae: 4.3071 - val_loss: 26.9252 - val_mse: 26.8560 - val_mae: 4.1355\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 30.0795 - mse: 30.0062 - mae: 4.2687 - val_loss: 26.7390 - val_mse: 26.6622 - val_mae: 4.1188\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 29.5753 - mse: 29.4946 - mae: 4.2326 - val_loss: 26.2928 - val_mse: 26.2088 - val_mae: 4.0845\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 29.4310 - mse: 29.3434 - mae: 4.2190 - val_loss: 26.7106 - val_mse: 26.6197 - val_mae: 4.1138\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 29.1742 - mse: 29.0801 - mae: 4.1982 - val_loss: 26.5975 - val_mse: 26.5008 - val_mae: 4.1034\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.8752 - mse: 28.7752 - mae: 4.1766 - val_loss: 25.6704 - val_mse: 25.5678 - val_mae: 4.0315\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.5835 - mse: 28.4778 - mae: 4.1571 - val_loss: 26.4342 - val_mse: 26.3262 - val_mae: 4.0918\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.2724 - mse: 28.1614 - mae: 4.1348 - val_loss: 25.8990 - val_mse: 25.7859 - val_mae: 4.0497\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.2592 - mse: 28.1433 - mae: 4.1305 - val_loss: 25.3417 - val_mse: 25.2238 - val_mae: 4.0077\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.8811 - mse: 27.7606 - mae: 4.1023 - val_loss: 24.8836 - val_mse: 24.7613 - val_mae: 3.9727\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 9s 2ms/step - loss: 27.8670 - mse: 27.7424 - mae: 4.0975 - val_loss: 24.6589 - val_mse: 24.5329 - val_mae: 3.9551\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 12s 2ms/step - loss: 27.7460 - mse: 27.6180 - mae: 4.0895 - val_loss: 24.3435 - val_mse: 24.2146 - val_mae: 3.9311\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 27.4490 - mse: 27.3183 - mae: 4.0726 - val_loss: 25.7517 - val_mse: 25.6201 - val_mae: 4.0393\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 27.3123 - mse: 27.1788 - mae: 4.0585 - val_loss: 24.4187 - val_mse: 24.2845 - val_mae: 3.9347\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 27.3599 - mse: 27.2241 - mae: 4.0575 - val_loss: 24.9033 - val_mse: 24.7669 - val_mae: 3.9728\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 27.0867 - mse: 26.9489 - mae: 4.0386 - val_loss: 25.0913 - val_mse: 24.9530 - val_mae: 3.9868\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 27.0350 - mse: 26.8953 - mae: 4.0381 - val_loss: 25.8671 - val_mse: 25.7268 - val_mae: 4.0502\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 12s 3ms/step - loss: 27.1036 - mse: 26.9620 - mae: 4.0386 - val_loss: 25.3004 - val_mse: 25.1585 - val_mae: 4.0032\n",
      "bias 0.0050788308\n",
      "si 0.47098517\n",
      "rmse 0.05015821\n",
      "kgeprime [0.76380997]\n",
      "rmse_95 0.07891216\n",
      "rmse_99 0.088689335\n",
      "pearson 0.8792802530418489\n",
      "pearson_95 0.4683351033483487\n",
      "pearson_99 0.2418570832210684\n",
      "rscore 0.7493355952241654\n",
      "rscore_95 -5.293507559250654\n",
      "rscore_99 -7.1892502346570435\n",
      "nse [0.7493356]\n",
      "nse_95 [-5.29350756]\n",
      "nse_99 [-7.18925023]\n",
      "kge [0.67250062]\n",
      "ext_kge_95 [0.12891517]\n",
      "ext_kge_99 [0.00406731]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([26644 26668 26692], shape=(3,), dtype=int64) Times out: tf.Tensor(26692, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([31013 31037 31061], shape=(3,), dtype=int64) Times out: tf.Tensor(31061, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([75105 75129 75153], shape=(3,), dtype=int64) Times out: tf.Tensor(75153, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([37693 37717 37741], shape=(3,), dtype=int64) Times out: tf.Tensor(37741, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_260\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_261 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_520 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_521 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_260 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_520 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_521 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 13s 3ms/step - loss: 40.1922 - mse: 40.1410 - mae: 4.9475 - val_loss: 29.8701 - val_mse: 29.8117 - val_mae: 4.1653\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 12s 3ms/step - loss: 30.6949 - mse: 30.6313 - mae: 4.3697 - val_loss: 29.0176 - val_mse: 28.9464 - val_mae: 4.0836\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 12s 3ms/step - loss: 29.4304 - mse: 29.3543 - mae: 4.2783 - val_loss: 28.0323 - val_mse: 27.9511 - val_mae: 3.9968\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 13s 3ms/step - loss: 28.8598 - mse: 28.7764 - mae: 4.2346 - val_loss: 27.9465 - val_mse: 27.8598 - val_mae: 3.9827\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 12s 2ms/step - loss: 28.5858 - mse: 28.4977 - mae: 4.2142 - val_loss: 27.5012 - val_mse: 27.4106 - val_mae: 3.9576\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 28.4269 - mse: 28.3355 - mae: 4.2030 - val_loss: 27.4508 - val_mse: 27.3575 - val_mae: 3.9520\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 28.1760 - mse: 28.0815 - mae: 4.1848 - val_loss: 26.9735 - val_mse: 26.8771 - val_mae: 3.9067\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 27.7370 - mse: 27.6395 - mae: 4.1520 - val_loss: 26.4251 - val_mse: 26.3254 - val_mae: 3.8635\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 27.4191 - mse: 27.3178 - mae: 4.1266 - val_loss: 26.0592 - val_mse: 25.9560 - val_mae: 3.8262\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 27.1282 - mse: 27.0238 - mae: 4.1003 - val_loss: 25.7405 - val_mse: 25.6344 - val_mae: 3.8048\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 27.1027 - mse: 26.9955 - mae: 4.0945 - val_loss: 25.3431 - val_mse: 25.2344 - val_mae: 3.7701\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.8639 - mse: 26.7543 - mae: 4.0798 - val_loss: 25.1256 - val_mse: 25.0149 - val_mae: 3.7551\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.7364 - mse: 26.6247 - mae: 4.0730 - val_loss: 25.1694 - val_mse: 25.0564 - val_mae: 3.7615\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.6172 - mse: 26.5035 - mae: 4.0585 - val_loss: 24.9717 - val_mse: 24.8570 - val_mae: 3.7488\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.6136 - mse: 26.4981 - mae: 4.0559 - val_loss: 24.6719 - val_mse: 24.5556 - val_mae: 3.7217\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.4818 - mse: 26.3647 - mae: 4.0480 - val_loss: 24.8485 - val_mse: 24.7307 - val_mae: 3.7404\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.3285 - mse: 26.2100 - mae: 4.0321 - val_loss: 24.6719 - val_mse: 24.5527 - val_mae: 3.7242\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.2641 - mse: 26.1441 - mae: 4.0311 - val_loss: 24.7174 - val_mse: 24.5968 - val_mae: 3.7310\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.2138 - mse: 26.0927 - mae: 4.0247 - val_loss: 25.1041 - val_mse: 24.9826 - val_mae: 3.7635\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.2469 - mse: 26.1246 - mae: 4.0274 - val_loss: 24.6259 - val_mse: 24.5029 - val_mae: 3.7265\n",
      "bias -0.0032373848\n",
      "si 0.4759828\n",
      "rmse 0.049500443\n",
      "kgeprime [0.68522395]\n",
      "rmse_95 0.10049339\n",
      "rmse_99 0.19170758\n",
      "pearson 0.8658565421895805\n",
      "pearson_95 0.016841904297019428\n",
      "pearson_99 -0.5833668905948702\n",
      "rscore 0.7442576256834283\n",
      "rscore_95 -1.9370604035379353\n",
      "rscore_99 -6.054257456114536\n",
      "nse [0.74425763]\n",
      "nse_95 [-1.9370604]\n",
      "nse_99 [-6.05425746]\n",
      "kge [0.74139781]\n",
      "ext_kge_95 [-0.02140406]\n",
      "ext_kge_99 [-0.63728886]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([17236 17260 17284], shape=(3,), dtype=int64) Times out: tf.Tensor(17284, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([9725 9749 9773], shape=(3,), dtype=int64) Times out: tf.Tensor(9773, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34053 34077 34101], shape=(3,), dtype=int64) Times out: tf.Tensor(34101, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([12532 12556 12580], shape=(3,), dtype=int64) Times out: tf.Tensor(12580, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_261\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_262 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_522 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_523 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_261 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_522 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_523 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 39.1026 - mse: 39.0465 - mae: 4.8681 - val_loss: 29.3744 - val_mse: 29.3069 - val_mae: 4.1988\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 30.4443 - mse: 30.3713 - mae: 4.3436 - val_loss: 28.4995 - val_mse: 28.4205 - val_mae: 4.1277\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 29.2022 - mse: 29.1190 - mae: 4.2504 - val_loss: 28.0639 - val_mse: 27.9760 - val_mae: 4.0907\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.6828 - mse: 28.5915 - mae: 4.2059 - val_loss: 27.7721 - val_mse: 27.6776 - val_mae: 4.0668\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.1661 - mse: 28.0689 - mae: 4.1676 - val_loss: 27.2767 - val_mse: 27.1771 - val_mae: 4.0262\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 28.0407 - mse: 27.9391 - mae: 4.1513 - val_loss: 27.1407 - val_mse: 27.0369 - val_mae: 4.0233\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.5645 - mse: 27.4585 - mae: 4.1195 - val_loss: 26.2808 - val_mse: 26.1728 - val_mae: 3.9493\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.1669 - mse: 27.0567 - mae: 4.0843 - val_loss: 26.6927 - val_mse: 26.5806 - val_mae: 3.9958\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.8966 - mse: 26.7825 - mae: 4.0641 - val_loss: 25.8309 - val_mse: 25.7150 - val_mae: 3.9248\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.6769 - mse: 26.5593 - mae: 4.0431 - val_loss: 25.4034 - val_mse: 25.2847 - val_mae: 3.8866\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.5156 - mse: 26.3952 - mae: 4.0319 - val_loss: 25.1129 - val_mse: 24.9911 - val_mae: 3.8645\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.3637 - mse: 26.2401 - mae: 4.0225 - val_loss: 24.9946 - val_mse: 24.8696 - val_mae: 3.8642\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.1789 - mse: 26.0524 - mae: 4.0087 - val_loss: 25.0821 - val_mse: 24.9544 - val_mae: 3.8590\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.0285 - mse: 25.8992 - mae: 3.9913 - val_loss: 24.7882 - val_mse: 24.6576 - val_mae: 3.8407\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.9474 - mse: 25.8155 - mae: 3.9880 - val_loss: 25.1040 - val_mse: 24.9712 - val_mae: 3.8694\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.8239 - mse: 25.6896 - mae: 3.9755 - val_loss: 24.8393 - val_mse: 24.7041 - val_mae: 3.8477\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.8154 - mse: 25.6789 - mae: 3.9760 - val_loss: 25.2977 - val_mse: 25.1604 - val_mae: 3.8851\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.6427 - mse: 25.5042 - mae: 3.9595 - val_loss: 25.0430 - val_mse: 24.9036 - val_mae: 3.8651\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.7326 - mse: 25.5920 - mae: 3.9677 - val_loss: 24.6184 - val_mse: 24.4771 - val_mae: 3.8235\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.5553 - mse: 25.4127 - mae: 3.9514 - val_loss: 24.6056 - val_mse: 24.4622 - val_mae: 3.8232\n",
      "bias 0.0005429639\n",
      "si 0.49063188\n",
      "rmse 0.049459245\n",
      "kgeprime [0.78709306]\n",
      "rmse_95 0.07952307\n",
      "rmse_99 0.09218851\n",
      "pearson 0.8553287510638022\n",
      "pearson_95 0.512058530216252\n",
      "pearson_99 0.2503461500497808\n",
      "rscore 0.7309460011599551\n",
      "rscore_95 -2.416617056046594\n",
      "rscore_99 -3.4559425342943744\n",
      "nse [0.730946]\n",
      "nse_95 [-2.41661706]\n",
      "nse_99 [-3.45594253]\n",
      "kge [0.7766535]\n",
      "ext_kge_95 [0.37597134]\n",
      "ext_kge_99 [0.21007828]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([110641 110665 110689], shape=(3,), dtype=int64) Times out: tf.Tensor(110689, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([143113 143137 143161], shape=(3,), dtype=int64) Times out: tf.Tensor(143161, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([45614 45638 45662], shape=(3,), dtype=int64) Times out: tf.Tensor(45662, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([148123 148147 148171], shape=(3,), dtype=int64) Times out: tf.Tensor(148171, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_262\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_263 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_524 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_525 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_262 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_524 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_525 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 43.6107 - mse: 43.5621 - mae: 5.0912 - val_loss: 27.0491 - val_mse: 26.9858 - val_mae: 4.1686\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.7315 - mse: 31.6597 - mae: 4.3903 - val_loss: 25.8073 - val_mse: 25.7280 - val_mae: 4.0667\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.7904 - mse: 30.7058 - mae: 4.3268 - val_loss: 25.0823 - val_mse: 24.9935 - val_mae: 4.0019\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.4594 - mse: 30.3672 - mae: 4.2969 - val_loss: 24.5358 - val_mse: 24.4405 - val_mae: 3.9440\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.1241 - mse: 30.0261 - mae: 4.2737 - val_loss: 24.2450 - val_mse: 24.1443 - val_mae: 3.9153\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.8800 - mse: 29.7769 - mae: 4.2552 - val_loss: 24.2609 - val_mse: 24.1551 - val_mae: 3.9125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.6479 - mse: 29.5395 - mae: 4.2407 - val_loss: 23.9994 - val_mse: 23.8882 - val_mae: 3.8964\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.5541 - mse: 29.4404 - mae: 4.2296 - val_loss: 23.7375 - val_mse: 23.6215 - val_mae: 3.8687\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.3422 - mse: 29.2236 - mae: 4.2139 - val_loss: 23.5956 - val_mse: 23.4745 - val_mae: 3.8594\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.1464 - mse: 29.0229 - mae: 4.1974 - val_loss: 23.2769 - val_mse: 23.1510 - val_mae: 3.8289\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.0458 - mse: 28.9180 - mae: 4.1900 - val_loss: 23.0078 - val_mse: 22.8778 - val_mae: 3.8034\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.8768 - mse: 28.7452 - mae: 4.1744 - val_loss: 22.9484 - val_mse: 22.8149 - val_mae: 3.8013\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.7735 - mse: 28.6387 - mae: 4.1704 - val_loss: 22.9532 - val_mse: 22.8167 - val_mae: 3.8032\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.6161 - mse: 28.4783 - mae: 4.1564 - val_loss: 22.6434 - val_mse: 22.5041 - val_mae: 3.7751\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.5140 - mse: 28.3737 - mae: 4.1459 - val_loss: 22.3591 - val_mse: 22.2176 - val_mae: 3.7516\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.5629 - mse: 28.4204 - mae: 4.1471 - val_loss: 22.5020 - val_mse: 22.3586 - val_mae: 3.7652\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.4183 - mse: 28.2740 - mae: 4.1380 - val_loss: 22.3025 - val_mse: 22.1572 - val_mae: 3.7523\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.3262 - mse: 28.1800 - mae: 4.1309 - val_loss: 22.0550 - val_mse: 21.9077 - val_mae: 3.7212\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.2056 - mse: 28.0578 - mae: 4.1208 - val_loss: 21.8883 - val_mse: 21.7395 - val_mae: 3.7105\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.1964 - mse: 28.0469 - mae: 4.1252 - val_loss: 21.7437 - val_mse: 21.5934 - val_mae: 3.6968\n",
      "bias 0.00042006798\n",
      "si 0.46405146\n",
      "rmse 0.04646873\n",
      "kgeprime [0.78265295]\n",
      "rmse_95 0.07014092\n",
      "rmse_99 0.068211496\n",
      "pearson 0.8706505732483492\n",
      "pearson_95 0.5529906743930073\n",
      "pearson_99 0.4706341031637561\n",
      "rscore 0.755002740407845\n",
      "rscore_95 -5.749610850370261\n",
      "rscore_99 -15.499001821231115\n",
      "nse [0.75500274]\n",
      "nse_95 [-5.74961085]\n",
      "nse_99 [-15.49900182]\n",
      "kge [0.77458922]\n",
      "ext_kge_95 [0.2006254]\n",
      "ext_kge_99 [-0.28768195]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -49.18 -48.86 -48.55 ... -42.93 -42.62\n",
      "  * longitude       (longitude) float32 167.5 167.8 168.1 ... 173.4 173.8 174.1\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -8.61 -8.42 ... 4.049\n",
      "    vgrd10m         (time, latitude, longitude) float32 9.659 9.499 ... 2.632\n",
      "    uw2             (time, latitude, longitude) float32 74.13 70.89 ... 16.39\n",
      "    vw2             (time, latitude, longitude) float32 93.29 90.22 ... 6.928\n",
      "    wind_magnitude  (time, latitude, longitude) float32 12.94 12.69 ... 4.829\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([12086 12110 12134], shape=(3,), dtype=int64) Times out: tf.Tensor(12134, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([64666 64690 64714], shape=(3,), dtype=int64) Times out: tf.Tensor(64714, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([98179 98203 98227], shape=(3,), dtype=int64) Times out: tf.Tensor(98227, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([730 754 778], shape=(3,), dtype=int64) Times out: tf.Tensor(778, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_263\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_264 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_526 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_527 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_263 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_526 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_263 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_527 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 44.7477 - mse: 44.6890 - mae: 5.1905 - val_loss: 34.2130 - val_mse: 34.1416 - val_mae: 4.6599\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.5391 - mse: 35.4599 - mae: 4.6629 - val_loss: 32.6563 - val_mse: 32.5696 - val_mae: 4.5641\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.4852 - mse: 34.3932 - mae: 4.5980 - val_loss: 32.1550 - val_mse: 32.0585 - val_mae: 4.5349\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.0414 - mse: 33.9414 - mae: 4.5693 - val_loss: 32.3533 - val_mse: 32.2505 - val_mae: 4.5348\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.7507 - mse: 33.6454 - mae: 4.5491 - val_loss: 32.2344 - val_mse: 32.1270 - val_mae: 4.5235\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.4820 - mse: 33.3725 - mae: 4.5315 - val_loss: 31.7310 - val_mse: 31.6192 - val_mae: 4.4988\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.1956 - mse: 33.0817 - mae: 4.5120 - val_loss: 31.4617 - val_mse: 31.3452 - val_mae: 4.4780\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 32.7503 - mse: 32.6312 - mae: 4.4789 - val_loss: 31.9601 - val_mse: 31.8383 - val_mae: 4.4952\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.5954 - mse: 32.4712 - mae: 4.4693 - val_loss: 32.2540 - val_mse: 32.1270 - val_mae: 4.5242\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.3134 - mse: 32.1842 - mae: 4.4481 - val_loss: 32.1151 - val_mse: 31.9834 - val_mae: 4.5092\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.1547 - mse: 32.0207 - mae: 4.4384 - val_loss: 31.5699 - val_mse: 31.4333 - val_mae: 4.4738\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.0495 - mse: 31.9105 - mae: 4.4259 - val_loss: 31.5005 - val_mse: 31.3589 - val_mae: 4.4644\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.7933 - mse: 31.6490 - mae: 4.4117 - val_loss: 31.4151 - val_mse: 31.2686 - val_mae: 4.4509\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.7006 - mse: 31.5518 - mae: 4.4025 - val_loss: 30.9225 - val_mse: 30.7714 - val_mae: 4.4228\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.4975 - mse: 31.3445 - mae: 4.3862 - val_loss: 31.1101 - val_mse: 30.9548 - val_mae: 4.4374\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.3847 - mse: 31.2275 - mae: 4.3777 - val_loss: 31.1263 - val_mse: 30.9672 - val_mae: 4.4304\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.3022 - mse: 31.1413 - mae: 4.3686 - val_loss: 31.6503 - val_mse: 31.4875 - val_mae: 4.4723\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.2727 - mse: 31.1080 - mae: 4.3697 - val_loss: 30.9336 - val_mse: 30.7672 - val_mae: 4.4220\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.1332 - mse: 30.9652 - mae: 4.3599 - val_loss: 31.1871 - val_mse: 31.0173 - val_mae: 4.4365\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 31.0599 - mse: 30.8886 - mae: 4.3542 - val_loss: 30.2343 - val_mse: 30.0612 - val_mae: 4.3692\n",
      "bias -0.006964212\n",
      "si 0.5509279\n",
      "rmse 0.054828085\n",
      "kgeprime [0.58186595]\n",
      "rmse_95 0.07335246\n",
      "rmse_99 0.08897951\n",
      "pearson 0.8144631455255442\n",
      "pearson_95 0.5628750282028476\n",
      "pearson_99 0.5527418360434687\n",
      "rscore 0.6564496935545442\n",
      "rscore_95 -2.8215876377751377\n",
      "rscore_99 -13.900583523330043\n",
      "nse [0.65644969]\n",
      "nse_95 [-2.82158764]\n",
      "nse_99 [-13.90058352]\n",
      "kge [0.67547546]\n",
      "ext_kge_95 [0.43943534]\n",
      "ext_kge_99 [-0.04912104]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([81584 81608 81632], shape=(3,), dtype=int64) Times out: tf.Tensor(81632, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([84205 84229 84253], shape=(3,), dtype=int64) Times out: tf.Tensor(84253, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([63656 63680 63704], shape=(3,), dtype=int64) Times out: tf.Tensor(63704, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([53090 53114 53138], shape=(3,), dtype=int64) Times out: tf.Tensor(53138, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_264\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_265 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_528 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_529 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_264 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_528 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_264 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_529 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 45.5611 - mse: 45.5064 - mae: 5.2392 - val_loss: 38.9548 - val_mse: 38.8889 - val_mae: 4.9890\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.2598 - mse: 36.1864 - mae: 4.7034 - val_loss: 38.3382 - val_mse: 38.2569 - val_mae: 4.9644\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.0955 - mse: 35.0073 - mae: 4.6228 - val_loss: 36.7109 - val_mse: 36.6150 - val_mae: 4.8501\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 34.2502 - mse: 34.1497 - mae: 4.5663 - val_loss: 35.8462 - val_mse: 35.7396 - val_mae: 4.7874\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 34.0691 - mse: 33.9588 - mae: 4.5536 - val_loss: 35.7520 - val_mse: 35.6367 - val_mae: 4.7782\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.5954 - mse: 33.4769 - mae: 4.5251 - val_loss: 34.7835 - val_mse: 34.6604 - val_mae: 4.7036\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.3961 - mse: 33.2704 - mae: 4.5114 - val_loss: 35.0848 - val_mse: 34.9552 - val_mae: 4.7244\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.0774 - mse: 32.9456 - mae: 4.4856 - val_loss: 33.6540 - val_mse: 33.5187 - val_mae: 4.6169\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 32.8327 - mse: 32.6952 - mae: 4.4737 - val_loss: 33.4398 - val_mse: 33.2994 - val_mae: 4.6036\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 32.7114 - mse: 32.5690 - mae: 4.4616 - val_loss: 34.6167 - val_mse: 34.4717 - val_mae: 4.6919\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 32.4910 - mse: 32.3436 - mae: 4.4489 - val_loss: 33.0100 - val_mse: 32.8598 - val_mae: 4.5739\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 32.4591 - mse: 32.3073 - mae: 4.4450 - val_loss: 33.4348 - val_mse: 33.2809 - val_mae: 4.6129\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 32.3175 - mse: 32.1616 - mae: 4.4337 - val_loss: 32.9416 - val_mse: 32.7838 - val_mae: 4.5740\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 32.1075 - mse: 31.9475 - mae: 4.4216 - val_loss: 33.1812 - val_mse: 33.0194 - val_mae: 4.5926\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 32.0486 - mse: 31.8847 - mae: 4.4152 - val_loss: 33.1166 - val_mse: 32.9509 - val_mae: 4.5902\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 31.9755 - mse: 31.8080 - mae: 4.4107 - val_loss: 32.3832 - val_mse: 32.2141 - val_mae: 4.5347\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 31.8956 - mse: 31.7248 - mae: 4.4029 - val_loss: 33.1572 - val_mse: 32.9849 - val_mae: 4.5870\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 31.7326 - mse: 31.5586 - mae: 4.3911 - val_loss: 32.7122 - val_mse: 32.5372 - val_mae: 4.5611\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 31.6944 - mse: 31.5177 - mae: 4.3908 - val_loss: 32.8312 - val_mse: 32.6531 - val_mae: 4.5691\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 31.5642 - mse: 31.3847 - mae: 4.3823 - val_loss: 32.3898 - val_mse: 32.2091 - val_mae: 4.5324\n",
      "bias 0.010258371\n",
      "si 0.5219155\n",
      "rmse 0.056753006\n",
      "kgeprime [0.62944702]\n",
      "rmse_95 0.084529325\n",
      "rmse_99 0.07330002\n",
      "pearson 0.8442265015570178\n",
      "pearson_95 0.5320445350711491\n",
      "pearson_99 0.3947758318941032\n",
      "rscore 0.6912985879547792\n",
      "rscore_95 -7.982095117159666\n",
      "rscore_99 -15.635680242657273\n",
      "nse [0.69129859]\n",
      "nse_95 [-7.98209512]\n",
      "nse_99 [-15.63568024]\n",
      "kge [0.55523578]\n",
      "ext_kge_95 [-0.27621986]\n",
      "ext_kge_99 [-1.11758448]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([57344 57368 57392], shape=(3,), dtype=int64) Times out: tf.Tensor(57392, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([52517 52541 52565], shape=(3,), dtype=int64) Times out: tf.Tensor(52565, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([22172 22196 22220], shape=(3,), dtype=int64) Times out: tf.Tensor(22220, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([51860 51884 51908], shape=(3,), dtype=int64) Times out: tf.Tensor(51908, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_265\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_266 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_530 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_531 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_265 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_530 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_265 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_531 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 45.5736 - mse: 45.5296 - mae: 5.2932 - val_loss: 35.4918 - val_mse: 35.4380 - val_mae: 4.5152\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.5205 - mse: 36.4614 - mae: 4.7630 - val_loss: 33.8374 - val_mse: 33.7719 - val_mae: 4.4069\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.4324 - mse: 35.3633 - mae: 4.6952 - val_loss: 33.1592 - val_mse: 33.0863 - val_mae: 4.3657\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.8463 - mse: 34.7709 - mae: 4.6587 - val_loss: 33.0050 - val_mse: 32.9263 - val_mae: 4.3628\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.5667 - mse: 34.4859 - mae: 4.6414 - val_loss: 32.8018 - val_mse: 32.7179 - val_mae: 4.3487\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.2134 - mse: 34.1273 - mae: 4.6180 - val_loss: 32.5015 - val_mse: 32.4125 - val_mae: 4.3321\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 33.9675 - mse: 33.8765 - mae: 4.5959 - val_loss: 32.3282 - val_mse: 32.2343 - val_mae: 4.3226\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 33.7345 - mse: 33.6388 - mae: 4.5828 - val_loss: 32.2246 - val_mse: 32.1261 - val_mae: 4.3232\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.6488 - mse: 33.5488 - mae: 4.5806 - val_loss: 32.1403 - val_mse: 32.0378 - val_mae: 4.3156\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.4549 - mse: 33.3508 - mae: 4.5674 - val_loss: 31.9037 - val_mse: 31.7973 - val_mae: 4.3054\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.3508 - mse: 33.2428 - mae: 4.5562 - val_loss: 31.9564 - val_mse: 31.8458 - val_mae: 4.3021\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.1599 - mse: 33.0478 - mae: 4.5427 - val_loss: 31.7698 - val_mse: 31.6553 - val_mae: 4.2911\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.1529 - mse: 33.0369 - mae: 4.5387 - val_loss: 31.7506 - val_mse: 31.6319 - val_mae: 4.2974\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 32.8165 - mse: 32.6960 - mae: 4.5215 - val_loss: 31.3750 - val_mse: 31.2520 - val_mae: 4.2683\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 32.7093 - mse: 32.5846 - mae: 4.5070 - val_loss: 31.3230 - val_mse: 31.1960 - val_mae: 4.2616\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 32.5522 - mse: 32.4234 - mae: 4.5022 - val_loss: 31.1956 - val_mse: 31.0643 - val_mae: 4.2520\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 32.4680 - mse: 32.3353 - mae: 4.4949 - val_loss: 31.1054 - val_mse: 30.9709 - val_mae: 4.2486\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 32.4067 - mse: 32.2707 - mae: 4.4928 - val_loss: 31.0167 - val_mse: 30.8786 - val_mae: 4.2449\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 32.3166 - mse: 32.1771 - mae: 4.4800 - val_loss: 31.2007 - val_mse: 31.0593 - val_mae: 4.2528\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 32.0830 - mse: 31.9400 - mae: 4.4672 - val_loss: 30.7920 - val_mse: 30.6470 - val_mae: 4.2289\n",
      "bias 0.001006318\n",
      "si 0.5255651\n",
      "rmse 0.05535973\n",
      "kgeprime [0.74051318]\n",
      "rmse_95 0.11081184\n",
      "rmse_99 0.19350706\n",
      "pearson 0.8362989421618137\n",
      "pearson_95 0.1399514149755018\n",
      "pearson_99 -0.5037075180090925\n",
      "rscore 0.6956500052630774\n",
      "rscore_95 -2.7254637614635846\n",
      "rscore_99 -6.563227792240433\n",
      "nse [0.69565001]\n",
      "nse_95 [-2.72546376]\n",
      "nse_99 [-6.56322779]\n",
      "kge [0.72071536]\n",
      "ext_kge_95 [0.08045318]\n",
      "ext_kge_99 [-0.56920534]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([7676 7700 7724], shape=(3,), dtype=int64) Times out: tf.Tensor(7724, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([38258 38282 38306], shape=(3,), dtype=int64) Times out: tf.Tensor(38306, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([25550 25574 25598], shape=(3,), dtype=int64) Times out: tf.Tensor(25598, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([13109 13133 13157], shape=(3,), dtype=int64) Times out: tf.Tensor(13157, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_266\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_267 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_532 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_533 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_266 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_532 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_266 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_533 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 47.5574 - mse: 47.5062 - mae: 5.3597 - val_loss: 34.8676 - val_mse: 34.8063 - val_mae: 4.6485\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.6474 - mse: 37.5811 - mae: 4.8018 - val_loss: 33.7783 - val_mse: 33.7061 - val_mae: 4.5852\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.1504 - mse: 36.0739 - mae: 4.7132 - val_loss: 33.3319 - val_mse: 33.2519 - val_mae: 4.5461\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.5218 - mse: 35.4401 - mae: 4.6723 - val_loss: 33.1470 - val_mse: 33.0638 - val_mae: 4.5335\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.3629 - mse: 35.2789 - mae: 4.6629 - val_loss: 32.5571 - val_mse: 32.4720 - val_mae: 4.4861\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.2013 - mse: 35.1156 - mae: 4.6509 - val_loss: 32.5800 - val_mse: 32.4934 - val_mae: 4.4890\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 34.9656 - mse: 34.8784 - mae: 4.6366 - val_loss: 33.2770 - val_mse: 33.1887 - val_mae: 4.5437\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.7363 - mse: 34.6475 - mae: 4.6206 - val_loss: 32.2512 - val_mse: 32.1613 - val_mae: 4.4656\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.6038 - mse: 34.5129 - mae: 4.6100 - val_loss: 32.1372 - val_mse: 32.0450 - val_mae: 4.4588\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.4944 - mse: 34.4013 - mae: 4.6018 - val_loss: 32.1335 - val_mse: 32.0393 - val_mae: 4.4555\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.2337 - mse: 34.1382 - mae: 4.5857 - val_loss: 31.7793 - val_mse: 31.6826 - val_mae: 4.4308\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.0529 - mse: 33.9547 - mae: 4.5720 - val_loss: 31.3404 - val_mse: 31.2408 - val_mae: 4.3981\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 33.9561 - mse: 33.8555 - mae: 4.5641 - val_loss: 31.7851 - val_mse: 31.6832 - val_mae: 4.4304\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 33.9402 - mse: 33.8373 - mae: 4.5640 - val_loss: 31.1147 - val_mse: 31.0108 - val_mae: 4.3800\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 33.6584 - mse: 33.5534 - mae: 4.5475 - val_loss: 31.6662 - val_mse: 31.5603 - val_mae: 4.4221\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 33.6426 - mse: 33.5354 - mae: 4.5418 - val_loss: 31.2635 - val_mse: 31.1552 - val_mae: 4.3919\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.6657 - mse: 33.5563 - mae: 4.5488 - val_loss: 31.0018 - val_mse: 30.8914 - val_mae: 4.3728\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.2973 - mse: 33.1857 - mae: 4.5209 - val_loss: 30.8280 - val_mse: 30.7153 - val_mae: 4.3575\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 33.2263 - mse: 33.1122 - mae: 4.5136 - val_loss: 31.3981 - val_mse: 31.2830 - val_mae: 4.4036\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 33.2769 - mse: 33.1606 - mae: 4.5143 - val_loss: 31.1797 - val_mse: 31.0623 - val_mae: 4.3892\n",
      "bias 0.007155329\n",
      "si 0.5605591\n",
      "rmse 0.05573357\n",
      "kgeprime [0.68165592]\n",
      "rmse_95 0.0800704\n",
      "rmse_99 0.11243054\n",
      "pearson 0.8080625530500527\n",
      "pearson_95 0.4096553149253698\n",
      "pearson_99 0.3720631291990565\n",
      "rscore 0.6448475657303836\n",
      "rscore_95 -3.7141468540410756\n",
      "rscore_99 -19.140321895893496\n",
      "nse [0.64484757]\n",
      "nse_95 [-3.71414685]\n",
      "nse_99 [-19.1403219]\n",
      "kge [0.66814673]\n",
      "ext_kge_95 [0.27986972]\n",
      "ext_kge_99 [-0.14255942]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([93204 93228 93252], shape=(3,), dtype=int64) Times out: tf.Tensor(93252, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([119974 119998 120022], shape=(3,), dtype=int64) Times out: tf.Tensor(120022, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([145877 145901 145925], shape=(3,), dtype=int64) Times out: tf.Tensor(145925, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([160186 160210 160234], shape=(3,), dtype=int64) Times out: tf.Tensor(160234, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_267\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_268 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_534 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_535 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_267 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_534 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_267 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_535 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 45.7454 - mse: 45.7017 - mae: 5.2539 - val_loss: 32.3321 - val_mse: 32.2776 - val_mae: 4.5035\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5869 - mse: 36.5237 - mae: 4.7427 - val_loss: 30.6061 - val_mse: 30.5354 - val_mae: 4.3862\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.4739 - mse: 35.3971 - mae: 4.6720 - val_loss: 29.8001 - val_mse: 29.7182 - val_mae: 4.3257\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.9287 - mse: 34.8426 - mae: 4.6346 - val_loss: 29.5432 - val_mse: 29.4534 - val_mae: 4.3045\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.6900 - mse: 34.5970 - mae: 4.6223 - val_loss: 29.7334 - val_mse: 29.6372 - val_mae: 4.3236\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.3423 - mse: 34.2432 - mae: 4.5977 - val_loss: 28.7353 - val_mse: 28.6328 - val_mae: 4.2395\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.2073 - mse: 34.1021 - mae: 4.5855 - val_loss: 29.0163 - val_mse: 28.9084 - val_mae: 4.2745\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.9227 - mse: 33.8122 - mae: 4.5655 - val_loss: 27.9743 - val_mse: 27.8610 - val_mae: 4.1893\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.6998 - mse: 33.5843 - mae: 4.5498 - val_loss: 27.6736 - val_mse: 27.5555 - val_mae: 4.1657\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.5005 - mse: 33.3802 - mae: 4.5382 - val_loss: 28.4531 - val_mse: 28.3303 - val_mae: 4.2374\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.2860 - mse: 33.1612 - mae: 4.5233 - val_loss: 27.3109 - val_mse: 27.1835 - val_mae: 4.1375\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.2748 - mse: 33.1454 - mae: 4.5222 - val_loss: 27.0781 - val_mse: 26.9463 - val_mae: 4.1171\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.0995 - mse: 32.9659 - mae: 4.5066 - val_loss: 27.2940 - val_mse: 27.1583 - val_mae: 4.1405\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 33.0156 - mse: 32.8780 - mae: 4.5036 - val_loss: 27.3783 - val_mse: 27.2388 - val_mae: 4.1500\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.8709 - mse: 32.7296 - mae: 4.4939 - val_loss: 27.1849 - val_mse: 27.0417 - val_mae: 4.1353\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.7587 - mse: 32.6139 - mae: 4.4824 - val_loss: 27.1832 - val_mse: 27.0366 - val_mae: 4.1357\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 32.5876 - mse: 32.4394 - mae: 4.4756 - val_loss: 27.3000 - val_mse: 27.1499 - val_mae: 4.1486\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.5106 - mse: 32.3587 - mae: 4.4721 - val_loss: 26.8949 - val_mse: 26.7411 - val_mae: 4.1111\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.6122 - mse: 32.4568 - mae: 4.4753 - val_loss: 26.9605 - val_mse: 26.8037 - val_mae: 4.1173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 32.4885 - mse: 32.3301 - mae: 4.4675 - val_loss: 27.1783 - val_mse: 27.0182 - val_mae: 4.1345\n",
      "bias 0.0071206302\n",
      "si 0.50869423\n",
      "rmse 0.051979076\n",
      "kgeprime [0.72904296]\n",
      "rmse_95 0.08763353\n",
      "rmse_99 0.1138489\n",
      "pearson 0.8445952332644026\n",
      "pearson_95 0.537086588309469\n",
      "pearson_99 0.8065359350587725\n",
      "rscore 0.7061907057235612\n",
      "rscore_95 -2.852406855238749\n",
      "rscore_99 -5.620990133345912\n",
      "nse [0.70619071]\n",
      "nse_95 [-2.85240686]\n",
      "nse_99 [-5.62099013]\n",
      "kge [0.66708037]\n",
      "ext_kge_95 [0.43614371]\n",
      "ext_kge_99 [0.5844192]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -47.61 -47.3 -46.99 ... -41.37 -41.06\n",
      "  * longitude       (longitude) float32 167.8 168.1 168.4 ... 174.1 174.4 174.7\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -7.45 -7.01 ... -2.5\n",
      "    vgrd10m         (time, latitude, longitude) float32 9.308 8.61 ... 6.952\n",
      "    uw2             (time, latitude, longitude) float32 55.5 49.13 ... 6.248\n",
      "    vw2             (time, latitude, longitude) float32 86.65 74.13 ... 48.33\n",
      "    wind_magnitude  (time, latitude, longitude) float32 11.92 11.1 ... 7.388\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([81445 81469 81493], shape=(3,), dtype=int64) Times out: tf.Tensor(81493, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([65429 65453 65477], shape=(3,), dtype=int64) Times out: tf.Tensor(65477, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([123244 123268 123292], shape=(3,), dtype=int64) Times out: tf.Tensor(123292, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([25273 25297 25321], shape=(3,), dtype=int64) Times out: tf.Tensor(25321, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_268\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_269 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_536 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_537 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_268 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_536 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_268 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_537 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 50.5619 - mse: 50.5097 - mae: 5.5321 - val_loss: 42.5510 - val_mse: 42.4888 - val_mae: 5.1245\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 41.9533 - mse: 41.8851 - mae: 5.0584 - val_loss: 39.6818 - val_mse: 39.6078 - val_mae: 4.9529\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 40.0337 - mse: 39.9538 - mae: 4.9475 - val_loss: 38.6216 - val_mse: 38.5364 - val_mae: 4.8780\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 39.1810 - mse: 39.0915 - mae: 4.8960 - val_loss: 38.3229 - val_mse: 38.2294 - val_mae: 4.8590\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.5924 - mse: 38.4954 - mae: 4.8600 - val_loss: 37.9758 - val_mse: 37.8753 - val_mae: 4.8423\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.1492 - mse: 38.0458 - mae: 4.8284 - val_loss: 37.7432 - val_mse: 37.6364 - val_mae: 4.8238\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.6205 - mse: 37.5107 - mae: 4.7968 - val_loss: 37.4683 - val_mse: 37.3559 - val_mae: 4.8084\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.5179 - mse: 37.4027 - mae: 4.7886 - val_loss: 37.3998 - val_mse: 37.2825 - val_mae: 4.8054\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.1533 - mse: 37.0334 - mae: 4.7663 - val_loss: 37.1343 - val_mse: 37.0120 - val_mae: 4.7875\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.7518 - mse: 36.6271 - mae: 4.7433 - val_loss: 36.8048 - val_mse: 36.6781 - val_mae: 4.7660\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5982 - mse: 36.4693 - mae: 4.7311 - val_loss: 36.1770 - val_mse: 36.0460 - val_mae: 4.7263\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.3615 - mse: 36.2284 - mae: 4.7163 - val_loss: 36.0097 - val_mse: 35.8747 - val_mae: 4.7113\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.1642 - mse: 36.0272 - mae: 4.7028 - val_loss: 35.8448 - val_mse: 35.7057 - val_mae: 4.6980\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.9617 - mse: 35.8207 - mae: 4.6876 - val_loss: 36.0932 - val_mse: 35.9502 - val_mae: 4.7176\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.7555 - mse: 35.6108 - mae: 4.6747 - val_loss: 36.2719 - val_mse: 36.1254 - val_mae: 4.7351\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.5746 - mse: 35.4261 - mae: 4.6606 - val_loss: 36.6863 - val_mse: 36.5361 - val_mae: 4.7642\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 35.3112 - mse: 35.1591 - mae: 4.6442 - val_loss: 36.6635 - val_mse: 36.5097 - val_mae: 4.7667\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.2368 - mse: 35.0814 - mae: 4.6370 - val_loss: 36.1932 - val_mse: 36.0361 - val_mae: 4.7382\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.1026 - mse: 34.9438 - mae: 4.6355 - val_loss: 36.0403 - val_mse: 35.8796 - val_mae: 4.7265\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 34.9932 - mse: 34.8311 - mae: 4.6212 - val_loss: 35.2407 - val_mse: 35.0769 - val_mae: 4.6658\n",
      "bias -0.007432017\n",
      "si 0.56801856\n",
      "rmse 0.059225738\n",
      "kgeprime [0.54181093]\n",
      "rmse_95 0.09577427\n",
      "rmse_99 0.11518246\n",
      "pearson 0.8011595928008021\n",
      "pearson_95 0.5470940967406724\n",
      "pearson_99 0.06263176946145896\n",
      "rscore 0.6361064092543997\n",
      "rscore_95 -4.127323011837414\n",
      "rscore_99 -51.17792675852519\n",
      "nse [0.63610641]\n",
      "nse_95 [-4.12732301]\n",
      "nse_99 [-51.17792676]\n",
      "kge [0.6403288]\n",
      "ext_kge_95 [0.32212671]\n",
      "ext_kge_99 [-0.9486879]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([70796 70820 70844], shape=(3,), dtype=int64) Times out: tf.Tensor(70844, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([37675 37699 37723], shape=(3,), dtype=int64) Times out: tf.Tensor(37723, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([116192 116216 116240], shape=(3,), dtype=int64) Times out: tf.Tensor(116240, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([111753 111777 111801], shape=(3,), dtype=int64) Times out: tf.Tensor(111801, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_269\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_270 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_538 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_539 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_269 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_538 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_269 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_539 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 53.9055 - mse: 53.8595 - mae: 5.6964 - val_loss: 45.4230 - val_mse: 45.3634 - val_mae: 5.4128\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 42.6162 - mse: 42.5496 - mae: 5.0718 - val_loss: 42.3756 - val_mse: 42.3015 - val_mae: 5.2479\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 40.7949 - mse: 40.7149 - mae: 4.9613 - val_loss: 40.4423 - val_mse: 40.3564 - val_mae: 5.1345\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.7314 - mse: 39.6416 - mae: 4.8941 - val_loss: 38.7031 - val_mse: 38.6094 - val_mae: 5.0232\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 39.3140 - mse: 39.2171 - mae: 4.8668 - val_loss: 38.6746 - val_mse: 38.5744 - val_mae: 5.0216\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.6749 - mse: 38.5722 - mae: 4.8332 - val_loss: 37.4643 - val_mse: 37.3588 - val_mae: 4.9405\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.3452 - mse: 38.2372 - mae: 4.8090 - val_loss: 37.9816 - val_mse: 37.8709 - val_mae: 4.9749\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.9934 - mse: 37.8803 - mae: 4.7882 - val_loss: 37.4395 - val_mse: 37.3239 - val_mae: 4.9412\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.6102 - mse: 37.4926 - mae: 4.7647 - val_loss: 37.1584 - val_mse: 37.0384 - val_mae: 4.9237\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.3584 - mse: 37.2363 - mae: 4.7475 - val_loss: 36.4976 - val_mse: 36.3734 - val_mae: 4.8755\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.2447 - mse: 37.1187 - mae: 4.7423 - val_loss: 36.7020 - val_mse: 36.5742 - val_mae: 4.8944\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.1059 - mse: 36.9763 - mae: 4.7296 - val_loss: 36.2670 - val_mse: 36.1358 - val_mae: 4.8636\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.8121 - mse: 36.6794 - mae: 4.7087 - val_loss: 35.9997 - val_mse: 35.8655 - val_mae: 4.8417\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.6022 - mse: 36.4663 - mae: 4.6995 - val_loss: 36.3264 - val_mse: 36.1888 - val_mae: 4.8685\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.4148 - mse: 36.2759 - mae: 4.6833 - val_loss: 36.1510 - val_mse: 36.0107 - val_mae: 4.8597\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.1680 - mse: 36.0264 - mae: 4.6699 - val_loss: 35.8146 - val_mse: 35.6716 - val_mae: 4.8360\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.1382 - mse: 35.9939 - mae: 4.6661 - val_loss: 35.4266 - val_mse: 35.2808 - val_mae: 4.8118\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.7886 - mse: 35.6416 - mae: 4.6453 - val_loss: 35.4112 - val_mse: 35.2628 - val_mae: 4.8042\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.8786 - mse: 35.7289 - mae: 4.6488 - val_loss: 35.7140 - val_mse: 35.5630 - val_mae: 4.8310\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.6346 - mse: 35.4823 - mae: 4.6327 - val_loss: 35.3251 - val_mse: 35.1713 - val_mae: 4.8065\n",
      "bias 0.009797869\n",
      "si 0.5269312\n",
      "rmse 0.059305422\n",
      "kgeprime [0.6605242]\n",
      "rmse_95 0.09205877\n",
      "rmse_99 0.08851701\n",
      "pearson 0.8372294806377014\n",
      "pearson_95 0.59491559617138\n",
      "pearson_99 0.5101534242937207\n",
      "rscore 0.6855937057726095\n",
      "rscore_95 -5.96241099878028\n",
      "rscore_99 -4.229011720470456\n",
      "nse [0.68559371]\n",
      "nse_95 [-5.962411]\n",
      "nse_99 [-4.22901172]\n",
      "kge [0.58628924]\n",
      "ext_kge_95 [0.21020448]\n",
      "ext_kge_99 [0.35392353]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([75979 76003 76027], shape=(3,), dtype=int64) Times out: tf.Tensor(76027, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([71468 71492 71516], shape=(3,), dtype=int64) Times out: tf.Tensor(71516, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([71622 71646 71670], shape=(3,), dtype=int64) Times out: tf.Tensor(71670, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([73104 73128 73152], shape=(3,), dtype=int64) Times out: tf.Tensor(73152, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_270\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_271 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_540 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_541 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_270 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_540 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_270 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_541 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 51.6091 - mse: 51.5577 - mae: 5.6677 - val_loss: 42.6180 - val_mse: 42.5515 - val_mae: 4.9208\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 42.5695 - mse: 42.4973 - mae: 5.1428 - val_loss: 41.4327 - val_mse: 41.3531 - val_mae: 4.8466\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 40.9162 - mse: 40.8325 - mae: 5.0453 - val_loss: 40.3018 - val_mse: 40.2121 - val_mae: 4.7832\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 39.7826 - mse: 39.6892 - mae: 4.9726 - val_loss: 38.7325 - val_mse: 38.6334 - val_mae: 4.6954\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 39.0919 - mse: 38.9894 - mae: 4.9281 - val_loss: 38.1034 - val_mse: 37.9952 - val_mae: 4.6577\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 38.4627 - mse: 38.3512 - mae: 4.8900 - val_loss: 37.8869 - val_mse: 37.7701 - val_mae: 4.6437\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.8925 - mse: 37.7728 - mae: 4.8535 - val_loss: 37.4398 - val_mse: 37.3155 - val_mae: 4.6173\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.4747 - mse: 37.3475 - mae: 4.8246 - val_loss: 37.2030 - val_mse: 37.0716 - val_mae: 4.6033\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.1026 - mse: 36.9687 - mae: 4.8008 - val_loss: 36.8361 - val_mse: 36.6982 - val_mae: 4.5811\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.0752 - mse: 36.9352 - mae: 4.7984 - val_loss: 36.7609 - val_mse: 36.6171 - val_mae: 4.5751\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.6940 - mse: 36.5482 - mae: 4.7752 - val_loss: 36.6310 - val_mse: 36.4821 - val_mae: 4.5759\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.5599 - mse: 36.4090 - mae: 4.7656 - val_loss: 36.3835 - val_mse: 36.2291 - val_mae: 4.5546\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.2496 - mse: 36.0932 - mae: 4.7481 - val_loss: 36.2174 - val_mse: 36.0575 - val_mae: 4.5444\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.1075 - mse: 35.9457 - mae: 4.7349 - val_loss: 36.3846 - val_mse: 36.2195 - val_mae: 4.5512\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.7611 - mse: 35.5942 - mae: 4.7156 - val_loss: 35.8800 - val_mse: 35.7100 - val_mae: 4.5201\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 35.6459 - mse: 35.4740 - mae: 4.7036 - val_loss: 35.9912 - val_mse: 35.8162 - val_mae: 4.5270\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.4259 - mse: 35.2490 - mae: 4.6948 - val_loss: 35.7561 - val_mse: 35.5764 - val_mae: 4.5111\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.2829 - mse: 35.1014 - mae: 4.6851 - val_loss: 35.9632 - val_mse: 35.7786 - val_mae: 4.5250\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.0846 - mse: 34.8983 - mae: 4.6708 - val_loss: 35.6014 - val_mse: 35.4122 - val_mae: 4.5001\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.0478 - mse: 34.8570 - mae: 4.6652 - val_loss: 35.5142 - val_mse: 35.3209 - val_mae: 4.4975\n",
      "bias -0.0055414382\n",
      "si 0.55364954\n",
      "rmse 0.05943138\n",
      "kgeprime [0.59066317]\n",
      "rmse_95 0.11454495\n",
      "rmse_99 0.20298326\n",
      "pearson 0.8138080453082018\n",
      "pearson_95 0.06768087364364195\n",
      "pearson_99 -0.5821364818155614\n",
      "rscore 0.6588415317990947\n",
      "rscore_95 -2.208610390726591\n",
      "rscore_99 -6.287952298621554\n",
      "nse [0.65884153]\n",
      "nse_95 [-2.20861039]\n",
      "nse_99 [-6.2879523]\n",
      "kge [0.67343057]\n",
      "ext_kge_95 [0.0260032]\n",
      "ext_kge_99 [-0.63815474]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([4634 4658 4682], shape=(3,), dtype=int64) Times out: tf.Tensor(4682, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([5967 5991 6015], shape=(3,), dtype=int64) Times out: tf.Tensor(6015, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([23376 23400 23424], shape=(3,), dtype=int64) Times out: tf.Tensor(23424, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([ 9969  9993 10017], shape=(3,), dtype=int64) Times out: tf.Tensor(10017, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_271\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_272 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_542 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_543 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_271 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_542 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_271 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_543 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 52.2680 - mse: 52.2167 - mae: 5.6308 - val_loss: 39.4566 - val_mse: 39.3839 - val_mae: 4.9515\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 41.7138 - mse: 41.6290 - mae: 5.0496 - val_loss: 38.3482 - val_mse: 38.2532 - val_mae: 4.8802\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.9072 - mse: 39.8040 - mae: 4.9422 - val_loss: 37.5231 - val_mse: 37.4133 - val_mae: 4.8156\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.0357 - mse: 38.9204 - mae: 4.8885 - val_loss: 37.2366 - val_mse: 37.1163 - val_mae: 4.8010\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.5677 - mse: 38.4426 - mae: 4.8599 - val_loss: 36.5292 - val_mse: 36.3992 - val_mae: 4.7496\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.9760 - mse: 37.8423 - mae: 4.8222 - val_loss: 36.5722 - val_mse: 36.4344 - val_mae: 4.7497\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.6088 - mse: 37.4675 - mae: 4.7957 - val_loss: 36.3502 - val_mse: 36.2054 - val_mae: 4.7408\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.3147 - mse: 37.1670 - mae: 4.7794 - val_loss: 35.9920 - val_mse: 35.8411 - val_mae: 4.7178\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.9013 - mse: 36.7475 - mae: 4.7529 - val_loss: 36.0142 - val_mse: 35.8577 - val_mae: 4.7204\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.6526 - mse: 36.4935 - mae: 4.7383 - val_loss: 35.4346 - val_mse: 35.2730 - val_mae: 4.6827\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.4183 - mse: 36.2541 - mae: 4.7222 - val_loss: 35.4177 - val_mse: 35.2512 - val_mae: 4.6861\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.1326 - mse: 35.9639 - mae: 4.7035 - val_loss: 35.1718 - val_mse: 35.0008 - val_mae: 4.6637\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.9790 - mse: 35.8061 - mae: 4.6920 - val_loss: 35.3629 - val_mse: 35.1878 - val_mae: 4.6777\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.7438 - mse: 35.5664 - mae: 4.6779 - val_loss: 35.2219 - val_mse: 35.0424 - val_mae: 4.6704\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.5095 - mse: 35.3280 - mae: 4.6557 - val_loss: 35.0005 - val_mse: 34.8172 - val_mae: 4.6551\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.2960 - mse: 35.1110 - mae: 4.6447 - val_loss: 35.1736 - val_mse: 34.9867 - val_mae: 4.6678\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.1098 - mse: 34.9209 - mae: 4.6380 - val_loss: 35.3726 - val_mse: 35.1820 - val_mae: 4.6808\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.9484 - mse: 34.7560 - mae: 4.6246 - val_loss: 34.4933 - val_mse: 34.2987 - val_mae: 4.6163\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.7986 - mse: 34.6027 - mae: 4.6146 - val_loss: 34.7430 - val_mse: 34.5452 - val_mae: 4.6358\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.6119 - mse: 34.4125 - mae: 4.5994 - val_loss: 34.6115 - val_mse: 34.4103 - val_mae: 4.6260\n",
      "bias 0.0018437766\n",
      "si 0.5884927\n",
      "rmse 0.058660235\n",
      "kgeprime [0.77200718]\n",
      "rmse_95 0.0773412\n",
      "rmse_99 0.09220124\n",
      "pearson 0.789291965225118\n",
      "pearson_95 0.37957583693389163\n",
      "pearson_99 0.5700278888618908\n",
      "rscore 0.6142263939123523\n",
      "rscore_95 -5.269613745203405\n",
      "rscore_99 -23.160161594355507\n",
      "nse [0.61422639]\n",
      "nse_95 [-5.26961375]\n",
      "nse_99 [-23.16016159]\n",
      "kge [0.75103375]\n",
      "ext_kge_95 [-0.06880165]\n",
      "ext_kge_99 [-1.17335564]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([122403 122427 122451], shape=(3,), dtype=int64) Times out: tf.Tensor(122451, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([85185 85209 85233], shape=(3,), dtype=int64) Times out: tf.Tensor(85233, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([85602 85626 85650], shape=(3,), dtype=int64) Times out: tf.Tensor(85650, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([56521 56545 56569], shape=(3,), dtype=int64) Times out: tf.Tensor(56569, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_272\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_273 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_544 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_545 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_272 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_544 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_272 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_545 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 51.9481 - mse: 51.8998 - mae: 5.6234 - val_loss: 37.3155 - val_mse: 37.2525 - val_mae: 4.7901\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 42.3901 - mse: 42.3179 - mae: 5.0941 - val_loss: 35.2214 - val_mse: 35.1409 - val_mae: 4.6661\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 40.7147 - mse: 40.6272 - mae: 4.9926 - val_loss: 34.0810 - val_mse: 33.9871 - val_mae: 4.5914\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 39.9411 - mse: 39.8418 - mae: 4.9467 - val_loss: 33.5699 - val_mse: 33.4657 - val_mae: 4.5601\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 39.1466 - mse: 39.0375 - mae: 4.8981 - val_loss: 33.0909 - val_mse: 32.9770 - val_mae: 4.5282\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.8671 - mse: 38.7485 - mae: 4.8777 - val_loss: 32.7630 - val_mse: 32.6397 - val_mae: 4.5054\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.4102 - mse: 38.2831 - mae: 4.8477 - val_loss: 32.4016 - val_mse: 32.2705 - val_mae: 4.4734\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.1592 - mse: 38.0248 - mae: 4.8319 - val_loss: 32.2996 - val_mse: 32.1619 - val_mae: 4.4716\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.8349 - mse: 37.6942 - mae: 4.8155 - val_loss: 31.8740 - val_mse: 31.7303 - val_mae: 4.4353\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.5050 - mse: 37.3585 - mae: 4.7946 - val_loss: 31.6715 - val_mse: 31.5224 - val_mae: 4.4233\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.3788 - mse: 37.2269 - mae: 4.7840 - val_loss: 31.5499 - val_mse: 31.3951 - val_mae: 4.4198\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.0618 - mse: 36.9049 - mae: 4.7656 - val_loss: 31.1468 - val_mse: 30.9876 - val_mae: 4.3867\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.0303 - mse: 36.8689 - mae: 4.7618 - val_loss: 30.9550 - val_mse: 30.7913 - val_mae: 4.3746\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.7641 - mse: 36.5984 - mae: 4.7442 - val_loss: 30.8361 - val_mse: 30.6682 - val_mae: 4.3668\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5066 - mse: 36.3368 - mae: 4.7247 - val_loss: 30.7640 - val_mse: 30.5920 - val_mae: 4.3636\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5283 - mse: 36.3545 - mae: 4.7282 - val_loss: 30.7835 - val_mse: 30.6077 - val_mae: 4.3680\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.1036 - mse: 35.9256 - mae: 4.7038 - val_loss: 30.5070 - val_mse: 30.3271 - val_mae: 4.3486\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.2265 - mse: 36.0446 - mae: 4.7120 - val_loss: 30.3420 - val_mse: 30.1582 - val_mae: 4.3324\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.9466 - mse: 35.7611 - mae: 4.6947 - val_loss: 30.2624 - val_mse: 30.0750 - val_mae: 4.3254\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.8502 - mse: 35.6611 - mae: 4.6809 - val_loss: 30.2370 - val_mse: 30.0458 - val_mae: 4.3311\n",
      "bias 0.0028178783\n",
      "si 0.53785634\n",
      "rmse 0.05481409\n",
      "kgeprime [0.76427846]\n",
      "rmse_95 0.091557525\n",
      "rmse_99 0.1259246\n",
      "pearson 0.8228379013095736\n",
      "pearson_95 0.6145804949369272\n",
      "pearson_99 0.6586365515285378\n",
      "rscore 0.6754775563065245\n",
      "rscore_95 -1.655577024773006\n",
      "rscore_99 -2.7599374965648558\n",
      "nse [0.67547756]\n",
      "nse_95 [-1.65557702]\n",
      "nse_99 [-2.7599375]\n",
      "kge [0.71667846]\n",
      "ext_kge_95 [0.49994173]\n",
      "ext_kge_99 [0.51270158]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -47.3 -46.99 -46.68 ... -40.75 -40.43\n",
      "  * longitude       (longitude) float32 165.3 165.6 165.9 ... 171.2 171.6 171.9\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -7.529 -7.42 ... -5.433\n",
      "    vgrd10m         (time, latitude, longitude) float32 9.009 9.17 ... -5.151\n",
      "    uw2             (time, latitude, longitude) float32 56.68 55.05 ... 29.52\n",
      "    vw2             (time, latitude, longitude) float32 81.17 84.08 ... 26.53\n",
      "    wind_magnitude  (time, latitude, longitude) float32 11.74 11.8 ... 7.487\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([61393 61417 61441], shape=(3,), dtype=int64) Times out: tf.Tensor(61441, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([133456 133480 133504], shape=(3,), dtype=int64) Times out: tf.Tensor(133504, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([32382 32406 32430], shape=(3,), dtype=int64) Times out: tf.Tensor(32430, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([18499 18523 18547], shape=(3,), dtype=int64) Times out: tf.Tensor(18547, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_273\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_274 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_546 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_547 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_273 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_546 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_273 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_547 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 30.6796 - mse: 30.6271 - mae: 4.2610 - val_loss: 19.0805 - val_mse: 19.0188 - val_mae: 3.4493\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.2628 - mse: 23.1957 - mae: 3.7627 - val_loss: 18.3244 - val_mse: 18.2517 - val_mae: 3.3818\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.5427 - mse: 22.4657 - mae: 3.6992 - val_loss: 17.8447 - val_mse: 17.7636 - val_mae: 3.3315\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2987 - mse: 22.2149 - mae: 3.6757 - val_loss: 17.7552 - val_mse: 17.6688 - val_mae: 3.3287\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0837 - mse: 21.9948 - mae: 3.6566 - val_loss: 18.0522 - val_mse: 17.9608 - val_mae: 3.3644\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.8831 - mse: 21.7894 - mae: 3.6361 - val_loss: 18.3552 - val_mse: 18.2590 - val_mae: 3.3979\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5953 - mse: 21.4969 - mae: 3.6116 - val_loss: 17.5478 - val_mse: 17.4469 - val_mae: 3.3118\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4081 - mse: 21.3048 - mae: 3.5960 - val_loss: 17.5376 - val_mse: 17.4320 - val_mae: 3.3156\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1943 - mse: 21.0872 - mae: 3.5775 - val_loss: 17.4238 - val_mse: 17.3149 - val_mae: 3.3034\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.0724 - mse: 20.9617 - mae: 3.5627 - val_loss: 17.2000 - val_mse: 17.0877 - val_mae: 3.2811\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8754 - mse: 20.7615 - mae: 3.5468 - val_loss: 17.3121 - val_mse: 17.1965 - val_mae: 3.2936\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7707 - mse: 20.6537 - mae: 3.5380 - val_loss: 17.1066 - val_mse: 16.9882 - val_mae: 3.2730\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7277 - mse: 20.6080 - mae: 3.5310 - val_loss: 17.2360 - val_mse: 17.1150 - val_mae: 3.2871\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5995 - mse: 20.4773 - mae: 3.5190 - val_loss: 17.1103 - val_mse: 16.9871 - val_mae: 3.2747\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5471 - mse: 20.4226 - mae: 3.5097 - val_loss: 17.0818 - val_mse: 16.9561 - val_mae: 3.2707\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3809 - mse: 20.2544 - mae: 3.4940 - val_loss: 17.0328 - val_mse: 16.9054 - val_mae: 3.2667\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4020 - mse: 20.2734 - mae: 3.4986 - val_loss: 17.5699 - val_mse: 17.4405 - val_mae: 3.3246\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3137 - mse: 20.1836 - mae: 3.4905 - val_loss: 16.8900 - val_mse: 16.7591 - val_mae: 3.2457\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3230 - mse: 20.1912 - mae: 3.4897 - val_loss: 16.9172 - val_mse: 16.7845 - val_mae: 3.2507\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.2621 - mse: 20.1286 - mae: 3.4812 - val_loss: 17.0321 - val_mse: 16.8980 - val_mae: 3.2654\n",
      "bias -0.0053036767\n",
      "si 0.465565\n",
      "rmse 0.04110716\n",
      "kgeprime [0.66370022]\n",
      "rmse_95 0.05506122\n",
      "rmse_99 0.066841185\n",
      "pearson 0.8682866086594393\n",
      "pearson_95 0.49313320775149044\n",
      "pearson_99 0.04127367560248807\n",
      "rscore 0.7497009483908087\n",
      "rscore_95 -2.5801932201804556\n",
      "rscore_99 -40.32095005236404\n",
      "nse [0.74970095]\n",
      "nse_95 [-2.58019322]\n",
      "nse_99 [-40.32095005]\n",
      "kge [0.74790851]\n",
      "ext_kge_95 [0.28355477]\n",
      "ext_kge_99 [-2.12944138]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([3531 3555 3579], shape=(3,), dtype=int64) Times out: tf.Tensor(3579, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77701 77725 77749], shape=(3,), dtype=int64) Times out: tf.Tensor(77749, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([101748 101772 101796], shape=(3,), dtype=int64) Times out: tf.Tensor(101796, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([76505 76529 76553], shape=(3,), dtype=int64) Times out: tf.Tensor(76553, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_274\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_275 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_548 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_549 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_274 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_548 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_274 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_549 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.4949 - mse: 35.4502 - mae: 4.6064 - val_loss: 24.7607 - val_mse: 24.7085 - val_mae: 3.9250\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.1488 - mse: 25.0916 - mae: 3.9020 - val_loss: 21.4394 - val_mse: 21.3780 - val_mae: 3.6764\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.5210 - mse: 23.4536 - mae: 3.7685 - val_loss: 21.1581 - val_mse: 21.0861 - val_mae: 3.6610\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.7621 - mse: 22.6846 - mae: 3.7055 - val_loss: 20.3990 - val_mse: 20.3181 - val_mae: 3.5802\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3934 - mse: 22.3084 - mae: 3.6753 - val_loss: 19.5827 - val_mse: 19.4957 - val_mae: 3.5053\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.1690 - mse: 22.0788 - mae: 3.6569 - val_loss: 19.5711 - val_mse: 19.4794 - val_mae: 3.5061\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0082 - mse: 21.9139 - mae: 3.6415 - val_loss: 19.2834 - val_mse: 19.1879 - val_mae: 3.4754\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8305 - mse: 21.7328 - mae: 3.6214 - val_loss: 19.4645 - val_mse: 19.3659 - val_mae: 3.4811\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.6228 - mse: 21.5219 - mae: 3.6054 - val_loss: 18.8848 - val_mse: 18.7831 - val_mae: 3.4316\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.4817 - mse: 21.3778 - mae: 3.5924 - val_loss: 18.9182 - val_mse: 18.8137 - val_mae: 3.4349\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3316 - mse: 21.2248 - mae: 3.5789 - val_loss: 18.6090 - val_mse: 18.5016 - val_mae: 3.4043\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.1854 - mse: 21.0760 - mae: 3.5653 - val_loss: 18.4874 - val_mse: 18.3776 - val_mae: 3.3927\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.0368 - mse: 20.9251 - mae: 3.5521 - val_loss: 18.3877 - val_mse: 18.2757 - val_mae: 3.3812\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.9630 - mse: 20.8491 - mae: 3.5413 - val_loss: 18.5971 - val_mse: 18.4829 - val_mae: 3.4029\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.7962 - mse: 20.6803 - mae: 3.5308 - val_loss: 18.5999 - val_mse: 18.4838 - val_mae: 3.4058\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.6879 - mse: 20.5703 - mae: 3.5186 - val_loss: 18.6636 - val_mse: 18.5459 - val_mae: 3.4093\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.5865 - mse: 20.4672 - mae: 3.5127 - val_loss: 18.5744 - val_mse: 18.4550 - val_mae: 3.4038\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.4789 - mse: 20.3580 - mae: 3.5019 - val_loss: 18.0878 - val_mse: 17.9670 - val_mae: 3.3487\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.4776 - mse: 20.3554 - mae: 3.4971 - val_loss: 18.3537 - val_mse: 18.2316 - val_mae: 3.3848\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.4671 - mse: 20.3437 - mae: 3.4991 - val_loss: 17.8265 - val_mse: 17.7030 - val_mae: 3.3340\n",
      "bias 0.0016537647\n",
      "si 0.43377513\n",
      "rmse 0.042075\n",
      "kgeprime [0.82872913]\n",
      "rmse_95 0.06219839\n",
      "rmse_99 0.07292264\n",
      "pearson 0.8917370994274384\n",
      "pearson_95 0.5435894134585343\n",
      "pearson_99 0.20430614472820588\n",
      "rscore 0.7911863987727048\n",
      "rscore_95 -2.2004319292116814\n",
      "rscore_99 -7.587662775596128\n",
      "nse [0.7911864]\n",
      "nse_95 [-2.20043193]\n",
      "nse_99 [-7.58766278]\n",
      "kge [0.79201103]\n",
      "ext_kge_95 [0.3211573]\n",
      "ext_kge_99 [-0.16363176]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([60621 60645 60669], shape=(3,), dtype=int64) Times out: tf.Tensor(60669, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([9441 9465 9489], shape=(3,), dtype=int64) Times out: tf.Tensor(9489, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([39391 39415 39439], shape=(3,), dtype=int64) Times out: tf.Tensor(39439, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([6069 6093 6117], shape=(3,), dtype=int64) Times out: tf.Tensor(6117, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_275\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_276 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_550 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_551 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_275 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_550 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_275 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_551 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 31.5123 - mse: 31.4610 - mae: 4.3539 - val_loss: 23.0742 - val_mse: 23.0108 - val_mae: 3.6481\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.6790 - mse: 23.6096 - mae: 3.8233 - val_loss: 22.3468 - val_mse: 22.2711 - val_mae: 3.5925\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.6450 - mse: 22.5640 - mae: 3.7403 - val_loss: 21.9519 - val_mse: 21.8662 - val_mae: 3.5599\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.0517 - mse: 21.9628 - mae: 3.6862 - val_loss: 21.4313 - val_mse: 21.3387 - val_mae: 3.5130\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.7149 - mse: 21.6205 - mae: 3.6625 - val_loss: 21.3284 - val_mse: 21.2316 - val_mae: 3.5010\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.5950 - mse: 21.4968 - mae: 3.6489 - val_loss: 21.1892 - val_mse: 21.0893 - val_mae: 3.4820\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.3905 - mse: 21.2892 - mae: 3.6287 - val_loss: 20.7588 - val_mse: 20.6557 - val_mae: 3.4421\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.1778 - mse: 21.0738 - mae: 3.6125 - val_loss: 20.4158 - val_mse: 20.3101 - val_mae: 3.4105\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.0378 - mse: 20.9315 - mae: 3.6000 - val_loss: 20.4116 - val_mse: 20.3042 - val_mae: 3.4076\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.8806 - mse: 20.7723 - mae: 3.5865 - val_loss: 20.0977 - val_mse: 19.9881 - val_mae: 3.3786\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.7758 - mse: 20.6655 - mae: 3.5725 - val_loss: 20.0501 - val_mse: 19.9386 - val_mae: 3.3728\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.6209 - mse: 20.5086 - mae: 3.5641 - val_loss: 19.7265 - val_mse: 19.6134 - val_mae: 3.3393\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.5138 - mse: 20.3999 - mae: 3.5560 - val_loss: 19.7784 - val_mse: 19.6635 - val_mae: 3.3427\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.4897 - mse: 20.3740 - mae: 3.5514 - val_loss: 19.7356 - val_mse: 19.6190 - val_mae: 3.3376\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.4569 - mse: 20.3395 - mae: 3.5464 - val_loss: 19.4917 - val_mse: 19.3734 - val_mae: 3.3133\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2505 - mse: 20.1314 - mae: 3.5301 - val_loss: 19.7024 - val_mse: 19.5826 - val_mae: 3.3332\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.3620 - mse: 20.2415 - mae: 3.5367 - val_loss: 19.5074 - val_mse: 19.3862 - val_mae: 3.3148\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2465 - mse: 20.1246 - mae: 3.5278 - val_loss: 19.3678 - val_mse: 19.2454 - val_mae: 3.3019\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2382 - mse: 20.1151 - mae: 3.5251 - val_loss: 19.6818 - val_mse: 19.5583 - val_mae: 3.3327\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2111 - mse: 20.0870 - mae: 3.5225 - val_loss: 19.8603 - val_mse: 19.7355 - val_mae: 3.3517\n",
      "bias 0.0048688496\n",
      "si 0.47695276\n",
      "rmse 0.0444247\n",
      "kgeprime [0.78016746]\n",
      "rmse_95 0.09394951\n",
      "rmse_99 0.16249572\n",
      "pearson 0.8683857145661098\n",
      "pearson_95 0.09921733635530838\n",
      "pearson_99 -0.7670998288051133\n",
      "rscore 0.7440894076625257\n",
      "rscore_95 -2.9638015437145597\n",
      "rscore_99 -10.309404968433002\n",
      "nse [0.74408941]\n",
      "nse_95 [-2.96380154]\n",
      "nse_99 [-10.30940497]\n",
      "kge [0.69822179]\n",
      "ext_kge_95 [0.05228937]\n",
      "ext_kge_99 [-0.86428467]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([30090 30114 30138], shape=(3,), dtype=int64) Times out: tf.Tensor(30138, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([14583 14607 14631], shape=(3,), dtype=int64) Times out: tf.Tensor(14631, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([35277 35301 35325], shape=(3,), dtype=int64) Times out: tf.Tensor(35325, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([38068 38092 38116], shape=(3,), dtype=int64) Times out: tf.Tensor(38116, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_276\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_277 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_552 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_553 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_276 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_552 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_276 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_553 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.3341 - mse: 33.2815 - mae: 4.4327 - val_loss: 23.6343 - val_mse: 23.5676 - val_mae: 3.8100\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.7426 - mse: 22.6704 - mae: 3.7132 - val_loss: 22.8931 - val_mse: 22.8173 - val_mae: 3.7484\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.9288 - mse: 21.8501 - mae: 3.6462 - val_loss: 22.4350 - val_mse: 22.3539 - val_mae: 3.7046\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5942 - mse: 21.5109 - mae: 3.6186 - val_loss: 22.2527 - val_mse: 22.1669 - val_mae: 3.6934\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3257 - mse: 21.2375 - mae: 3.5912 - val_loss: 22.7563 - val_mse: 22.6654 - val_mae: 3.7564\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.1616 - mse: 21.0680 - mae: 3.5805 - val_loss: 21.8550 - val_mse: 21.7586 - val_mae: 3.6497\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.9198 - mse: 20.8211 - mae: 3.5574 - val_loss: 21.8416 - val_mse: 21.7402 - val_mae: 3.6539\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.8159 - mse: 20.7120 - mae: 3.5498 - val_loss: 21.9172 - val_mse: 21.8110 - val_mae: 3.6687\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.6509 - mse: 20.5422 - mae: 3.5350 - val_loss: 21.8721 - val_mse: 21.7611 - val_mae: 3.6620\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.5191 - mse: 20.4059 - mae: 3.5220 - val_loss: 21.6730 - val_mse: 21.5574 - val_mae: 3.6384\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.3493 - mse: 20.2318 - mae: 3.5067 - val_loss: 22.0321 - val_mse: 21.9125 - val_mae: 3.6834\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.2725 - mse: 20.1512 - mae: 3.4994 - val_loss: 21.3308 - val_mse: 21.2074 - val_mae: 3.5952\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.1208 - mse: 19.9958 - mae: 3.4842 - val_loss: 21.3131 - val_mse: 21.1863 - val_mae: 3.6035\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0739 - mse: 19.9454 - mae: 3.4745 - val_loss: 21.1993 - val_mse: 21.0689 - val_mae: 3.5862\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.9527 - mse: 19.8207 - mae: 3.4672 - val_loss: 21.7192 - val_mse: 21.5859 - val_mae: 3.6558\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.8553 - mse: 19.7207 - mae: 3.4598 - val_loss: 21.0561 - val_mse: 20.9201 - val_mae: 3.5839\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7907 - mse: 19.6537 - mae: 3.4532 - val_loss: 21.0114 - val_mse: 20.8734 - val_mae: 3.5804\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7196 - mse: 19.5801 - mae: 3.4497 - val_loss: 20.9296 - val_mse: 20.7891 - val_mae: 3.5709\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.6073 - mse: 19.4657 - mae: 3.4347 - val_loss: 21.5266 - val_mse: 21.3839 - val_mae: 3.6484\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.5168 - mse: 19.3731 - mae: 3.4265 - val_loss: 21.2475 - val_mse: 21.1029 - val_mae: 3.6117\n",
      "bias 0.0063361465\n",
      "si 0.50479585\n",
      "rmse 0.045937896\n",
      "kgeprime [0.67212384]\n",
      "rmse_95 0.065860465\n",
      "rmse_99 0.081815645\n",
      "pearson 0.8498927262727892\n",
      "pearson_95 0.5869022992077653\n",
      "pearson_99 0.5444455413044726\n",
      "rscore 0.7124856335157255\n",
      "rscore_95 -1.8769313662117098\n",
      "rscore_99 -10.737572578281211\n",
      "nse [0.71248563]\n",
      "nse_95 [-1.87693137]\n",
      "nse_99 [-10.73757258]\n",
      "kge [0.71576026]\n",
      "ext_kge_95 [0.42698728]\n",
      "ext_kge_99 [-0.06911092]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([79976 80000 80024], shape=(3,), dtype=int64) Times out: tf.Tensor(80024, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([187854 187878 187902], shape=(3,), dtype=int64) Times out: tf.Tensor(187902, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([144063 144087 144111], shape=(3,), dtype=int64) Times out: tf.Tensor(144111, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77068 77092 77116], shape=(3,), dtype=int64) Times out: tf.Tensor(77116, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_277\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_278 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_554 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_555 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_277 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_554 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_277 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_555 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 31.5014 - mse: 31.4457 - mae: 4.3068 - val_loss: 20.4478 - val_mse: 20.3802 - val_mae: 3.5978\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.9437 - mse: 23.8697 - mae: 3.7998 - val_loss: 19.6741 - val_mse: 19.5951 - val_mae: 3.5272\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.3528 - mse: 23.2705 - mae: 3.7514 - val_loss: 19.1193 - val_mse: 19.0346 - val_mae: 3.4703\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0640 - mse: 22.9773 - mae: 3.7298 - val_loss: 19.1121 - val_mse: 19.0234 - val_mae: 3.4701\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 22.9578 - mse: 22.8676 - mae: 3.7192 - val_loss: 18.9197 - val_mse: 18.8279 - val_mae: 3.4509\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7435 - mse: 22.6500 - mae: 3.7024 - val_loss: 18.9305 - val_mse: 18.8357 - val_mae: 3.4526\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6139 - mse: 22.5173 - mae: 3.6945 - val_loss: 18.6301 - val_mse: 18.5318 - val_mae: 3.4200\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4670 - mse: 22.3670 - mae: 3.6761 - val_loss: 18.7210 - val_mse: 18.6192 - val_mae: 3.4318\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4207 - mse: 22.3174 - mae: 3.6718 - val_loss: 18.5939 - val_mse: 18.4888 - val_mae: 3.4225\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2724 - mse: 22.1655 - mae: 3.6538 - val_loss: 18.3853 - val_mse: 18.2765 - val_mae: 3.4046\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 22.1510 - mse: 22.0405 - mae: 3.6460 - val_loss: 18.3295 - val_mse: 18.2172 - val_mae: 3.4017\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 21.9149 - mse: 21.8008 - mae: 3.6241 - val_loss: 18.1042 - val_mse: 17.9885 - val_mae: 3.3808\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9035 - mse: 21.7861 - mae: 3.6255 - val_loss: 18.1631 - val_mse: 18.0443 - val_mae: 3.3901\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.8473 - mse: 21.7271 - mae: 3.6156 - val_loss: 17.7692 - val_mse: 17.6475 - val_mae: 3.3516\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.6883 - mse: 21.5654 - mae: 3.5997 - val_loss: 17.4785 - val_mse: 17.3543 - val_mae: 3.3214\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5636 - mse: 21.4384 - mae: 3.5917 - val_loss: 17.4693 - val_mse: 17.3429 - val_mae: 3.3210\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5435 - mse: 21.4161 - mae: 3.5909 - val_loss: 17.4432 - val_mse: 17.3149 - val_mae: 3.3198\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4992 - mse: 21.3700 - mae: 3.5840 - val_loss: 17.2441 - val_mse: 17.1139 - val_mae: 3.2989\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.3740 - mse: 21.2431 - mae: 3.5733 - val_loss: 17.1700 - val_mse: 17.0383 - val_mae: 3.2885\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2653 - mse: 21.1330 - mae: 3.5659 - val_loss: 17.5856 - val_mse: 17.4530 - val_mae: 3.3375\n",
      "bias 0.008008819\n",
      "si 0.45069772\n",
      "rmse 0.04177676\n",
      "kgeprime [0.6840257]\n",
      "rmse_95 0.062559076\n",
      "rmse_99 0.06988288\n",
      "pearson 0.8793487044662892\n",
      "pearson_95 0.43506982995695226\n",
      "pearson_99 0.7187582500021658\n",
      "rscore 0.7623349064766947\n",
      "rscore_95 -6.118432751574448\n",
      "rscore_99 -15.0966241790629\n",
      "nse [0.76233491]\n",
      "nse_95 [-6.11843275]\n",
      "nse_99 [-15.09662418]\n",
      "kge [0.66632516]\n",
      "ext_kge_95 [0.13973488]\n",
      "ext_kge_99 [0.08390455]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -46.99 -46.68 -46.37 ... -40.43 -40.12\n",
      "  * longitude       (longitude) float32 169.4 169.7 170.0 ... 175.6 175.9 176.2\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -2.549 ... 0.03632\n",
      "    vgrd10m         (time, latitude, longitude) float32 1.649 1.478 ... -0.3653\n",
      "    uw2             (time, latitude, longitude) float32 6.496 2.921 ... 0.001319\n",
      "    vw2             (time, latitude, longitude) float32 2.72 2.186 ... 0.1335\n",
      "    wind_magnitude  (time, latitude, longitude) float32 3.036 2.26 ... 0.3671\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([130425 130449 130473], shape=(3,), dtype=int64) Times out: tf.Tensor(130473, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([23382 23406 23430], shape=(3,), dtype=int64) Times out: tf.Tensor(23430, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([151863 151887 151911], shape=(3,), dtype=int64) Times out: tf.Tensor(151911, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([21184 21208 21232], shape=(3,), dtype=int64) Times out: tf.Tensor(21232, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_278\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_279 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_556 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_557 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_278 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_556 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_278 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_557 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 51.1620 - mse: 51.1060 - mae: 5.5704 - val_loss: 41.6104 - val_mse: 41.5360 - val_mae: 5.0362\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 40.7049 - mse: 40.6198 - mae: 4.9901 - val_loss: 38.0461 - val_mse: 37.9509 - val_mae: 4.8368\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.9234 - mse: 38.8202 - mae: 4.8765 - val_loss: 36.7760 - val_mse: 36.6651 - val_mae: 4.7571\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.1053 - mse: 37.9892 - mae: 4.8227 - val_loss: 36.7121 - val_mse: 36.5907 - val_mae: 4.7514\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.7369 - mse: 37.6118 - mae: 4.8019 - val_loss: 36.2051 - val_mse: 36.0761 - val_mae: 4.7255\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.2840 - mse: 37.1524 - mae: 4.7717 - val_loss: 36.0060 - val_mse: 35.8713 - val_mae: 4.7138\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.2320 - mse: 37.0950 - mae: 4.7631 - val_loss: 35.9197 - val_mse: 35.7799 - val_mae: 4.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.8989 - mse: 36.7570 - mae: 4.7433 - val_loss: 35.6837 - val_mse: 35.5391 - val_mae: 4.6988\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.8680 - mse: 36.7212 - mae: 4.7365 - val_loss: 35.7834 - val_mse: 35.6340 - val_mae: 4.7028\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5317 - mse: 36.3803 - mae: 4.7155 - val_loss: 35.8018 - val_mse: 35.6482 - val_mae: 4.7083\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.4334 - mse: 36.2779 - mae: 4.7083 - val_loss: 35.3440 - val_mse: 35.1862 - val_mae: 4.6821\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.3059 - mse: 36.1461 - mae: 4.6994 - val_loss: 35.6710 - val_mse: 35.5091 - val_mae: 4.7010\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.1426 - mse: 35.9792 - mae: 4.6887 - val_loss: 35.4050 - val_mse: 35.2395 - val_mae: 4.6864\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.9281 - mse: 35.7608 - mae: 4.6728 - val_loss: 35.3649 - val_mse: 35.1954 - val_mae: 4.6799\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.8652 - mse: 35.6941 - mae: 4.6689 - val_loss: 35.3165 - val_mse: 35.1432 - val_mae: 4.6788\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.8965 - mse: 35.7218 - mae: 4.6687 - val_loss: 34.8974 - val_mse: 34.7208 - val_mae: 4.6553\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.5852 - mse: 35.4069 - mae: 4.6501 - val_loss: 36.0826 - val_mse: 35.9023 - val_mae: 4.7301\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.4589 - mse: 35.2773 - mae: 4.6412 - val_loss: 35.4608 - val_mse: 35.2772 - val_mae: 4.6909\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.3825 - mse: 35.1974 - mae: 4.6320 - val_loss: 34.9610 - val_mse: 34.7739 - val_mae: 4.6548\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.1791 - mse: 34.9905 - mae: 4.6216 - val_loss: 35.3119 - val_mse: 35.1214 - val_mae: 4.6797\n",
      "bias -0.009247181\n",
      "si 0.584208\n",
      "rmse 0.059263315\n",
      "kgeprime [0.46733591]\n",
      "rmse_95 0.092109874\n",
      "rmse_99 0.1156516\n",
      "pearson 0.7890646069015776\n",
      "pearson_95 0.6157584478269645\n",
      "pearson_99 0.4390846951490213\n",
      "rscore 0.6131682106168539\n",
      "rscore_95 -2.6502360279949193\n",
      "rscore_99 -8.904226300847343\n",
      "nse [0.61316821]\n",
      "nse_95 [-2.65023603]\n",
      "nse_99 [-8.9042263]\n",
      "kge [0.58109558]\n",
      "ext_kge_95 [0.4455914]\n",
      "ext_kge_99 [-0.03926883]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([74857 74881 74905], shape=(3,), dtype=int64) Times out: tf.Tensor(74905, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([97252 97276 97300], shape=(3,), dtype=int64) Times out: tf.Tensor(97300, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([68421 68445 68469], shape=(3,), dtype=int64) Times out: tf.Tensor(68469, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([68425 68449 68473], shape=(3,), dtype=int64) Times out: tf.Tensor(68473, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_279\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_280 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_558 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_559 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_279 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_558 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_279 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_559 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 48.1162 - mse: 48.0673 - mae: 5.3877 - val_loss: 40.7201 - val_mse: 40.6557 - val_mae: 5.1094\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.8406 - mse: 39.7700 - mae: 4.9145 - val_loss: 37.8457 - val_mse: 37.7667 - val_mae: 4.9187\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.4014 - mse: 38.3175 - mae: 4.8201 - val_loss: 36.8123 - val_mse: 36.7222 - val_mae: 4.8443\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.7208 - mse: 37.6273 - mae: 4.7778 - val_loss: 36.3703 - val_mse: 36.2718 - val_mae: 4.8160\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.3578 - mse: 37.2570 - mae: 4.7488 - val_loss: 35.7578 - val_mse: 35.6526 - val_mae: 4.7725\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.0246 - mse: 36.9175 - mae: 4.7302 - val_loss: 36.0585 - val_mse: 35.9477 - val_mae: 4.7883\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.7029 - mse: 36.5902 - mae: 4.7080 - val_loss: 35.9148 - val_mse: 35.7985 - val_mae: 4.7873\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.5095 - mse: 36.3916 - mae: 4.6951 - val_loss: 35.3706 - val_mse: 35.2497 - val_mae: 4.7538\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.3737 - mse: 36.2511 - mae: 4.6839 - val_loss: 35.2187 - val_mse: 35.0930 - val_mae: 4.7263\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.0907 - mse: 35.9636 - mae: 4.6641 - val_loss: 35.1127 - val_mse: 34.9825 - val_mae: 4.7278\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.8862 - mse: 35.7544 - mae: 4.6523 - val_loss: 35.2960 - val_mse: 35.1612 - val_mae: 4.7294\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.6517 - mse: 35.5156 - mae: 4.6384 - val_loss: 35.8433 - val_mse: 35.7044 - val_mae: 4.7807\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.5187 - mse: 35.3785 - mae: 4.6263 - val_loss: 35.0466 - val_mse: 34.9036 - val_mae: 4.7347\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.4164 - mse: 35.2722 - mae: 4.6185 - val_loss: 35.1091 - val_mse: 34.9624 - val_mae: 4.7257\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.1295 - mse: 34.9819 - mae: 4.6009 - val_loss: 35.3639 - val_mse: 35.2137 - val_mae: 4.7447\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.0294 - mse: 34.8780 - mae: 4.5912 - val_loss: 34.9799 - val_mse: 34.8260 - val_mae: 4.7243\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.9716 - mse: 34.8166 - mae: 4.5903 - val_loss: 34.8192 - val_mse: 34.6618 - val_mae: 4.7044\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.8188 - mse: 34.6604 - mae: 4.5790 - val_loss: 35.3462 - val_mse: 35.1855 - val_mae: 4.7441\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.5843 - mse: 34.4226 - mae: 4.5624 - val_loss: 35.3899 - val_mse: 35.2257 - val_mae: 4.7499\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 34.5978 - mse: 34.4327 - mae: 4.5612 - val_loss: 35.0194 - val_mse: 34.8518 - val_mae: 4.7096\n",
      "bias 0.0014814149\n",
      "si 0.5511889\n",
      "rmse 0.059035365\n",
      "kgeprime [0.71045396]\n",
      "rmse_95 0.084524505\n",
      "rmse_99 0.07741797\n",
      "pearson 0.8197651299441735\n",
      "pearson_95 0.6873646387497694\n",
      "pearson_99 0.7838918293057912\n",
      "rscore 0.665803256478861\n",
      "rscore_95 -5.3772111261305096\n",
      "rscore_99 -7.26693330012699\n",
      "nse [0.66580326]\n",
      "nse_95 [-5.37721113]\n",
      "nse_99 [-7.2669333]\n",
      "kge [0.68217366]\n",
      "ext_kge_95 [0.02845689]\n",
      "ext_kge_99 [-0.2990858]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([44742 44766 44790], shape=(3,), dtype=int64) Times out: tf.Tensor(44790, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([70680 70704 70728], shape=(3,), dtype=int64) Times out: tf.Tensor(70728, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([66426 66450 66474], shape=(3,), dtype=int64) Times out: tf.Tensor(66474, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([67900 67924 67948], shape=(3,), dtype=int64) Times out: tf.Tensor(67948, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_280\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_281 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_560 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_561 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_280 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_560 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_280 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_561 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 48.2928 - mse: 48.2463 - mae: 5.4657 - val_loss: 40.2731 - val_mse: 40.2114 - val_mae: 4.8050\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 39.0288 - mse: 38.9610 - mae: 4.9338 - val_loss: 39.1821 - val_mse: 39.1065 - val_mae: 4.7334\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.5501 - mse: 37.4707 - mae: 4.8377 - val_loss: 38.6514 - val_mse: 38.5667 - val_mae: 4.7048\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.0036 - mse: 36.9157 - mae: 4.7995 - val_loss: 37.8212 - val_mse: 37.7287 - val_mae: 4.6526\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.5124 - mse: 36.4171 - mae: 4.7699 - val_loss: 37.3313 - val_mse: 37.2320 - val_mae: 4.6110\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.2002 - mse: 36.0982 - mae: 4.7553 - val_loss: 37.3829 - val_mse: 37.2771 - val_mae: 4.6057\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.9973 - mse: 35.8891 - mae: 4.7384 - val_loss: 37.0289 - val_mse: 36.9171 - val_mae: 4.5806\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.6551 - mse: 35.5411 - mae: 4.7177 - val_loss: 37.1073 - val_mse: 36.9897 - val_mae: 4.5834\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.4830 - mse: 35.3633 - mae: 4.7062 - val_loss: 37.1138 - val_mse: 36.9911 - val_mae: 4.5802\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.1919 - mse: 35.0672 - mae: 4.6902 - val_loss: 36.4653 - val_mse: 36.3377 - val_mae: 4.5318\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.0471 - mse: 34.9176 - mae: 4.6797 - val_loss: 36.5240 - val_mse: 36.3916 - val_mae: 4.5360\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.8184 - mse: 34.6842 - mae: 4.6625 - val_loss: 35.9862 - val_mse: 35.8490 - val_mae: 4.4991\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.7181 - mse: 34.5792 - mae: 4.6580 - val_loss: 36.1360 - val_mse: 35.9941 - val_mae: 4.5060\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.4768 - mse: 34.3330 - mae: 4.6380 - val_loss: 35.8965 - val_mse: 35.7499 - val_mae: 4.4937\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.2779 - mse: 34.1298 - mae: 4.6248 - val_loss: 36.1179 - val_mse: 35.9674 - val_mae: 4.4983\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 34.0964 - mse: 33.9442 - mae: 4.6126 - val_loss: 35.5058 - val_mse: 35.3512 - val_mae: 4.4554\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.9675 - mse: 33.8115 - mae: 4.6038 - val_loss: 35.6611 - val_mse: 35.5029 - val_mae: 4.4707\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.8577 - mse: 33.6980 - mae: 4.5939 - val_loss: 35.6480 - val_mse: 35.4857 - val_mae: 4.4658\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.7254 - mse: 33.5618 - mae: 4.5838 - val_loss: 35.5950 - val_mse: 35.4290 - val_mae: 4.4671\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 33.6121 - mse: 33.4450 - mae: 4.5774 - val_loss: 35.6835 - val_mse: 35.5142 - val_mae: 4.4692\n",
      "bias -0.006735961\n",
      "si 0.57713974\n",
      "rmse 0.059593793\n",
      "kgeprime [0.52037647]\n",
      "rmse_95 0.113679305\n",
      "rmse_99 0.20958367\n",
      "pearson 0.7959622665903056\n",
      "pearson_95 -0.060327558410243856\n",
      "pearson_99 -0.693290019247669\n",
      "rscore 0.6269961275041858\n",
      "rscore_95 -2.6933043016170872\n",
      "rscore_99 -7.810161476658198\n",
      "nse [0.62699613]\n",
      "nse_95 [-2.6933043]\n",
      "nse_99 [-7.81016148]\n",
      "kge [0.61593699]\n",
      "ext_kge_95 [-0.09950745]\n",
      "ext_kge_99 [-0.7613519]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([32267 32291 32315], shape=(3,), dtype=int64) Times out: tf.Tensor(32315, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([33684 33708 33732], shape=(3,), dtype=int64) Times out: tf.Tensor(33732, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([36250 36274 36298], shape=(3,), dtype=int64) Times out: tf.Tensor(36298, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([24671 24695 24719], shape=(3,), dtype=int64) Times out: tf.Tensor(24719, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_281\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_282 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_562 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_563 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_281 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_562 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_281 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_563 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 53.3061 - mse: 53.2594 - mae: 5.6765 - val_loss: 38.9255 - val_mse: 38.8605 - val_mae: 4.9387\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 41.7670 - mse: 41.6940 - mae: 5.0453 - val_loss: 36.6252 - val_mse: 36.5441 - val_mae: 4.7493\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.9936 - mse: 39.9070 - mae: 4.9350 - val_loss: 36.1372 - val_mse: 36.0451 - val_mae: 4.7009\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.2525 - mse: 39.1571 - mae: 4.8904 - val_loss: 35.3440 - val_mse: 35.2450 - val_mae: 4.6508\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.8383 - mse: 38.7364 - mae: 4.8636 - val_loss: 35.1301 - val_mse: 35.0253 - val_mae: 4.6321\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.4460 - mse: 38.3387 - mae: 4.8407 - val_loss: 34.9987 - val_mse: 34.8889 - val_mae: 4.6227\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.2512 - mse: 38.1391 - mae: 4.8248 - val_loss: 34.7150 - val_mse: 34.6006 - val_mae: 4.6062\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.0259 - mse: 37.9094 - mae: 4.8117 - val_loss: 34.8479 - val_mse: 34.7294 - val_mae: 4.6142\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.7906 - mse: 37.6700 - mae: 4.7969 - val_loss: 34.3995 - val_mse: 34.2770 - val_mae: 4.5885\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.7254 - mse: 37.6008 - mae: 4.7959 - val_loss: 34.3061 - val_mse: 34.1798 - val_mae: 4.5813\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.3452 - mse: 37.2167 - mae: 4.7692 - val_loss: 34.2241 - val_mse: 34.0940 - val_mae: 4.5799\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.2038 - mse: 37.0714 - mae: 4.7597 - val_loss: 34.2158 - val_mse: 34.0821 - val_mae: 4.5774\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.0711 - mse: 36.9355 - mae: 4.7502 - val_loss: 34.1760 - val_mse: 34.0387 - val_mae: 4.5752\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.9712 - mse: 36.8319 - mae: 4.7446 - val_loss: 34.4389 - val_mse: 34.2981 - val_mae: 4.5939\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.8508 - mse: 36.7083 - mae: 4.7339 - val_loss: 34.1815 - val_mse: 34.0376 - val_mae: 4.5774\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.7947 - mse: 36.6488 - mae: 4.7336 - val_loss: 33.9442 - val_mse: 33.7970 - val_mae: 4.5638\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.4940 - mse: 36.3451 - mae: 4.7105 - val_loss: 34.1824 - val_mse: 34.0319 - val_mae: 4.5776\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.5743 - mse: 36.4221 - mae: 4.7172 - val_loss: 34.0785 - val_mse: 33.9252 - val_mae: 4.5685\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.4156 - mse: 36.2606 - mae: 4.7067 - val_loss: 34.0317 - val_mse: 33.8755 - val_mae: 4.5714\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.1802 - mse: 36.0225 - mae: 4.6919 - val_loss: 34.3547 - val_mse: 34.1959 - val_mae: 4.5904\n",
      "bias 0.005024205\n",
      "si 0.6093585\n",
      "rmse 0.058477256\n",
      "kgeprime [0.71624989]\n",
      "rmse_95 0.08586377\n",
      "rmse_99 0.077298425\n",
      "pearson 0.7694149655962912\n",
      "pearson_95 0.5759774451613411\n",
      "pearson_99 0.4071000875885093\n",
      "rscore 0.5838084517456461\n",
      "rscore_95 -4.376799588022156\n",
      "rscore_99 -4.191117684468322\n",
      "nse [0.58380845]\n",
      "nse_95 [-4.37679959]\n",
      "nse_99 [-4.19111768]\n",
      "kge [0.67480253]\n",
      "ext_kge_95 [0.08687537]\n",
      "ext_kge_99 [0.15863761]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([169371 169395 169419], shape=(3,), dtype=int64) Times out: tf.Tensor(169419, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([97439 97463 97487], shape=(3,), dtype=int64) Times out: tf.Tensor(97487, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([48122 48146 48170], shape=(3,), dtype=int64) Times out: tf.Tensor(48170, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([40089 40113 40137], shape=(3,), dtype=int64) Times out: tf.Tensor(40137, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_282\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_283 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_564 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_565 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_282 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_564 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_282 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_565 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 54.1880 - mse: 54.1386 - mae: 5.7412 - val_loss: 39.4802 - val_mse: 39.4132 - val_mae: 4.9582\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 43.4716 - mse: 43.3907 - mae: 5.1484 - val_loss: 36.5767 - val_mse: 36.4814 - val_mae: 4.7674\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 40.7434 - mse: 40.6369 - mae: 4.9778 - val_loss: 34.1281 - val_mse: 34.0131 - val_mae: 4.6240\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 39.1186 - mse: 38.9970 - mae: 4.8768 - val_loss: 33.0854 - val_mse: 32.9593 - val_mae: 4.5442\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.5173 - mse: 38.3879 - mae: 4.8441 - val_loss: 32.7113 - val_mse: 32.5795 - val_mae: 4.5126\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.1507 - mse: 38.0163 - mae: 4.8170 - val_loss: 32.5075 - val_mse: 32.3709 - val_mae: 4.5053\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.7828 - mse: 37.6437 - mae: 4.7972 - val_loss: 32.4306 - val_mse: 32.2897 - val_mae: 4.5088\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.6052 - mse: 37.4617 - mae: 4.7820 - val_loss: 31.9504 - val_mse: 31.8051 - val_mae: 4.4654\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.2417 - mse: 37.0936 - mae: 4.7624 - val_loss: 31.8100 - val_mse: 31.6601 - val_mae: 4.4582\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.2162 - mse: 37.0639 - mae: 4.7567 - val_loss: 32.3236 - val_mse: 32.1696 - val_mae: 4.5112\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.9627 - mse: 36.8062 - mae: 4.7443 - val_loss: 31.6559 - val_mse: 31.4975 - val_mae: 4.4511\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.7953 - mse: 36.6350 - mae: 4.7335 - val_loss: 31.3017 - val_mse: 31.1399 - val_mae: 4.4192\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5909 - mse: 36.4269 - mae: 4.7173 - val_loss: 31.3103 - val_mse: 31.1447 - val_mae: 4.4243\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.3186 - mse: 36.1513 - mae: 4.7036 - val_loss: 31.4172 - val_mse: 31.2487 - val_mae: 4.4393\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.2075 - mse: 36.0368 - mae: 4.6908 - val_loss: 31.5826 - val_mse: 31.4104 - val_mae: 4.4550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.1695 - mse: 35.9956 - mae: 4.6926 - val_loss: 31.8429 - val_mse: 31.6679 - val_mae: 4.4798\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.0092 - mse: 35.8327 - mae: 4.6762 - val_loss: 31.0154 - val_mse: 30.8376 - val_mae: 4.4035\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.7446 - mse: 35.5653 - mae: 4.6613 - val_loss: 30.7549 - val_mse: 30.5742 - val_mae: 4.3808\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.6462 - mse: 35.4640 - mae: 4.6548 - val_loss: 30.4474 - val_mse: 30.2641 - val_mae: 4.3550\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.6021 - mse: 35.4170 - mae: 4.6489 - val_loss: 30.3770 - val_mse: 30.1906 - val_mae: 4.3469\n",
      "bias 0.0010519649\n",
      "si 0.5690264\n",
      "rmse 0.054945994\n",
      "kgeprime [0.72036326]\n",
      "rmse_95 0.08769006\n",
      "rmse_99 0.11176121\n",
      "pearson 0.7986358749761335\n",
      "pearson_95 0.579802882714932\n",
      "pearson_99 0.443529402981138\n",
      "rscore 0.6374053356797307\n",
      "rscore_95 -1.977034420087922\n",
      "rscore_99 -3.501406113940857\n",
      "nse [0.63740534]\n",
      "nse_95 [-1.97703442]\n",
      "nse_99 [-3.50140611]\n",
      "kge [0.70126925]\n",
      "ext_kge_95 [0.46632464]\n",
      "ext_kge_99 [0.34148963]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -46.68 -46.37 -46.05 ... -40.43 -40.12\n",
      "  * longitude       (longitude) float32 169.4 169.7 170.0 ... 175.6 175.9 176.2\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -1.568 ... 0.03632\n",
      "    vgrd10m         (time, latitude, longitude) float32 0.2393 ... -0.3653\n",
      "    uw2             (time, latitude, longitude) float32 2.459 ... 0.001319\n",
      "    vw2             (time, latitude, longitude) float32 0.05725 ... 0.1335\n",
      "    wind_magnitude  (time, latitude, longitude) float32 1.586 0.4669 ... 0.3671\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([126161 126185 126209], shape=(3,), dtype=int64) Times out: tf.Tensor(126209, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([85567 85591 85615], shape=(3,), dtype=int64) Times out: tf.Tensor(85615, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([98549 98573 98597], shape=(3,), dtype=int64) Times out: tf.Tensor(98597, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([83621 83645 83669], shape=(3,), dtype=int64) Times out: tf.Tensor(83669, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_283\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_284 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_566 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_567 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_283 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_566 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_283 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_567 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 52.8303 - mse: 52.7841 - mae: 5.6580 - val_loss: 46.2779 - val_mse: 46.2203 - val_mae: 5.2864\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 42.6749 - mse: 42.6087 - mae: 5.1021 - val_loss: 39.8528 - val_mse: 39.7789 - val_mae: 4.9236\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 40.0395 - mse: 39.9598 - mae: 4.9507 - val_loss: 38.7223 - val_mse: 38.6375 - val_mae: 4.8660\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 39.2933 - mse: 39.2045 - mae: 4.8980 - val_loss: 38.1423 - val_mse: 38.0502 - val_mae: 4.8354\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.7125 - mse: 38.6176 - mae: 4.8632 - val_loss: 37.7474 - val_mse: 37.6500 - val_mae: 4.8116\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.4570 - mse: 38.3570 - mae: 4.8456 - val_loss: 37.1978 - val_mse: 37.0952 - val_mae: 4.7724\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.2048 - mse: 38.0997 - mae: 4.8275 - val_loss: 37.2784 - val_mse: 37.1708 - val_mae: 4.7817\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.7236 - mse: 37.6136 - mae: 4.7977 - val_loss: 36.8386 - val_mse: 36.7263 - val_mae: 4.7507\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.6794 - mse: 37.5647 - mae: 4.7927 - val_loss: 36.6062 - val_mse: 36.4891 - val_mae: 4.7347\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.4510 - mse: 37.3318 - mae: 4.7729 - val_loss: 36.9666 - val_mse: 36.8451 - val_mae: 4.7581\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.1487 - mse: 37.0251 - mae: 4.7573 - val_loss: 36.5899 - val_mse: 36.4641 - val_mae: 4.7362\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.9666 - mse: 36.8388 - mae: 4.7447 - val_loss: 36.6903 - val_mse: 36.5605 - val_mae: 4.7441\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.6967 - mse: 36.5649 - mae: 4.7274 - val_loss: 36.3806 - val_mse: 36.2469 - val_mae: 4.7235\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5869 - mse: 36.4512 - mae: 4.7208 - val_loss: 35.9045 - val_mse: 35.7668 - val_mae: 4.6923\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.4711 - mse: 36.3316 - mae: 4.7113 - val_loss: 35.7313 - val_mse: 35.5901 - val_mae: 4.6831\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.2413 - mse: 36.0982 - mae: 4.6944 - val_loss: 36.1904 - val_mse: 36.0455 - val_mae: 4.7145\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.0984 - mse: 35.9518 - mae: 4.6868 - val_loss: 36.0169 - val_mse: 35.8691 - val_mae: 4.7055\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.9634 - mse: 35.8137 - mae: 4.6726 - val_loss: 36.3646 - val_mse: 36.2132 - val_mae: 4.7296\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 35.7552 - mse: 35.6024 - mae: 4.6620 - val_loss: 36.1296 - val_mse: 35.9752 - val_mae: 4.7115\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 35.7911 - mse: 35.6351 - mae: 4.6630 - val_loss: 36.1668 - val_mse: 36.0095 - val_mae: 4.7159\n",
      "bias -0.0097468505\n",
      "si 0.5887386\n",
      "rmse 0.06000789\n",
      "kgeprime [0.44135597]\n",
      "rmse_95 0.0984949\n",
      "rmse_99 0.12859738\n",
      "pearson 0.7855844036809055\n",
      "pearson_95 0.6193888926299972\n",
      "pearson_99 0.45261932706644203\n",
      "rscore 0.6066988127352374\n",
      "rscore_95 -2.587701016890346\n",
      "rscore_99 -8.042054422998993\n",
      "nse [0.60669881]\n",
      "nse_95 [-2.58770102]\n",
      "nse_99 [-8.04205442]\n",
      "kge [0.55887157]\n",
      "ext_kge_95 [0.47817632]\n",
      "ext_kge_99 [0.1578942]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([71740 71764 71788], shape=(3,), dtype=int64) Times out: tf.Tensor(71788, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([1432 1456 1480], shape=(3,), dtype=int64) Times out: tf.Tensor(1480, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([99066 99090 99114], shape=(3,), dtype=int64) Times out: tf.Tensor(99114, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([30433 30457 30481], shape=(3,), dtype=int64) Times out: tf.Tensor(30481, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_284\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_285 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_568 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_569 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_284 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_568 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_284 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_569 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 51.4441 - mse: 51.3992 - mae: 5.5512 - val_loss: 44.6026 - val_mse: 44.5427 - val_mae: 5.3473\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 41.7820 - mse: 41.7150 - mae: 5.0204 - val_loss: 40.5507 - val_mse: 40.4759 - val_mae: 5.1132\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 40.0595 - mse: 39.9802 - mae: 4.9131 - val_loss: 39.1449 - val_mse: 39.0606 - val_mae: 5.0207\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.2490 - mse: 39.1613 - mae: 4.8639 - val_loss: 38.6336 - val_mse: 38.5424 - val_mae: 4.9907\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.8271 - mse: 38.7334 - mae: 4.8348 - val_loss: 37.3270 - val_mse: 37.2304 - val_mae: 4.8861\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.4207 - mse: 38.3222 - mae: 4.8122 - val_loss: 37.7077 - val_mse: 37.6069 - val_mae: 4.9234\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 38.1211 - mse: 38.0185 - mae: 4.7968 - val_loss: 37.6151 - val_mse: 37.5102 - val_mae: 4.9195\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.8496 - mse: 37.7433 - mae: 4.7767 - val_loss: 37.3486 - val_mse: 37.2404 - val_mae: 4.8926\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 37.8660 - mse: 37.7563 - mae: 4.7728 - val_loss: 37.4103 - val_mse: 37.2990 - val_mae: 4.9102\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.5440 - mse: 37.4312 - mae: 4.7506 - val_loss: 36.9367 - val_mse: 36.8223 - val_mae: 4.8697\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.4695 - mse: 37.3537 - mae: 4.7470 - val_loss: 37.1882 - val_mse: 37.0708 - val_mae: 4.8882\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.2384 - mse: 37.1198 - mae: 4.7367 - val_loss: 36.4968 - val_mse: 36.3768 - val_mae: 4.8429\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.0967 - mse: 36.9752 - mae: 4.7263 - val_loss: 37.2636 - val_mse: 37.1407 - val_mae: 4.8923\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.0067 - mse: 36.8824 - mae: 4.7202 - val_loss: 36.2331 - val_mse: 36.1075 - val_mae: 4.8221\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.7575 - mse: 36.6305 - mae: 4.7012 - val_loss: 36.5975 - val_mse: 36.4690 - val_mae: 4.8469\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.6941 - mse: 36.5643 - mae: 4.6967 - val_loss: 36.6232 - val_mse: 36.4920 - val_mae: 4.8501\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.5272 - mse: 36.3948 - mae: 4.6849 - val_loss: 36.5042 - val_mse: 36.3704 - val_mae: 4.8436\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.4126 - mse: 36.2777 - mae: 4.6801 - val_loss: 37.7283 - val_mse: 37.5920 - val_mae: 4.9344\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.3542 - mse: 36.2166 - mae: 4.6758 - val_loss: 36.6434 - val_mse: 36.5045 - val_mae: 4.8551\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 36.2116 - mse: 36.0716 - mae: 4.6659 - val_loss: 37.8748 - val_mse: 37.7334 - val_mae: 4.9398\n",
      "bias 0.013166317\n",
      "si 0.55960166\n",
      "rmse 0.06142753\n",
      "kgeprime [0.50902337]\n",
      "rmse_95 0.101880014\n",
      "rmse_99 0.0893921\n",
      "pearson 0.8172350972107612\n",
      "pearson_95 0.7075251487723653\n",
      "pearson_99 0.6269950152966013\n",
      "rscore 0.6393733638969317\n",
      "rscore_95 -7.274947574689456\n",
      "rscore_99 -8.779472230277705\n",
      "nse [0.63937336]\n",
      "nse_95 [-7.27494757]\n",
      "nse_99 [-8.77947223]\n",
      "kge [0.46470283]\n",
      "ext_kge_95 [0.13756838]\n",
      "ext_kge_99 [0.07269342]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([9466 9490 9514], shape=(3,), dtype=int64) Times out: tf.Tensor(9514, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([65868 65892 65916], shape=(3,), dtype=int64) Times out: tf.Tensor(65916, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([22058 22082 22106], shape=(3,), dtype=int64) Times out: tf.Tensor(22106, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([829 853 877], shape=(3,), dtype=int64) Times out: tf.Tensor(877, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_285\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_286 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_570 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_571 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_285 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_570 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_285 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_571 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 50.6649 - mse: 50.6130 - mae: 5.5843 - val_loss: 41.1489 - val_mse: 41.0769 - val_mae: 4.8589\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 40.4065 - mse: 40.3268 - mae: 5.0150 - val_loss: 39.2002 - val_mse: 39.1112 - val_mae: 4.7347\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 38.7603 - mse: 38.6667 - mae: 4.9084 - val_loss: 38.4423 - val_mse: 38.3422 - val_mae: 4.6906\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 38.1877 - mse: 38.0840 - mae: 4.8724 - val_loss: 38.2533 - val_mse: 38.1445 - val_mae: 4.6795\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.8970 - mse: 37.7852 - mae: 4.8496 - val_loss: 37.8598 - val_mse: 37.7437 - val_mae: 4.6513\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.5052 - mse: 37.3868 - mae: 4.8262 - val_loss: 37.7415 - val_mse: 37.6197 - val_mae: 4.6345\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.3695 - mse: 37.2455 - mae: 4.8165 - val_loss: 37.5094 - val_mse: 37.3823 - val_mae: 4.6192\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 37.0580 - mse: 36.9290 - mae: 4.7975 - val_loss: 37.2175 - val_mse: 37.0854 - val_mae: 4.5998\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.8166 - mse: 36.6828 - mae: 4.7813 - val_loss: 37.1711 - val_mse: 37.0344 - val_mae: 4.5899\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.5579 - mse: 36.4197 - mae: 4.7608 - val_loss: 37.0155 - val_mse: 36.8746 - val_mae: 4.5726\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.4424 - mse: 36.2999 - mae: 4.7584 - val_loss: 37.0884 - val_mse: 36.9432 - val_mae: 4.5759\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.1913 - mse: 36.0449 - mae: 4.7416 - val_loss: 36.9407 - val_mse: 36.7920 - val_mae: 4.5611\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 36.1046 - mse: 35.9543 - mae: 4.7352 - val_loss: 36.7959 - val_mse: 36.6435 - val_mae: 4.5513\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.9251 - mse: 35.7710 - mae: 4.7220 - val_loss: 36.9540 - val_mse: 36.7976 - val_mae: 4.5617\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.8241 - mse: 35.6663 - mae: 4.7180 - val_loss: 36.5502 - val_mse: 36.3905 - val_mae: 4.5340\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.6397 - mse: 35.4786 - mae: 4.7068 - val_loss: 36.3442 - val_mse: 36.1811 - val_mae: 4.5163\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.4888 - mse: 35.3241 - mae: 4.6964 - val_loss: 36.2587 - val_mse: 36.0918 - val_mae: 4.5070\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.2949 - mse: 35.1264 - mae: 4.6839 - val_loss: 36.1650 - val_mse: 35.9947 - val_mae: 4.4928\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.3038 - mse: 35.1322 - mae: 4.6814 - val_loss: 36.2651 - val_mse: 36.0920 - val_mae: 4.4993\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 35.1278 - mse: 34.9534 - mae: 4.6689 - val_loss: 36.0841 - val_mse: 35.9079 - val_mae: 4.4917\n",
      "bias -0.0035198887\n",
      "si 0.58266413\n",
      "rmse 0.05992316\n",
      "kgeprime [0.63899051]\n",
      "rmse_95 0.114759736\n",
      "rmse_99 0.21120645\n",
      "pearson 0.7907799224100869\n",
      "pearson_95 -0.0905019856047607\n",
      "pearson_99 -0.6757161631219619\n",
      "rscore 0.623709263721642\n",
      "rscore_95 -2.7478105230958323\n",
      "rscore_99 -8.190799792094188\n",
      "nse [0.62370926]\n",
      "nse_95 [-2.74781052]\n",
      "nse_99 [-8.19079979]\n",
      "kge [0.6954626]\n",
      "ext_kge_95 [-0.12684927]\n",
      "ext_kge_99 [-0.75644653]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([3423 3447 3471], shape=(3,), dtype=int64) Times out: tf.Tensor(3471, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([13769 13793 13817], shape=(3,), dtype=int64) Times out: tf.Tensor(13817, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([33272 33296 33320], shape=(3,), dtype=int64) Times out: tf.Tensor(33320, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([32629 32653 32677], shape=(3,), dtype=int64) Times out: tf.Tensor(32677, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_286\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_287 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_572 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_573 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_286 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_572 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_286 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_573 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 50.2789 - mse: 50.2305 - mae: 5.5006 - val_loss: 38.2719 - val_mse: 38.2089 - val_mae: 4.8872\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 40.8914 - mse: 40.8217 - mae: 4.9861 - val_loss: 36.6920 - val_mse: 36.6161 - val_mae: 4.7593\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 39.4807 - mse: 39.4007 - mae: 4.9035 - val_loss: 36.1037 - val_mse: 36.0201 - val_mae: 4.7132\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.7434 - mse: 38.6576 - mae: 4.8579 - val_loss: 35.7273 - val_mse: 35.6392 - val_mae: 4.6840\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.3458 - mse: 38.2554 - mae: 4.8321 - val_loss: 35.5458 - val_mse: 35.4533 - val_mae: 4.6672\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 38.1381 - mse: 38.0439 - mae: 4.8182 - val_loss: 35.2719 - val_mse: 35.1759 - val_mae: 4.6534\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.9714 - mse: 37.8734 - mae: 4.8099 - val_loss: 35.4478 - val_mse: 35.3481 - val_mae: 4.6515\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.5941 - mse: 37.4923 - mae: 4.7861 - val_loss: 35.0851 - val_mse: 34.9815 - val_mae: 4.6295\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.4981 - mse: 37.3925 - mae: 4.7794 - val_loss: 34.8485 - val_mse: 34.7413 - val_mae: 4.6110\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 37.0972 - mse: 36.9879 - mae: 4.7564 - val_loss: 35.3347 - val_mse: 35.2237 - val_mae: 4.6363\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.9793 - mse: 36.8661 - mae: 4.7456 - val_loss: 35.0158 - val_mse: 34.9005 - val_mae: 4.6150\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.7682 - mse: 36.6511 - mae: 4.7338 - val_loss: 34.4608 - val_mse: 34.3420 - val_mae: 4.5818\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.5515 - mse: 36.4304 - mae: 4.7159 - val_loss: 34.3826 - val_mse: 34.2598 - val_mae: 4.5755\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.3988 - mse: 36.2740 - mae: 4.7073 - val_loss: 34.3003 - val_mse: 34.1739 - val_mae: 4.5693\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.3568 - mse: 36.2282 - mae: 4.7030 - val_loss: 34.3139 - val_mse: 34.1838 - val_mae: 4.5658\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 36.1396 - mse: 36.0072 - mae: 4.6906 - val_loss: 35.1088 - val_mse: 34.9746 - val_mae: 4.6139\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.9257 - mse: 35.7894 - mae: 4.6783 - val_loss: 34.3057 - val_mse: 34.1678 - val_mae: 4.5668\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.8099 - mse: 35.6698 - mae: 4.6669 - val_loss: 34.4921 - val_mse: 34.3505 - val_mae: 4.5698\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.7228 - mse: 35.5788 - mae: 4.6617 - val_loss: 34.0818 - val_mse: 33.9362 - val_mae: 4.5484\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 35.6558 - mse: 35.5082 - mae: 4.6576 - val_loss: 34.0169 - val_mse: 33.8679 - val_mae: 4.5465\n",
      "bias 0.0031265595\n",
      "si 0.60633904\n",
      "rmse 0.058196146\n",
      "kgeprime [0.72596528]\n",
      "rmse_95 0.08698497\n",
      "rmse_99 0.07892541\n",
      "pearson 0.7696660449854045\n",
      "pearson_95 0.6107024951844416\n",
      "pearson_99 0.48039489111730216\n",
      "rscore 0.5902155960751121\n",
      "rscore_95 -3.962268723518142\n",
      "rscore_99 -3.9773121894205437\n",
      "nse [0.5902156]\n",
      "nse_95 [-3.96226872]\n",
      "nse_99 [-3.97731219]\n",
      "kge [0.6786954]\n",
      "ext_kge_95 [0.19044883]\n",
      "ext_kge_99 [0.31220233]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([163152 163176 163200], shape=(3,), dtype=int64) Times out: tf.Tensor(163200, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77545 77569 77593], shape=(3,), dtype=int64) Times out: tf.Tensor(77593, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([148548 148572 148596], shape=(3,), dtype=int64) Times out: tf.Tensor(148596, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([180900 180924 180948], shape=(3,), dtype=int64) Times out: tf.Tensor(180948, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_287\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_288 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_574 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_575 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_287 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_574 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_287 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_575 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 52.4000 - mse: 52.3536 - mae: 5.6294 - val_loss: 36.8652 - val_mse: 36.8051 - val_mae: 4.8071\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 41.2221 - mse: 41.1548 - mae: 5.0159 - val_loss: 35.4592 - val_mse: 35.3853 - val_mae: 4.7414\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 39.5968 - mse: 39.5178 - mae: 4.9125 - val_loss: 34.1690 - val_mse: 34.0853 - val_mae: 4.6461\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.9969 - mse: 38.9096 - mae: 4.8715 - val_loss: 33.9497 - val_mse: 33.8589 - val_mae: 4.6356\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.4257 - mse: 38.3320 - mae: 4.8335 - val_loss: 33.7377 - val_mse: 33.6412 - val_mae: 4.6204\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.2896 - mse: 38.1909 - mae: 4.8258 - val_loss: 33.3647 - val_mse: 33.2638 - val_mae: 4.5943\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 38.0802 - mse: 37.9773 - mae: 4.8122 - val_loss: 33.2048 - val_mse: 33.0997 - val_mae: 4.5840\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.7852 - mse: 37.6782 - mae: 4.7881 - val_loss: 32.8022 - val_mse: 32.6928 - val_mae: 4.5519\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.6282 - mse: 37.5168 - mae: 4.7811 - val_loss: 33.0675 - val_mse: 32.9542 - val_mae: 4.5815\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.4609 - mse: 37.3455 - mae: 4.7727 - val_loss: 33.3701 - val_mse: 33.2528 - val_mae: 4.6086\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 37.3380 - mse: 37.2188 - mae: 4.7610 - val_loss: 32.2069 - val_mse: 32.0858 - val_mae: 4.5081\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.9829 - mse: 36.8600 - mae: 4.7443 - val_loss: 31.9306 - val_mse: 31.8058 - val_mae: 4.4910\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.9283 - mse: 36.8016 - mae: 4.7350 - val_loss: 32.6061 - val_mse: 32.4777 - val_mae: 4.5485\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.8223 - mse: 36.6920 - mae: 4.7292 - val_loss: 31.5971 - val_mse: 31.4650 - val_mae: 4.4617\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.6494 - mse: 36.5156 - mae: 4.7187 - val_loss: 31.6483 - val_mse: 31.5125 - val_mae: 4.4698\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.5889 - mse: 36.4515 - mae: 4.7154 - val_loss: 31.8153 - val_mse: 31.6763 - val_mae: 4.4889\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.3172 - mse: 36.1765 - mae: 4.6946 - val_loss: 31.3110 - val_mse: 31.1686 - val_mae: 4.4450\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 36.2498 - mse: 36.1059 - mae: 4.6884 - val_loss: 31.1705 - val_mse: 31.0250 - val_mae: 4.4342\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.1441 - mse: 35.9968 - mae: 4.6807 - val_loss: 31.3054 - val_mse: 31.1565 - val_mae: 4.4406\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 36.0534 - mse: 35.9029 - mae: 4.6742 - val_loss: 31.4592 - val_mse: 31.3070 - val_mae: 4.4599\n",
      "bias 0.009426685\n",
      "si 0.5714021\n",
      "rmse 0.05595267\n",
      "kgeprime [0.619968]\n",
      "rmse_95 0.09748324\n",
      "rmse_99 0.12468039\n",
      "pearson 0.7968468378566652\n",
      "pearson_95 0.5851245406990466\n",
      "pearson_99 0.4515115911111995\n",
      "rscore 0.6240452581254394\n",
      "rscore_95 -2.631176560521924\n",
      "rscore_99 -4.687494419461636\n",
      "nse [0.62404526]\n",
      "nse_95 [-2.63117656]\n",
      "nse_99 [-4.68749442]\n",
      "kge [0.57687892]\n",
      "ext_kge_95 [0.44151869]\n",
      "ext_kge_99 [0.32370589]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -45.74 -45.43 -45.12 ... -39.5 -39.18\n",
      "  * longitude       (longitude) float32 170.3 170.6 170.9 ... 176.6 176.9 177.2\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 1.59 3.1 ... 2.468 2.854\n",
      "    vgrd10m         (time, latitude, longitude) float32 3.239 5.13 ... -1.498\n",
      "    uw2             (time, latitude, longitude) float32 2.527 9.61 ... 8.147\n",
      "    vw2             (time, latitude, longitude) float32 10.49 26.31 ... 2.243\n",
      "    wind_magnitude  (time, latitude, longitude) float32 3.608 5.994 ... 3.223\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([48156 48180 48204], shape=(3,), dtype=int64) Times out: tf.Tensor(48204, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([47102 47126 47150], shape=(3,), dtype=int64) Times out: tf.Tensor(47150, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([93907 93931 93955], shape=(3,), dtype=int64) Times out: tf.Tensor(93955, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([43968 43992 44016], shape=(3,), dtype=int64) Times out: tf.Tensor(44016, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_288\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_289 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_576 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_577 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_288 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_576 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_288 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_577 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.7008 - mse: 28.6544 - mae: 4.1928 - val_loss: 21.3347 - val_mse: 21.2780 - val_mae: 3.6748\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.8285 - mse: 23.7660 - mae: 3.8457 - val_loss: 20.4980 - val_mse: 20.4312 - val_mae: 3.5937\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.4493 - mse: 23.3802 - mae: 3.8119 - val_loss: 20.3834 - val_mse: 20.3120 - val_mae: 3.5837\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.2500 - mse: 23.1774 - mae: 3.7922 - val_loss: 20.0294 - val_mse: 19.9556 - val_mae: 3.5465\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.1789 - mse: 23.1042 - mae: 3.7848 - val_loss: 20.4143 - val_mse: 20.3385 - val_mae: 3.5875\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0958 - mse: 23.0192 - mae: 3.7794 - val_loss: 20.2463 - val_mse: 20.1688 - val_mae: 3.5686\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0431 - mse: 22.9647 - mae: 3.7737 - val_loss: 20.1435 - val_mse: 20.0641 - val_mae: 3.5648\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.8497 - mse: 22.7695 - mae: 3.7568 - val_loss: 20.5024 - val_mse: 20.4208 - val_mae: 3.6022\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7130 - mse: 22.6304 - mae: 3.7429 - val_loss: 19.7949 - val_mse: 19.7109 - val_mae: 3.5319\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6110 - mse: 22.5258 - mae: 3.7334 - val_loss: 19.7267 - val_mse: 19.6405 - val_mae: 3.5293\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4248 - mse: 22.3373 - mae: 3.7174 - val_loss: 19.4823 - val_mse: 19.3935 - val_mae: 3.5050\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.3498 - mse: 22.2599 - mae: 3.7125 - val_loss: 19.5298 - val_mse: 19.4386 - val_mae: 3.5100\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.1923 - mse: 22.1001 - mae: 3.6955 - val_loss: 19.5471 - val_mse: 19.4538 - val_mae: 3.5130\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.1509 - mse: 22.0567 - mae: 3.6919 - val_loss: 19.3981 - val_mse: 19.3026 - val_mae: 3.5006\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9839 - mse: 21.8877 - mae: 3.6801 - val_loss: 19.4496 - val_mse: 19.3526 - val_mae: 3.5033\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9003 - mse: 21.8023 - mae: 3.6692 - val_loss: 19.4101 - val_mse: 19.3111 - val_mae: 3.4995\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.8297 - mse: 21.7297 - mae: 3.6628 - val_loss: 19.4937 - val_mse: 19.3927 - val_mae: 3.5095\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.7256 - mse: 21.6238 - mae: 3.6566 - val_loss: 19.2136 - val_mse: 19.1109 - val_mae: 3.4838\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.7124 - mse: 21.6086 - mae: 3.6556 - val_loss: 19.2613 - val_mse: 19.1565 - val_mae: 3.4896\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.6739 - mse: 21.5683 - mae: 3.6521 - val_loss: 19.4648 - val_mse: 19.3582 - val_mae: 3.5085\n",
      "bias -0.0077672503\n",
      "si 0.565309\n",
      "rmse 0.043998018\n",
      "kgeprime [0.46255216]\n",
      "rmse_95 0.06462343\n",
      "rmse_99 0.07297852\n",
      "pearson 0.7986250389882181\n",
      "pearson_95 0.27026891546583276\n",
      "pearson_99 0.25480948345477394\n",
      "rscore 0.6254548051630112\n",
      "rscore_95 -5.830525106233566\n",
      "rscore_99 -29.007923424801803\n",
      "nse [0.62545481]\n",
      "nse_95 [-5.83052511]\n",
      "nse_99 [-29.00792342]\n",
      "kge [0.57763637]\n",
      "ext_kge_95 [0.10307321]\n",
      "ext_kge_99 [-0.13133163]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([47017 47041 47065], shape=(3,), dtype=int64) Times out: tf.Tensor(47065, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([57517 57541 57565], shape=(3,), dtype=int64) Times out: tf.Tensor(57565, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([43553 43577 43601], shape=(3,), dtype=int64) Times out: tf.Tensor(43601, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([870 894 918], shape=(3,), dtype=int64) Times out: tf.Tensor(918, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_289\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_290 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_578 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_579 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_289 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_578 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_289 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_579 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.2345 - mse: 27.1921 - mae: 4.0949 - val_loss: 25.1409 - val_mse: 25.0873 - val_mae: 3.9357\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.9297 - mse: 22.8708 - mae: 3.7787 - val_loss: 24.5501 - val_mse: 24.4852 - val_mae: 3.8826\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5723 - mse: 22.5040 - mae: 3.7454 - val_loss: 23.0754 - val_mse: 23.0035 - val_mae: 3.7481\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3467 - mse: 22.2730 - mae: 3.7234 - val_loss: 23.8212 - val_mse: 23.7448 - val_mae: 3.8187\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2188 - mse: 22.1413 - mae: 3.7133 - val_loss: 23.3968 - val_mse: 23.3172 - val_mae: 3.7808\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.1271 - mse: 22.0463 - mae: 3.7065 - val_loss: 23.3878 - val_mse: 23.3048 - val_mae: 3.7817\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0405 - mse: 21.9562 - mae: 3.6973 - val_loss: 23.3490 - val_mse: 23.2626 - val_mae: 3.7785\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8973 - mse: 21.8095 - mae: 3.6853 - val_loss: 22.5619 - val_mse: 22.4721 - val_mae: 3.7068\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7576 - mse: 21.6665 - mae: 3.6744 - val_loss: 22.5950 - val_mse: 22.5017 - val_mae: 3.7145\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.6388 - mse: 21.5439 - mae: 3.6633 - val_loss: 22.8602 - val_mse: 22.7634 - val_mae: 3.7395\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5453 - mse: 21.4470 - mae: 3.6506 - val_loss: 22.2087 - val_mse: 22.1086 - val_mae: 3.6802\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3885 - mse: 21.2870 - mae: 3.6396 - val_loss: 21.7888 - val_mse: 21.6857 - val_mae: 3.6425\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.2609 - mse: 21.1563 - mae: 3.6279 - val_loss: 22.1092 - val_mse: 22.0033 - val_mae: 3.6748\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.1351 - mse: 21.0278 - mae: 3.6172 - val_loss: 21.9601 - val_mse: 21.8515 - val_mae: 3.6631\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.0509 - mse: 20.9409 - mae: 3.6107 - val_loss: 21.8879 - val_mse: 21.7767 - val_mae: 3.6581\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.0042 - mse: 20.8916 - mae: 3.6041 - val_loss: 21.7852 - val_mse: 21.6715 - val_mae: 3.6510\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.8707 - mse: 20.7557 - mae: 3.5913 - val_loss: 22.0339 - val_mse: 21.9179 - val_mae: 3.6772\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.7726 - mse: 20.6552 - mae: 3.5826 - val_loss: 21.9958 - val_mse: 21.8777 - val_mae: 3.6772\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.7175 - mse: 20.5982 - mae: 3.5785 - val_loss: 21.7934 - val_mse: 21.6734 - val_mae: 3.6602\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.6689 - mse: 20.5478 - mae: 3.5751 - val_loss: 22.0478 - val_mse: 21.9257 - val_mae: 3.6861\n",
      "bias 0.010619597\n",
      "si 0.53590053\n",
      "rmse 0.04682483\n",
      "kgeprime [0.5087919]\n",
      "rmse_95 0.08024935\n",
      "rmse_99 0.10754737\n",
      "pearson 0.8301399778309332\n",
      "pearson_95 0.2815549949789132\n",
      "pearson_99 0.21716740260583156\n",
      "rscore 0.6641806830629684\n",
      "rscore_95 -7.506768937840709\n",
      "rscore_99 -27.03346398974334\n",
      "nse [0.66418068]\n",
      "nse_95 [-7.50676894]\n",
      "nse_99 [-27.03346399]\n",
      "kge [0.49665959]\n",
      "ext_kge_95 [0.06107197]\n",
      "ext_kge_99 [-0.73952117]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([38570 38594 38618], shape=(3,), dtype=int64) Times out: tf.Tensor(38618, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([42888 42912 42936], shape=(3,), dtype=int64) Times out: tf.Tensor(42936, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([43509 43533 43557], shape=(3,), dtype=int64) Times out: tf.Tensor(43557, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([8128 8152 8176], shape=(3,), dtype=int64) Times out: tf.Tensor(8176, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_290\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_291 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_580 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_581 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_290 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_580 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_290 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_581 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.6273 - mse: 26.5736 - mae: 4.0628 - val_loss: 22.1422 - val_mse: 22.0746 - val_mae: 3.6735\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.9792 - mse: 22.9072 - mae: 3.7897 - val_loss: 21.6440 - val_mse: 21.5657 - val_mae: 3.6300\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.6027 - mse: 22.5226 - mae: 3.7543 - val_loss: 21.5122 - val_mse: 21.4275 - val_mae: 3.6175\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.4326 - mse: 22.3470 - mae: 3.7381 - val_loss: 21.2531 - val_mse: 21.1637 - val_mae: 3.6014\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.2404 - mse: 22.1505 - mae: 3.7244 - val_loss: 21.1201 - val_mse: 21.0265 - val_mae: 3.5905\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.0835 - mse: 21.9894 - mae: 3.7087 - val_loss: 20.9073 - val_mse: 20.8096 - val_mae: 3.5692\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.8654 - mse: 21.7675 - mae: 3.6898 - val_loss: 20.7952 - val_mse: 20.6937 - val_mae: 3.5567\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.6619 - mse: 21.5602 - mae: 3.6722 - val_loss: 20.5950 - val_mse: 20.4903 - val_mae: 3.5396\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.5274 - mse: 21.4224 - mae: 3.6621 - val_loss: 20.4785 - val_mse: 20.3708 - val_mae: 3.5274\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.4789 - mse: 21.3710 - mae: 3.6548 - val_loss: 20.4547 - val_mse: 20.3443 - val_mae: 3.5242\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.3352 - mse: 21.2243 - mae: 3.6429 - val_loss: 20.2664 - val_mse: 20.1529 - val_mae: 3.5070\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.1989 - mse: 21.0847 - mae: 3.6332 - val_loss: 20.1421 - val_mse: 20.0255 - val_mae: 3.4969\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.1300 - mse: 21.0128 - mae: 3.6283 - val_loss: 20.0899 - val_mse: 19.9707 - val_mae: 3.4921\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.0598 - mse: 20.9400 - mae: 3.6163 - val_loss: 20.0822 - val_mse: 19.9604 - val_mae: 3.4903\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.0051 - mse: 20.8825 - mae: 3.6154 - val_loss: 20.0710 - val_mse: 19.9468 - val_mae: 3.4881\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.9348 - mse: 20.8097 - mae: 3.6111 - val_loss: 20.0577 - val_mse: 19.9309 - val_mae: 3.4859\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 20.8570 - mse: 20.7295 - mae: 3.6004 - val_loss: 19.9935 - val_mse: 19.8645 - val_mae: 3.4802\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.7919 - mse: 20.6620 - mae: 3.5947 - val_loss: 19.9442 - val_mse: 19.8127 - val_mae: 3.4752\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.7427 - mse: 20.6100 - mae: 3.5902 - val_loss: 20.1468 - val_mse: 20.0126 - val_mae: 3.4919\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.6650 - mse: 20.5298 - mae: 3.5859 - val_loss: 20.1067 - val_mse: 19.9701 - val_mae: 3.4890\n",
      "bias 0.004710062\n",
      "si 0.5465518\n",
      "rmse 0.04468791\n",
      "kgeprime [0.73925262]\n",
      "rmse_95 0.0746179\n",
      "rmse_99 0.08437398\n",
      "pearson 0.8165582322488358\n",
      "pearson_95 0.3496355008647696\n",
      "pearson_99 -0.2144749927114503\n",
      "rscore 0.6614371461234507\n",
      "rscore_95 -5.8128076394761825\n",
      "rscore_99 -21.33604444459692\n",
      "nse [0.66143715]\n",
      "nse_95 [-5.81280764]\n",
      "nse_99 [-21.33604444]\n",
      "kge [0.66156134]\n",
      "ext_kge_95 [0.16691654]\n",
      "ext_kge_99 [-0.39999974]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([29655 29679 29703], shape=(3,), dtype=int64) Times out: tf.Tensor(29703, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([36085 36109 36133], shape=(3,), dtype=int64) Times out: tf.Tensor(36133, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([28240 28264 28288], shape=(3,), dtype=int64) Times out: tf.Tensor(28288, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34914 34938 34962], shape=(3,), dtype=int64) Times out: tf.Tensor(34962, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_291\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_292 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_582 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_583 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_291 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_582 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_291 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_583 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.8574 - mse: 27.8209 - mae: 4.1399 - val_loss: 23.9357 - val_mse: 23.8908 - val_mae: 3.8749\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.5393 - mse: 23.4900 - mae: 3.8189 - val_loss: 23.4062 - val_mse: 23.3526 - val_mae: 3.8196\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 23.0006 - mse: 22.9440 - mae: 3.7781 - val_loss: 23.2428 - val_mse: 23.1829 - val_mae: 3.7918\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.7693 - mse: 22.7078 - mae: 3.7562 - val_loss: 23.1482 - val_mse: 23.0844 - val_mae: 3.7776\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 22.7235 - mse: 22.6586 - mae: 3.7557 - val_loss: 23.2292 - val_mse: 23.1627 - val_mae: 3.7877\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 22.6125 - mse: 22.5444 - mae: 3.7439 - val_loss: 23.0873 - val_mse: 23.0176 - val_mae: 3.7749\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5128 - mse: 22.4416 - mae: 3.7361 - val_loss: 22.9502 - val_mse: 22.8771 - val_mae: 3.7615\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3095 - mse: 22.2352 - mae: 3.7161 - val_loss: 22.7471 - val_mse: 22.6712 - val_mae: 3.7429\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0284 - mse: 21.9512 - mae: 3.6972 - val_loss: 22.6128 - val_mse: 22.5341 - val_mae: 3.7302\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0727 - mse: 21.9928 - mae: 3.6986 - val_loss: 22.7724 - val_mse: 22.6913 - val_mae: 3.7497\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8910 - mse: 21.8086 - mae: 3.6826 - val_loss: 22.4801 - val_mse: 22.3964 - val_mae: 3.7224\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8153 - mse: 21.7306 - mae: 3.6762 - val_loss: 22.4054 - val_mse: 22.3195 - val_mae: 3.7181\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7612 - mse: 21.6742 - mae: 3.6694 - val_loss: 22.2731 - val_mse: 22.1850 - val_mae: 3.7045\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.6179 - mse: 21.5289 - mae: 3.6607 - val_loss: 22.2905 - val_mse: 22.2003 - val_mae: 3.7054\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5461 - mse: 21.4549 - mae: 3.6536 - val_loss: 22.1891 - val_mse: 22.0969 - val_mae: 3.6977\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.5556 - mse: 21.4624 - mae: 3.6510 - val_loss: 22.1329 - val_mse: 22.0386 - val_mae: 3.6966\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.5104 - mse: 21.4152 - mae: 3.6476 - val_loss: 22.0828 - val_mse: 21.9867 - val_mae: 3.6932\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.4018 - mse: 21.3049 - mae: 3.6373 - val_loss: 22.0618 - val_mse: 21.9638 - val_mae: 3.6927\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3878 - mse: 21.2889 - mae: 3.6325 - val_loss: 22.2280 - val_mse: 22.1281 - val_mae: 3.7069\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3503 - mse: 21.2494 - mae: 3.6323 - val_loss: 22.0620 - val_mse: 21.9601 - val_mae: 3.6931\n",
      "bias 0.0017702486\n",
      "si 0.604231\n",
      "rmse 0.046861652\n",
      "kgeprime [0.72903643]\n",
      "rmse_95 0.07414191\n",
      "rmse_99 0.091538616\n",
      "pearson 0.7714344958631256\n",
      "pearson_95 0.3348071198744111\n",
      "pearson_99 0.1369165228228603\n",
      "rscore 0.5929833917631124\n",
      "rscore_95 -7.558965462847633\n",
      "rscore_99 -25.486896869072687\n",
      "nse [0.59298339]\n",
      "nse_95 [-7.55896546]\n",
      "nse_99 [-25.48689687]\n",
      "kge [0.69474405]\n",
      "ext_kge_95 [0.05623422]\n",
      "ext_kge_99 [-0.44177589]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([53933 53957 53981], shape=(3,), dtype=int64) Times out: tf.Tensor(53981, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([41461 41485 41509], shape=(3,), dtype=int64) Times out: tf.Tensor(41509, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([173294 173318 173342], shape=(3,), dtype=int64) Times out: tf.Tensor(173342, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([106305 106329 106353], shape=(3,), dtype=int64) Times out: tf.Tensor(106353, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_292\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_293 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_584 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_585 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_292 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_584 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_292 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_585 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.7854 - mse: 28.7406 - mae: 4.2014 - val_loss: 22.0937 - val_mse: 22.0404 - val_mae: 3.7249\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.8918 - mse: 23.8342 - mae: 3.8433 - val_loss: 20.7412 - val_mse: 20.6795 - val_mae: 3.6298\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.4269 - mse: 23.3625 - mae: 3.7991 - val_loss: 20.5939 - val_mse: 20.5272 - val_mae: 3.6076\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.2946 - mse: 23.2264 - mae: 3.7848 - val_loss: 20.5447 - val_mse: 20.4750 - val_mae: 3.6098\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.1512 - mse: 23.0801 - mae: 3.7746 - val_loss: 20.3764 - val_mse: 20.3038 - val_mae: 3.5987\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0639 - mse: 22.9898 - mae: 3.7668 - val_loss: 20.2707 - val_mse: 20.1948 - val_mae: 3.5777\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.9527 - mse: 22.8753 - mae: 3.7614 - val_loss: 20.2839 - val_mse: 20.2049 - val_mae: 3.5800\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7775 - mse: 22.6972 - mae: 3.7409 - val_loss: 20.2279 - val_mse: 20.1458 - val_mae: 3.5700\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6363 - mse: 22.5529 - mae: 3.7276 - val_loss: 19.8875 - val_mse: 19.8024 - val_mae: 3.5524\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4865 - mse: 22.4001 - mae: 3.7180 - val_loss: 19.8703 - val_mse: 19.7824 - val_mae: 3.5416\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4606 - mse: 22.3718 - mae: 3.7181 - val_loss: 19.6116 - val_mse: 19.5211 - val_mae: 3.5306\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.3100 - mse: 22.2186 - mae: 3.7045 - val_loss: 19.7674 - val_mse: 19.6747 - val_mae: 3.5350\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2196 - mse: 22.1258 - mae: 3.6959 - val_loss: 19.5908 - val_mse: 19.4957 - val_mae: 3.5185\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.1293 - mse: 22.0332 - mae: 3.6908 - val_loss: 19.5158 - val_mse: 19.4183 - val_mae: 3.5196\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0528 - mse: 21.9543 - mae: 3.6822 - val_loss: 19.4898 - val_mse: 19.3902 - val_mae: 3.5185\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9930 - mse: 21.8924 - mae: 3.6775 - val_loss: 19.2868 - val_mse: 19.1850 - val_mae: 3.4998\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9345 - mse: 21.8317 - mae: 3.6726 - val_loss: 19.3392 - val_mse: 19.2352 - val_mae: 3.4953\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.8396 - mse: 21.7345 - mae: 3.6653 - val_loss: 19.1825 - val_mse: 19.0762 - val_mae: 3.4916\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.7965 - mse: 21.6892 - mae: 3.6573 - val_loss: 19.2278 - val_mse: 19.1193 - val_mae: 3.4965\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.7231 - mse: 21.6136 - mae: 3.6543 - val_loss: 19.2733 - val_mse: 19.1625 - val_mae: 3.5003\n",
      "bias -0.0022327092\n",
      "si 0.5654914\n",
      "rmse 0.043774936\n",
      "kgeprime [0.61426909]\n",
      "rmse_95 0.0694018\n",
      "rmse_99 0.09103583\n",
      "pearson 0.8005851174916729\n",
      "pearson_95 0.5266079232257755\n",
      "pearson_99 0.6486378075893765\n",
      "rscore 0.6360138694356827\n",
      "rscore_95 -3.61332527221212\n",
      "rscore_99 -9.907580782385905\n",
      "nse [0.63601387]\n",
      "nse_95 [-3.61332527]\n",
      "nse_99 [-9.90758078]\n",
      "kge [0.65991811]\n",
      "ext_kge_95 [0.41630965]\n",
      "ext_kge_99 [0.4736608]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -45.12 -44.8 -44.49 ... -38.87 -38.56\n",
      "  * longitude       (longitude) float32 168.1 168.4 168.8 ... 174.1 174.4 174.7\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -1.679 -2.549 ... -2.487\n",
      "    vgrd10m         (time, latitude, longitude) float32 3.758 3.549 ... -0.1581\n",
      "    uw2             (time, latitude, longitude) float32 2.82 6.496 ... 6.184\n",
      "    vw2             (time, latitude, longitude) float32 14.12 12.59 ... 0.02499\n",
      "    wind_magnitude  (time, latitude, longitude) float32 4.116 4.369 ... 2.492\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([32211 32235 32259], shape=(3,), dtype=int64) Times out: tf.Tensor(32259, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([46304 46328 46352], shape=(3,), dtype=int64) Times out: tf.Tensor(46352, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([94844 94868 94892], shape=(3,), dtype=int64) Times out: tf.Tensor(94892, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([150677 150701 150725], shape=(3,), dtype=int64) Times out: tf.Tensor(150725, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_293\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_294 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_586 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_587 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_293 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_586 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_293 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_587 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.0715 - mse: 29.0255 - mae: 4.1400 - val_loss: 20.4079 - val_mse: 20.3544 - val_mae: 3.4942\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.4400 - mse: 23.3822 - mae: 3.7508 - val_loss: 19.6536 - val_mse: 19.5921 - val_mae: 3.4341\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7469 - mse: 22.6822 - mae: 3.6922 - val_loss: 19.2239 - val_mse: 19.1562 - val_mae: 3.4258\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2225 - mse: 22.1525 - mae: 3.6504 - val_loss: 18.7821 - val_mse: 18.7096 - val_mae: 3.3740\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9780 - mse: 21.9031 - mae: 3.6282 - val_loss: 18.5164 - val_mse: 18.4390 - val_mae: 3.3580\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5525 - mse: 21.4728 - mae: 3.5916 - val_loss: 18.2665 - val_mse: 18.1846 - val_mae: 3.3414\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2833 - mse: 21.1995 - mae: 3.5702 - val_loss: 18.0753 - val_mse: 17.9897 - val_mae: 3.3244\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1497 - mse: 21.0623 - mae: 3.5600 - val_loss: 17.8423 - val_mse: 17.7529 - val_mae: 3.3037\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8826 - mse: 20.7914 - mae: 3.5352 - val_loss: 17.7496 - val_mse: 17.6565 - val_mae: 3.2943\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7078 - mse: 20.6129 - mae: 3.5207 - val_loss: 17.3385 - val_mse: 17.2418 - val_mae: 3.2532\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3920 - mse: 20.2935 - mae: 3.4924 - val_loss: 17.1356 - val_mse: 17.0355 - val_mae: 3.2367\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1761 - mse: 20.0745 - mae: 3.4756 - val_loss: 17.0452 - val_mse: 16.9421 - val_mae: 3.2257\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9585 - mse: 19.8539 - mae: 3.4562 - val_loss: 16.8354 - val_mse: 16.7296 - val_mae: 3.2083\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8588 - mse: 19.7516 - mae: 3.4441 - val_loss: 16.9362 - val_mse: 16.8277 - val_mae: 3.2129\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.7754 - mse: 19.6657 - mae: 3.4403 - val_loss: 16.7037 - val_mse: 16.5928 - val_mae: 3.1988\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6418 - mse: 19.5296 - mae: 3.4251 - val_loss: 16.3889 - val_mse: 16.2755 - val_mae: 3.1596\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5707 - mse: 19.4561 - mae: 3.4183 - val_loss: 16.3612 - val_mse: 16.2453 - val_mae: 3.1623\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3879 - mse: 19.2710 - mae: 3.4024 - val_loss: 16.2470 - val_mse: 16.1290 - val_mae: 3.1512\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2696 - mse: 19.1505 - mae: 3.3932 - val_loss: 16.2349 - val_mse: 16.1148 - val_mae: 3.1530\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2645 - mse: 19.1432 - mae: 3.3884 - val_loss: 16.1674 - val_mse: 16.0451 - val_mae: 3.1409\n",
      "bias 0.0011514381\n",
      "si 0.4714587\n",
      "rmse 0.0400563\n",
      "kgeprime [0.77824491]\n",
      "rmse_95 0.06813381\n",
      "rmse_99 0.10139458\n",
      "pearson 0.8631256799112017\n",
      "pearson_95 0.7103672078007462\n",
      "pearson_99 0.5566254173519716\n",
      "rscore 0.7407162406020371\n",
      "rscore_95 -0.46594246654695226\n",
      "rscore_99 -2.4960708816113866\n",
      "nse [0.74071624]\n",
      "nse_95 [-0.46594247]\n",
      "nse_99 [-2.49607088]\n",
      "kge [0.75453766]\n",
      "ext_kge_95 [0.6260102]\n",
      "ext_kge_99 [0.46032438]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([14346 14370 14394], shape=(3,), dtype=int64) Times out: tf.Tensor(14394, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([5607 5631 5655], shape=(3,), dtype=int64) Times out: tf.Tensor(5655, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([39363 39387 39411], shape=(3,), dtype=int64) Times out: tf.Tensor(39411, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([64567 64591 64615], shape=(3,), dtype=int64) Times out: tf.Tensor(64615, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_294\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_295 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_588 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_589 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_294 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_588 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_294 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_589 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.5381 - mse: 26.4822 - mae: 3.9575 - val_loss: 22.0411 - val_mse: 21.9746 - val_mae: 3.6218\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5638 - mse: 21.4925 - mae: 3.6113 - val_loss: 20.8881 - val_mse: 20.8106 - val_mae: 3.5341\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.8334 - mse: 20.7514 - mae: 3.5466 - val_loss: 20.3068 - val_mse: 20.2189 - val_mae: 3.4786\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.4296 - mse: 20.3376 - mae: 3.5107 - val_loss: 19.8766 - val_mse: 19.7797 - val_mae: 3.4463\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.1191 - mse: 20.0187 - mae: 3.4867 - val_loss: 19.5411 - val_mse: 19.4366 - val_mae: 3.4214\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.8126 - mse: 19.7050 - mae: 3.4581 - val_loss: 19.3803 - val_mse: 19.2690 - val_mae: 3.4032\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5286 - mse: 19.4143 - mae: 3.4345 - val_loss: 19.1319 - val_mse: 19.0144 - val_mae: 3.3797\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2663 - mse: 19.1461 - mae: 3.4141 - val_loss: 18.8276 - val_mse: 18.7044 - val_mae: 3.3562\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.0750 - mse: 18.9493 - mae: 3.3921 - val_loss: 18.6331 - val_mse: 18.5051 - val_mae: 3.3339\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8711 - mse: 18.7410 - mae: 3.3770 - val_loss: 18.6250 - val_mse: 18.4926 - val_mae: 3.3267\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6397 - mse: 18.5052 - mae: 3.3589 - val_loss: 18.3484 - val_mse: 18.2118 - val_mae: 3.2997\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.5504 - mse: 18.4119 - mae: 3.3442 - val_loss: 18.2178 - val_mse: 18.0772 - val_mae: 3.2933\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3082 - mse: 18.1659 - mae: 3.3251 - val_loss: 18.2507 - val_mse: 18.1066 - val_mae: 3.2946\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.1373 - mse: 17.9916 - mae: 3.3079 - val_loss: 17.8413 - val_mse: 17.6940 - val_mae: 3.2540\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.0469 - mse: 17.8979 - mae: 3.3004 - val_loss: 17.8593 - val_mse: 17.7089 - val_mae: 3.2534\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.9735 - mse: 17.8216 - mae: 3.2933 - val_loss: 17.8166 - val_mse: 17.6632 - val_mae: 3.2519\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.8484 - mse: 17.6938 - mae: 3.2815 - val_loss: 17.7287 - val_mse: 17.5727 - val_mae: 3.2444\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.7550 - mse: 17.5981 - mae: 3.2738 - val_loss: 17.6411 - val_mse: 17.4830 - val_mae: 3.2368\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.6459 - mse: 17.4867 - mae: 3.2620 - val_loss: 17.6792 - val_mse: 17.5189 - val_mae: 3.2357\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.6585 - mse: 17.4973 - mae: 3.2571 - val_loss: 17.9376 - val_mse: 17.7756 - val_mae: 3.2547\n",
      "bias 0.0010436838\n",
      "si 0.45631993\n",
      "rmse 0.042161085\n",
      "kgeprime [0.7799687]\n",
      "rmse_95 0.07076549\n",
      "rmse_99 0.089376815\n",
      "pearson 0.8776823122676802\n",
      "pearson_95 0.6155149802467119\n",
      "pearson_99 0.6871804377285761\n",
      "rscore 0.7630670721920867\n",
      "rscore_95 -1.1955340408595951\n",
      "rscore_99 -11.999957222858393\n",
      "nse [0.76306707]\n",
      "nse_95 [-1.19553404]\n",
      "nse_99 [-11.99995722]\n",
      "kge [0.7576584]\n",
      "ext_kge_95 [0.52925416]\n",
      "ext_kge_99 [-0.05340496]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([37101 37125 37149], shape=(3,), dtype=int64) Times out: tf.Tensor(37149, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([33336 33360 33384], shape=(3,), dtype=int64) Times out: tf.Tensor(33384, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([37168 37192 37216], shape=(3,), dtype=int64) Times out: tf.Tensor(37216, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([55259 55283 55307], shape=(3,), dtype=int64) Times out: tf.Tensor(55307, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_295\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_296 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_590 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_591 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_295 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_590 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_295 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_591 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 27.1372 - mse: 27.0847 - mae: 4.0170 - val_loss: 21.3889 - val_mse: 21.3233 - val_mae: 3.5592\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.1085 - mse: 22.0368 - mae: 3.6622 - val_loss: 20.3708 - val_mse: 20.2905 - val_mae: 3.4684\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.2431 - mse: 21.1575 - mae: 3.5877 - val_loss: 20.0804 - val_mse: 19.9878 - val_mae: 3.4467\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.6191 - mse: 20.5222 - mae: 3.5332 - val_loss: 19.3142 - val_mse: 19.2118 - val_mae: 3.3768\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2507 - mse: 20.1452 - mae: 3.4993 - val_loss: 18.9256 - val_mse: 18.8158 - val_mae: 3.3408\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.9322 - mse: 19.8196 - mae: 3.4757 - val_loss: 18.3556 - val_mse: 18.2392 - val_mae: 3.2886\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.5455 - mse: 19.4265 - mae: 3.4402 - val_loss: 18.3216 - val_mse: 18.1996 - val_mae: 3.2814\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3877 - mse: 19.2635 - mae: 3.4254 - val_loss: 18.2036 - val_mse: 18.0766 - val_mae: 3.2680\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.1086 - mse: 18.9797 - mae: 3.3994 - val_loss: 17.7190 - val_mse: 17.5877 - val_mae: 3.2210\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.9201 - mse: 18.7868 - mae: 3.3850 - val_loss: 17.6234 - val_mse: 17.4877 - val_mae: 3.2157\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.7041 - mse: 18.5670 - mae: 3.3671 - val_loss: 17.6939 - val_mse: 17.5548 - val_mae: 3.2179\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.4804 - mse: 18.3399 - mae: 3.3477 - val_loss: 17.2860 - val_mse: 17.1435 - val_mae: 3.1750\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.4017 - mse: 18.2580 - mae: 3.3386 - val_loss: 17.1999 - val_mse: 17.0543 - val_mae: 3.1665\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.2671 - mse: 18.1203 - mae: 3.3287 - val_loss: 17.3499 - val_mse: 17.2014 - val_mae: 3.1842\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.2037 - mse: 18.0543 - mae: 3.3190 - val_loss: 17.2753 - val_mse: 17.1245 - val_mae: 3.1745\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.0808 - mse: 17.9293 - mae: 3.3065 - val_loss: 16.8727 - val_mse: 16.7198 - val_mae: 3.1347\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.9943 - mse: 17.8406 - mae: 3.3011 - val_loss: 17.3443 - val_mse: 17.1895 - val_mae: 3.1801\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.0104 - mse: 17.8548 - mae: 3.3021 - val_loss: 17.1254 - val_mse: 16.9684 - val_mae: 3.1621\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.8718 - mse: 17.7143 - mae: 3.2861 - val_loss: 16.9821 - val_mse: 16.8232 - val_mae: 3.1483\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.8639 - mse: 17.7047 - mae: 3.2871 - val_loss: 17.1190 - val_mse: 16.9586 - val_mae: 3.1581\n",
      "bias 0.006054591\n",
      "si 0.4703076\n",
      "rmse 0.041180775\n",
      "kgeprime [0.75378757]\n",
      "rmse_95 0.07813559\n",
      "rmse_99 0.1194641\n",
      "pearson 0.8654564389210712\n",
      "pearson_95 0.4174881980601214\n",
      "pearson_99 0.20873040342718238\n",
      "rscore 0.7426736956849931\n",
      "rscore_95 -2.140377857709252\n",
      "rscore_99 -17.118618719827946\n",
      "nse [0.7426737]\n",
      "nse_95 [-2.14037786]\n",
      "nse_99 [-17.11861872]\n",
      "kge [0.7089115]\n",
      "ext_kge_95 [0.29594101]\n",
      "ext_kge_99 [-1.06216156]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([2699 2723 2747], shape=(3,), dtype=int64) Times out: tf.Tensor(2747, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34179 34203 34227], shape=(3,), dtype=int64) Times out: tf.Tensor(34227, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([27491 27515 27539], shape=(3,), dtype=int64) Times out: tf.Tensor(27539, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([5716 5740 5764], shape=(3,), dtype=int64) Times out: tf.Tensor(5764, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_296\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_297 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_592 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_593 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_296 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_592 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_296 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_593 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 30.5197 - mse: 30.4682 - mae: 4.2211 - val_loss: 23.0340 - val_mse: 22.9713 - val_mae: 3.7675\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5318 - mse: 22.4655 - mae: 3.6766 - val_loss: 22.5541 - val_mse: 22.4843 - val_mae: 3.7139\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8039 - mse: 21.7306 - mae: 3.6183 - val_loss: 22.4728 - val_mse: 22.3956 - val_mae: 3.6941\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3098 - mse: 21.2286 - mae: 3.5743 - val_loss: 22.2244 - val_mse: 22.1392 - val_mae: 3.6676\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.9743 - mse: 20.8855 - mae: 3.5476 - val_loss: 21.9448 - val_mse: 21.8525 - val_mae: 3.6446\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.6452 - mse: 20.5495 - mae: 3.5215 - val_loss: 21.5660 - val_mse: 21.4673 - val_mae: 3.6144\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.3821 - mse: 20.2804 - mae: 3.4998 - val_loss: 21.5341 - val_mse: 21.4297 - val_mae: 3.6065\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.1910 - mse: 20.0841 - mae: 3.4821 - val_loss: 21.2310 - val_mse: 21.1219 - val_mae: 3.5823\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.8572 - mse: 19.7455 - mae: 3.4525 - val_loss: 21.2055 - val_mse: 21.0916 - val_mae: 3.5719\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6440 - mse: 19.5282 - mae: 3.4327 - val_loss: 20.8198 - val_mse: 20.7023 - val_mae: 3.5413\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4037 - mse: 19.2845 - mae: 3.4123 - val_loss: 20.6623 - val_mse: 20.5416 - val_mae: 3.5285\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2716 - mse: 19.1495 - mae: 3.4007 - val_loss: 20.6699 - val_mse: 20.5463 - val_mae: 3.5236\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.0265 - mse: 18.9016 - mae: 3.3806 - val_loss: 20.1884 - val_mse: 20.0625 - val_mae: 3.4915\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8660 - mse: 18.7385 - mae: 3.3636 - val_loss: 20.1510 - val_mse: 20.0222 - val_mae: 3.4821\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7959 - mse: 18.6656 - mae: 3.3610 - val_loss: 20.6821 - val_mse: 20.5505 - val_mae: 3.5121\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7049 - mse: 18.5720 - mae: 3.3474 - val_loss: 20.3246 - val_mse: 20.1907 - val_mae: 3.4869\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.5957 - mse: 18.4606 - mae: 3.3395 - val_loss: 20.0690 - val_mse: 19.9328 - val_mae: 3.4679\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.4408 - mse: 18.3033 - mae: 3.3275 - val_loss: 20.6640 - val_mse: 20.5255 - val_mae: 3.5066\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3231 - mse: 18.1836 - mae: 3.3173 - val_loss: 20.0105 - val_mse: 19.8699 - val_mae: 3.4581\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3629 - mse: 18.2213 - mae: 3.3168 - val_loss: 20.0382 - val_mse: 19.8955 - val_mae: 3.4633\n",
      "bias -0.0066005555\n",
      "si 0.5307155\n",
      "rmse 0.044604383\n",
      "kgeprime [0.59839492]\n",
      "rmse_95 0.05804095\n",
      "rmse_99 0.062696576\n",
      "pearson 0.8279192117516572\n",
      "pearson_95 0.5493987688998271\n",
      "pearson_99 0.6358581357145502\n",
      "rscore 0.6743625915661525\n",
      "rscore_95 -1.5266745930169274\n",
      "rscore_99 -2.7164873262708924\n",
      "nse [0.67436259]\n",
      "nse_95 [-1.52667459]\n",
      "nse_99 [-2.71648733]\n",
      "kge [0.69009862]\n",
      "ext_kge_95 [0.31376838]\n",
      "ext_kge_99 [0.09372026]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([87599 87623 87647], shape=(3,), dtype=int64) Times out: tf.Tensor(87647, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([130303 130327 130351], shape=(3,), dtype=int64) Times out: tf.Tensor(130351, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([76537 76561 76585], shape=(3,), dtype=int64) Times out: tf.Tensor(76585, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([48658 48682 48706], shape=(3,), dtype=int64) Times out: tf.Tensor(48706, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_297\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_298 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_594 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_595 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_297 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_594 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_297 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_595 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.3500 - mse: 37.3055 - mae: 4.6620 - val_loss: 22.9861 - val_mse: 22.9304 - val_mae: 3.7011\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.6149 - mse: 25.5556 - mae: 3.9015 - val_loss: 20.6128 - val_mse: 20.5501 - val_mae: 3.5181\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.4840 - mse: 24.4184 - mae: 3.8154 - val_loss: 20.9871 - val_mse: 20.9189 - val_mae: 3.5548\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.1161 - mse: 24.0454 - mae: 3.7877 - val_loss: 20.5769 - val_mse: 20.5040 - val_mae: 3.5176\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.6728 - mse: 23.5978 - mae: 3.7570 - val_loss: 20.5972 - val_mse: 20.5202 - val_mae: 3.5234\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.5131 - mse: 23.4340 - mae: 3.7477 - val_loss: 19.8700 - val_mse: 19.7889 - val_mae: 3.4572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.2862 - mse: 23.2034 - mae: 3.7254 - val_loss: 19.9729 - val_mse: 19.8883 - val_mae: 3.4706\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.1886 - mse: 23.1025 - mae: 3.7189 - val_loss: 19.9349 - val_mse: 19.8475 - val_mae: 3.4646\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0458 - mse: 22.9567 - mae: 3.7011 - val_loss: 19.4857 - val_mse: 19.3953 - val_mae: 3.4260\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.8219 - mse: 22.7301 - mae: 3.6911 - val_loss: 19.1229 - val_mse: 19.0296 - val_mae: 3.3976\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7607 - mse: 22.6663 - mae: 3.6862 - val_loss: 19.6875 - val_mse: 19.5920 - val_mae: 3.4417\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6963 - mse: 22.5997 - mae: 3.6775 - val_loss: 18.9730 - val_mse: 18.8754 - val_mae: 3.3829\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.5533 - mse: 22.4545 - mae: 3.6614 - val_loss: 18.9691 - val_mse: 18.8691 - val_mae: 3.3820\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4642 - mse: 22.3633 - mae: 3.6601 - val_loss: 18.9422 - val_mse: 18.8403 - val_mae: 3.3804\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4137 - mse: 22.3107 - mae: 3.6571 - val_loss: 18.5715 - val_mse: 18.4674 - val_mae: 3.3523\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2616 - mse: 22.1566 - mae: 3.6391 - val_loss: 18.9321 - val_mse: 18.8263 - val_mae: 3.3827\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2526 - mse: 22.1456 - mae: 3.6375 - val_loss: 18.4032 - val_mse: 18.2950 - val_mae: 3.3395\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2248 - mse: 22.1155 - mae: 3.6310 - val_loss: 18.3444 - val_mse: 18.2339 - val_mae: 3.3364\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0007 - mse: 21.8892 - mae: 3.6158 - val_loss: 18.0546 - val_mse: 17.9419 - val_mae: 3.3124\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9147 - mse: 21.8010 - mae: 3.6091 - val_loss: 17.9380 - val_mse: 17.8234 - val_mae: 3.3022\n",
      "bias 0.004414164\n",
      "si 0.48782584\n",
      "rmse 0.042217717\n",
      "kgeprime [0.77947848]\n",
      "rmse_95 0.075615436\n",
      "rmse_99 0.08448879\n",
      "pearson 0.8543451447633056\n",
      "pearson_95 0.7136814833079461\n",
      "pearson_99 0.6788056568504887\n",
      "rscore 0.7218923786846665\n",
      "rscore_95 -1.973143549766025\n",
      "rscore_99 -3.916080341766566\n",
      "nse [0.72189238]\n",
      "nse_95 [-1.97314355]\n",
      "nse_99 [-3.91608034]\n",
      "kge [0.70427733]\n",
      "ext_kge_95 [0.54712662]\n",
      "ext_kge_99 [0.39560968]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -44.8 -44.49 -44.18 ... -38.25 -37.94\n",
      "  * longitude       (longitude) float32 171.6 171.9 172.2 ... 177.5 177.8 178.1\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 5.6 5.339 ... 1.592\n",
      "    vgrd10m         (time, latitude, longitude) float32 4.828 5.048 ... 1.064\n",
      "    uw2             (time, latitude, longitude) float32 31.36 28.5 ... 2.533\n",
      "    vw2             (time, latitude, longitude) float32 23.31 25.49 ... 1.132\n",
      "    wind_magnitude  (time, latitude, longitude) float32 7.394 7.348 ... 1.914\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([86353 86377 86401], shape=(3,), dtype=int64) Times out: tf.Tensor(86401, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([49440 49464 49488], shape=(3,), dtype=int64) Times out: tf.Tensor(49488, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([30705 30729 30753], shape=(3,), dtype=int64) Times out: tf.Tensor(30753, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([18191 18215 18239], shape=(3,), dtype=int64) Times out: tf.Tensor(18239, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_298\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_299 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_596 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_597 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_298 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_596 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_298 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_597 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 33.1660 - mse: 33.1129 - mae: 4.4885 - val_loss: 24.1877 - val_mse: 24.1228 - val_mae: 3.8875\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 27.1625 - mse: 27.0902 - mae: 4.1021 - val_loss: 23.4108 - val_mse: 23.3317 - val_mae: 3.8314\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.3098 - mse: 26.2257 - mae: 4.0299 - val_loss: 23.3732 - val_mse: 23.2843 - val_mae: 3.8304\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.7152 - mse: 25.6220 - mae: 3.9865 - val_loss: 22.8494 - val_mse: 22.7524 - val_mae: 3.7740\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.3579 - mse: 25.2573 - mae: 3.9585 - val_loss: 23.2069 - val_mse: 23.1030 - val_mae: 3.8060\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.0995 - mse: 24.9922 - mae: 3.9315 - val_loss: 22.4334 - val_mse: 22.3231 - val_mae: 3.7344\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.6822 - mse: 24.5690 - mae: 3.8992 - val_loss: 22.2629 - val_mse: 22.1472 - val_mae: 3.7189\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.3428 - mse: 24.2245 - mae: 3.8723 - val_loss: 22.0947 - val_mse: 21.9743 - val_mae: 3.7061\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.1125 - mse: 23.9901 - mae: 3.8502 - val_loss: 21.8622 - val_mse: 21.7378 - val_mae: 3.6847\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.8153 - mse: 23.6889 - mae: 3.8268 - val_loss: 21.6457 - val_mse: 21.5176 - val_mae: 3.6621\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.5941 - mse: 23.4642 - mae: 3.8088 - val_loss: 21.5074 - val_mse: 21.3762 - val_mae: 3.6481\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.3647 - mse: 23.2319 - mae: 3.7896 - val_loss: 21.2977 - val_mse: 21.1638 - val_mae: 3.6311\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.2171 - mse: 23.0818 - mae: 3.7814 - val_loss: 21.1931 - val_mse: 21.0567 - val_mae: 3.6225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0878 - mse: 22.9503 - mae: 3.7699 - val_loss: 21.0559 - val_mse: 20.9177 - val_mae: 3.6088\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.9788 - mse: 22.8396 - mae: 3.7589 - val_loss: 21.3860 - val_mse: 21.2460 - val_mae: 3.6386\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.9130 - mse: 22.7718 - mae: 3.7530 - val_loss: 21.4203 - val_mse: 21.2783 - val_mae: 3.6432\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.8938 - mse: 22.7508 - mae: 3.7516 - val_loss: 20.8440 - val_mse: 20.7004 - val_mae: 3.5919\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7910 - mse: 22.6464 - mae: 3.7450 - val_loss: 21.1527 - val_mse: 21.0074 - val_mae: 3.6186\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7544 - mse: 22.6082 - mae: 3.7397 - val_loss: 21.5832 - val_mse: 21.4363 - val_mae: 3.6598\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6889 - mse: 22.5410 - mae: 3.7315 - val_loss: 20.9905 - val_mse: 20.8418 - val_mae: 3.6028\n",
      "bias -0.0060310997\n",
      "si 0.5050272\n",
      "rmse 0.04565283\n",
      "kgeprime [0.61102223]\n",
      "rmse_95 0.07303242\n",
      "rmse_99 0.07735202\n",
      "pearson 0.8402750816001928\n",
      "pearson_95 0.4061057213806278\n",
      "pearson_99 0.197042191874348\n",
      "rscore 0.7006306404876934\n",
      "rscore_95 -3.8910884067039166\n",
      "rscore_99 -9.501615454346316\n",
      "nse [0.70063064]\n",
      "nse_95 [-3.89108841]\n",
      "nse_99 [-9.50161545]\n",
      "kge [0.70014445]\n",
      "ext_kge_95 [0.21597721]\n",
      "ext_kge_99 [0.15591843]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([114224 114248 114272], shape=(3,), dtype=int64) Times out: tf.Tensor(114272, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([88360 88384 88408], shape=(3,), dtype=int64) Times out: tf.Tensor(88408, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([60495 60519 60543], shape=(3,), dtype=int64) Times out: tf.Tensor(60543, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([10486 10510 10534], shape=(3,), dtype=int64) Times out: tf.Tensor(10534, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_299\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_300 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_598 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_599 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_299 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_598 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_299 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_599 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 30.9199 - mse: 30.8749 - mae: 4.3494 - val_loss: 26.4317 - val_mse: 26.3755 - val_mae: 4.0013\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.7385 - mse: 25.6766 - mae: 3.9968 - val_loss: 26.3530 - val_mse: 26.2867 - val_mae: 3.9860\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.2653 - mse: 25.1953 - mae: 3.9597 - val_loss: 25.0038 - val_mse: 24.9315 - val_mae: 3.8787\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.9774 - mse: 24.9025 - mae: 3.9326 - val_loss: 25.0682 - val_mse: 24.9916 - val_mae: 3.8819\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.8042 - mse: 24.7248 - mae: 3.9197 - val_loss: 24.8766 - val_mse: 24.7955 - val_mae: 3.8683\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.4405 - mse: 24.3565 - mae: 3.8901 - val_loss: 23.9836 - val_mse: 23.8976 - val_mae: 3.7971\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.0509 - mse: 23.9622 - mae: 3.8542 - val_loss: 23.3599 - val_mse: 23.2699 - val_mae: 3.7505\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.7075 - mse: 23.6148 - mae: 3.8268 - val_loss: 23.1138 - val_mse: 23.0197 - val_mae: 3.7282\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.3792 - mse: 23.2827 - mae: 3.7993 - val_loss: 22.5395 - val_mse: 22.4416 - val_mae: 3.6842\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.1553 - mse: 23.0551 - mae: 3.7821 - val_loss: 22.3178 - val_mse: 22.2163 - val_mae: 3.6645\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.8937 - mse: 22.7901 - mae: 3.7604 - val_loss: 22.0511 - val_mse: 21.9466 - val_mae: 3.6436\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.6400 - mse: 22.5334 - mae: 3.7400 - val_loss: 21.9277 - val_mse: 21.8203 - val_mae: 3.6346\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5846 - mse: 22.4753 - mae: 3.7307 - val_loss: 21.7308 - val_mse: 21.6207 - val_mae: 3.6192\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3805 - mse: 22.2688 - mae: 3.7182 - val_loss: 21.8251 - val_mse: 21.7126 - val_mae: 3.6266\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3784 - mse: 22.2642 - mae: 3.7155 - val_loss: 21.9180 - val_mse: 21.8028 - val_mae: 3.6327\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2221 - mse: 22.1054 - mae: 3.7034 - val_loss: 21.6090 - val_mse: 21.4917 - val_mae: 3.6089\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2359 - mse: 22.1169 - mae: 3.7066 - val_loss: 21.4730 - val_mse: 21.3531 - val_mae: 3.5994\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0261 - mse: 21.9047 - mae: 3.6850 - val_loss: 21.7856 - val_mse: 21.6635 - val_mae: 3.6261\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0148 - mse: 21.8913 - mae: 3.6869 - val_loss: 21.7042 - val_mse: 21.5799 - val_mae: 3.6189\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0171 - mse: 21.8916 - mae: 3.6874 - val_loss: 21.2369 - val_mse: 21.1104 - val_mae: 3.5792\n",
      "bias 0.003823238\n",
      "si 0.4709278\n",
      "rmse 0.045946047\n",
      "kgeprime [0.80416417]\n",
      "rmse_95 0.07522273\n",
      "rmse_99 0.10283606\n",
      "pearson 0.8685263237877008\n",
      "pearson_95 0.45961300532785754\n",
      "pearson_99 0.6429152423443597\n",
      "rscore 0.7488770135772405\n",
      "rscore_95 -4.236366161745548\n",
      "rscore_99 -13.964024478370913\n",
      "nse [0.74887701]\n",
      "nse_95 [-4.23636616]\n",
      "nse_99 [-13.96402448]\n",
      "kge [0.73865498]\n",
      "ext_kge_95 [0.26182813]\n",
      "ext_kge_99 [0.01290892]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([49939 49963 49987], shape=(3,), dtype=int64) Times out: tf.Tensor(49987, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([62220 62244 62268], shape=(3,), dtype=int64) Times out: tf.Tensor(62268, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([3984 4008 4032], shape=(3,), dtype=int64) Times out: tf.Tensor(4032, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([1566 1590 1614], shape=(3,), dtype=int64) Times out: tf.Tensor(1614, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_300\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_301 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_600 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_601 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_300 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_600 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_300 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_601 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 31.7363 - mse: 31.6924 - mae: 4.4192 - val_loss: 24.2674 - val_mse: 24.2143 - val_mae: 3.8451\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 26.0323 - mse: 25.9731 - mae: 4.0331 - val_loss: 23.4385 - val_mse: 23.3727 - val_mae: 3.7552\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 25.3467 - mse: 25.2770 - mae: 3.9723 - val_loss: 23.2142 - val_mse: 23.1398 - val_mae: 3.7364\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.9480 - mse: 24.8710 - mae: 3.9392 - val_loss: 22.7916 - val_mse: 22.7112 - val_mae: 3.6938\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.6238 - mse: 24.5410 - mae: 3.9132 - val_loss: 23.0832 - val_mse: 22.9976 - val_mae: 3.7219\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.2931 - mse: 24.2054 - mae: 3.8849 - val_loss: 22.4941 - val_mse: 22.4039 - val_mae: 3.6666\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.0299 - mse: 23.9379 - mae: 3.8641 - val_loss: 22.0750 - val_mse: 21.9805 - val_mae: 3.6223\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.8390 - mse: 23.7429 - mae: 3.8483 - val_loss: 22.0312 - val_mse: 21.9330 - val_mae: 3.6295\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.5041 - mse: 23.4042 - mae: 3.8222 - val_loss: 21.5479 - val_mse: 21.4461 - val_mae: 3.5773\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.3772 - mse: 23.2740 - mae: 3.8122 - val_loss: 21.7775 - val_mse: 21.6723 - val_mae: 3.6080\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.0597 - mse: 22.9532 - mae: 3.7831 - val_loss: 21.2976 - val_mse: 21.1894 - val_mae: 3.5555\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.9500 - mse: 22.8403 - mae: 3.7744 - val_loss: 21.0499 - val_mse: 20.9383 - val_mae: 3.5275\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.8530 - mse: 22.7403 - mae: 3.7685 - val_loss: 21.1095 - val_mse: 20.9953 - val_mae: 3.5345\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.7492 - mse: 22.6341 - mae: 3.7578 - val_loss: 20.9865 - val_mse: 20.8700 - val_mae: 3.5209\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.7074 - mse: 22.5899 - mae: 3.7537 - val_loss: 20.9993 - val_mse: 20.8805 - val_mae: 3.5187\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.5329 - mse: 22.4132 - mae: 3.7401 - val_loss: 21.1931 - val_mse: 21.0721 - val_mae: 3.5427\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.3762 - mse: 22.2543 - mae: 3.7284 - val_loss: 21.0720 - val_mse: 20.9491 - val_mae: 3.5246\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.4515 - mse: 22.3277 - mae: 3.7335 - val_loss: 21.0285 - val_mse: 20.9035 - val_mae: 3.5214\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.3748 - mse: 22.2489 - mae: 3.7290 - val_loss: 21.1213 - val_mse: 20.9943 - val_mae: 3.5356\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.3997 - mse: 22.2720 - mae: 3.7291 - val_loss: 20.8801 - val_mse: 20.7514 - val_mae: 3.5054\n",
      "bias -0.0011895706\n",
      "si 0.49897212\n",
      "rmse 0.04555369\n",
      "kgeprime [0.7522921]\n",
      "rmse_95 0.070945166\n",
      "rmse_99 0.12461723\n",
      "pearson 0.846723642827358\n",
      "pearson_95 0.20420296804463553\n",
      "pearson_99 0.05122517251175708\n",
      "rscore 0.7166945764245665\n",
      "rscore_95 -2.6071707088188565\n",
      "rscore_99 -16.052106980758488\n",
      "nse [0.71669458]\n",
      "nse_95 [-2.60717071]\n",
      "nse_99 [-16.05210698]\n",
      "kge [0.77486794]\n",
      "ext_kge_95 [0.16268286]\n",
      "ext_kge_99 [-0.49470023]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([4648 4672 4696], shape=(3,), dtype=int64) Times out: tf.Tensor(4696, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34148 34172 34196], shape=(3,), dtype=int64) Times out: tf.Tensor(34196, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([7332 7356 7380], shape=(3,), dtype=int64) Times out: tf.Tensor(7380, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([29871 29895 29919], shape=(3,), dtype=int64) Times out: tf.Tensor(29919, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_301\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_302 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_602 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_603 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_301 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_602 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_301 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_603 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 35.7386 - mse: 35.6875 - mae: 4.6270 - val_loss: 26.5372 - val_mse: 26.4725 - val_mae: 4.1004\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 27.6084 - mse: 27.5396 - mae: 4.1159 - val_loss: 25.6002 - val_mse: 25.5283 - val_mae: 4.0001\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 27.1892 - mse: 27.1162 - mae: 4.0786 - val_loss: 25.3963 - val_mse: 25.3218 - val_mae: 3.9745\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 26.7979 - mse: 26.7225 - mae: 4.0543 - val_loss: 25.3336 - val_mse: 25.2567 - val_mae: 3.9629\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 26.6807 - mse: 26.6029 - mae: 4.0431 - val_loss: 25.1365 - val_mse: 25.0574 - val_mae: 3.9485\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 26.4655 - mse: 26.3853 - mae: 4.0279 - val_loss: 24.9641 - val_mse: 24.8825 - val_mae: 3.9391\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 26.2922 - mse: 26.2096 - mae: 4.0135 - val_loss: 24.8235 - val_mse: 24.7394 - val_mae: 3.9286\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 26.1433 - mse: 26.0581 - mae: 4.0001 - val_loss: 24.6559 - val_mse: 24.5693 - val_mae: 3.9126\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 26.0605 - mse: 25.9728 - mae: 3.9918 - val_loss: 24.5560 - val_mse: 24.4668 - val_mae: 3.9012\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.8002 - mse: 25.7103 - mae: 3.9755 - val_loss: 24.4014 - val_mse: 24.3101 - val_mae: 3.8932\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.6372 - mse: 25.5449 - mae: 3.9654 - val_loss: 24.4687 - val_mse: 24.3748 - val_mae: 3.8917\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.5503 - mse: 25.4556 - mae: 3.9588 - val_loss: 24.1450 - val_mse: 24.0490 - val_mae: 3.8707\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.4480 - mse: 25.3509 - mae: 3.9463 - val_loss: 24.0726 - val_mse: 23.9740 - val_mae: 3.8677\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.2551 - mse: 25.1557 - mae: 3.9292 - val_loss: 24.0394 - val_mse: 23.9387 - val_mae: 3.8571\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.1607 - mse: 25.0588 - mae: 3.9272 - val_loss: 24.1625 - val_mse: 24.0593 - val_mae: 3.8599\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.0935 - mse: 24.9893 - mae: 3.9168 - val_loss: 23.8763 - val_mse: 23.7707 - val_mae: 3.8438\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.0211 - mse: 24.9146 - mae: 3.9093 - val_loss: 23.7770 - val_mse: 23.6693 - val_mae: 3.8302\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.8813 - mse: 24.7724 - mae: 3.9019 - val_loss: 23.6501 - val_mse: 23.5400 - val_mae: 3.8253\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.7275 - mse: 24.6166 - mae: 3.8878 - val_loss: 23.5582 - val_mse: 23.4462 - val_mae: 3.8191\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.7491 - mse: 24.6361 - mae: 3.8883 - val_loss: 23.4859 - val_mse: 23.3717 - val_mae: 3.8183\n",
      "bias 0.0012942132\n",
      "si 0.5451524\n",
      "rmse 0.04834433\n",
      "kgeprime [0.76381245]\n",
      "rmse_95 0.07672051\n",
      "rmse_99 0.08566679\n",
      "pearson 0.8152682821159137\n",
      "pearson_95 0.48276364305888986\n",
      "pearson_99 0.16452695287228558\n",
      "rscore 0.6643743936108988\n",
      "rscore_95 -6.2174988857743045\n",
      "rscore_99 -13.987582262525972\n",
      "nse [0.66437439]\n",
      "nse_95 [-6.21749889]\n",
      "nse_99 [-13.98758226]\n",
      "kge [0.73995059]\n",
      "ext_kge_95 [0.15387004]\n",
      "ext_kge_99 [-0.19632318]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([82173 82197 82221], shape=(3,), dtype=int64) Times out: tf.Tensor(82221, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([56529 56553 56577], shape=(3,), dtype=int64) Times out: tf.Tensor(56577, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([185043 185067 185091], shape=(3,), dtype=int64) Times out: tf.Tensor(185091, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([189344 189368 189392], shape=(3,), dtype=int64) Times out: tf.Tensor(189392, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_302\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_303 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_604 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_605 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_302 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_604 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_302 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_605 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 34.0970 - mse: 34.0500 - mae: 4.5317 - val_loss: 24.0844 - val_mse: 24.0280 - val_mae: 3.9088\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 27.2006 - mse: 27.1418 - mae: 4.0899 - val_loss: 23.9560 - val_mse: 23.8957 - val_mae: 3.9041\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.8719 - mse: 26.8107 - mae: 4.0601 - val_loss: 23.3584 - val_mse: 23.2960 - val_mae: 3.8485\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.7101 - mse: 26.6468 - mae: 4.0454 - val_loss: 23.2281 - val_mse: 23.1637 - val_mae: 3.8413\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.5010 - mse: 26.4355 - mae: 4.0313 - val_loss: 23.3796 - val_mse: 23.3129 - val_mae: 3.8563\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.3780 - mse: 26.3102 - mae: 4.0186 - val_loss: 22.9356 - val_mse: 22.8666 - val_mae: 3.8138\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.3264 - mse: 26.2561 - mae: 4.0138 - val_loss: 22.8392 - val_mse: 22.7676 - val_mae: 3.8116\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.0926 - mse: 26.0195 - mae: 3.9941 - val_loss: 22.5453 - val_mse: 22.4705 - val_mae: 3.7839\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.7844 - mse: 25.7081 - mae: 3.9766 - val_loss: 22.5577 - val_mse: 22.4797 - val_mae: 3.7918\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.8040 - mse: 25.7243 - mae: 3.9721 - val_loss: 21.9782 - val_mse: 21.8965 - val_mae: 3.7350\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.4788 - mse: 25.3951 - mae: 3.9446 - val_loss: 21.7152 - val_mse: 21.6298 - val_mae: 3.7173\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.1949 - mse: 25.1076 - mae: 3.9258 - val_loss: 21.7366 - val_mse: 21.6475 - val_mae: 3.7241\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.0109 - mse: 24.9201 - mae: 3.9038 - val_loss: 21.2567 - val_mse: 21.1642 - val_mae: 3.6817\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.8027 - mse: 24.7084 - mae: 3.8871 - val_loss: 21.0809 - val_mse: 20.9850 - val_mae: 3.6738\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.6245 - mse: 24.5268 - mae: 3.8769 - val_loss: 20.8582 - val_mse: 20.7590 - val_mae: 3.6486\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.4841 - mse: 24.3835 - mae: 3.8583 - val_loss: 20.6370 - val_mse: 20.5346 - val_mae: 3.6341\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.2975 - mse: 24.1939 - mae: 3.8463 - val_loss: 20.8287 - val_mse: 20.7239 - val_mae: 3.6579\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.2849 - mse: 24.1786 - mae: 3.8429 - val_loss: 20.7088 - val_mse: 20.6011 - val_mae: 3.6357\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.2325 - mse: 24.1238 - mae: 3.8405 - val_loss: 20.5437 - val_mse: 20.4341 - val_mae: 3.6329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.1190 - mse: 24.0081 - mae: 3.8246 - val_loss: 20.4854 - val_mse: 20.3734 - val_mae: 3.6263\n",
      "bias 0.0017646408\n",
      "si 0.5005164\n",
      "rmse 0.04513685\n",
      "kgeprime [0.77645898]\n",
      "rmse_95 0.066529706\n",
      "rmse_99 0.080622524\n",
      "pearson 0.8448408361312462\n",
      "pearson_95 0.6083295664543934\n",
      "pearson_99 0.42945134620345493\n",
      "rscore 0.7115192093570184\n",
      "rscore_95 -2.4250886906929083\n",
      "rscore_99 -5.1615972379905175\n",
      "nse [0.71151921]\n",
      "nse_95 [-2.42508869]\n",
      "nse_99 [-5.16159724]\n",
      "kge [0.74301922]\n",
      "ext_kge_95 [0.49807398]\n",
      "ext_kge_99 [0.35024043]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -44.49 -44.18 -43.87 ... -38.25 -37.94\n",
      "  * longitude       (longitude) float32 170.0 170.3 170.6 ... 175.9 176.2 176.6\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -3.16 -2.209 ... 0.2841\n",
      "    vgrd10m         (time, latitude, longitude) float32 2.888 2.888 ... 2.363\n",
      "    uw2             (time, latitude, longitude) float32 9.984 4.88 ... 0.08074\n",
      "    vw2             (time, latitude, longitude) float32 8.343 8.343 ... 5.583\n",
      "    wind_magnitude  (time, latitude, longitude) float32 4.281 3.636 ... 2.38\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([76223 76247 76271], shape=(3,), dtype=int64) Times out: tf.Tensor(76271, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([2792 2816 2840], shape=(3,), dtype=int64) Times out: tf.Tensor(2840, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([16084 16108 16132], shape=(3,), dtype=int64) Times out: tf.Tensor(16132, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([79913 79937 79961], shape=(3,), dtype=int64) Times out: tf.Tensor(79961, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_303\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_304 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_606 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_607 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_303 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_606 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_303 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_607 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 25.2863 - mse: 25.2365 - mae: 3.9354 - val_loss: 20.4409 - val_mse: 20.3763 - val_mae: 3.5059\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8821 - mse: 20.8104 - mae: 3.5836 - val_loss: 19.2114 - val_mse: 19.1328 - val_mae: 3.4236\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.2427 - mse: 20.1596 - mae: 3.5299 - val_loss: 18.7415 - val_mse: 18.6543 - val_mae: 3.3843\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9194 - mse: 19.8296 - mae: 3.4963 - val_loss: 18.5030 - val_mse: 18.4106 - val_mae: 3.3762\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6883 - mse: 19.5942 - mae: 3.4733 - val_loss: 18.3506 - val_mse: 18.2543 - val_mae: 3.3612\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5453 - mse: 19.4473 - mae: 3.4579 - val_loss: 18.2275 - val_mse: 18.1278 - val_mae: 3.3429\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.4300 - mse: 19.3292 - mae: 3.4477 - val_loss: 18.2205 - val_mse: 18.1180 - val_mae: 3.3418\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3362 - mse: 19.2326 - mae: 3.4393 - val_loss: 18.1304 - val_mse: 18.0254 - val_mae: 3.3313\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.1635 - mse: 19.0576 - mae: 3.4261 - val_loss: 18.0738 - val_mse: 17.9666 - val_mae: 3.3288\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0504 - mse: 18.9423 - mae: 3.4157 - val_loss: 18.0958 - val_mse: 17.9864 - val_mae: 3.3314\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.9768 - mse: 18.8665 - mae: 3.4075 - val_loss: 17.9691 - val_mse: 17.8574 - val_mae: 3.3230\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8760 - mse: 18.7634 - mae: 3.3998 - val_loss: 17.9288 - val_mse: 17.8149 - val_mae: 3.3116\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.7796 - mse: 18.6647 - mae: 3.3898 - val_loss: 17.9664 - val_mse: 17.8504 - val_mae: 3.3262\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6676 - mse: 18.5506 - mae: 3.3817 - val_loss: 17.9270 - val_mse: 17.8088 - val_mae: 3.3189\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6298 - mse: 18.5108 - mae: 3.3752 - val_loss: 17.9183 - val_mse: 17.7982 - val_mae: 3.3263\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6129 - mse: 18.4921 - mae: 3.3750 - val_loss: 17.9705 - val_mse: 17.8485 - val_mae: 3.3223\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.4771 - mse: 18.3542 - mae: 3.3643 - val_loss: 17.8511 - val_mse: 17.7270 - val_mae: 3.3221\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.4415 - mse: 18.3164 - mae: 3.3589 - val_loss: 17.8519 - val_mse: 17.7256 - val_mae: 3.3226\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.3280 - mse: 18.2008 - mae: 3.3490 - val_loss: 17.7890 - val_mse: 17.6606 - val_mae: 3.3116\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.3044 - mse: 18.1752 - mae: 3.3457 - val_loss: 17.6783 - val_mse: 17.5478 - val_mae: 3.3043\n",
      "bias -0.001322398\n",
      "si 0.59217453\n",
      "rmse 0.041890115\n",
      "kgeprime [0.63491689]\n",
      "rmse_95 0.07193254\n",
      "rmse_99 0.106631674\n",
      "pearson 0.7804659973727465\n",
      "pearson_95 0.45279575410102085\n",
      "pearson_99 0.0027850491363344333\n",
      "rscore 0.6081115120789216\n",
      "rscore_95 -1.9773515702583664\n",
      "rscore_99 -13.579768597757049\n",
      "nse [0.60811151]\n",
      "nse_95 [-1.97735157]\n",
      "nse_99 [-13.5797686]\n",
      "kge [0.66631206]\n",
      "ext_kge_95 [0.36353533]\n",
      "ext_kge_99 [-0.09811253]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([111705 111729 111753], shape=(3,), dtype=int64) Times out: tf.Tensor(111753, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([115662 115686 115710], shape=(3,), dtype=int64) Times out: tf.Tensor(115710, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77878 77902 77926], shape=(3,), dtype=int64) Times out: tf.Tensor(77926, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([71041 71065 71089], shape=(3,), dtype=int64) Times out: tf.Tensor(71089, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_304\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_305 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_608 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_609 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_304 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_608 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_304 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_609 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.6898 - mse: 25.6500 - mae: 3.9482 - val_loss: 22.8161 - val_mse: 22.7659 - val_mae: 3.6969\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.0489 - mse: 20.9969 - mae: 3.5948 - val_loss: 21.5451 - val_mse: 21.4893 - val_mae: 3.6029\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.4106 - mse: 20.3533 - mae: 3.5419 - val_loss: 21.3223 - val_mse: 21.2618 - val_mae: 3.5846\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.1101 - mse: 20.0488 - mae: 3.5162 - val_loss: 21.1887 - val_mse: 21.1248 - val_mae: 3.5797\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.9229 - mse: 19.8586 - mae: 3.5032 - val_loss: 20.8171 - val_mse: 20.7507 - val_mae: 3.5487\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.7485 - mse: 19.6815 - mae: 3.4869 - val_loss: 21.1337 - val_mse: 21.0649 - val_mae: 3.5757\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6407 - mse: 19.5717 - mae: 3.4771 - val_loss: 20.4555 - val_mse: 20.3846 - val_mae: 3.5207\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.5300 - mse: 19.4590 - mae: 3.4632 - val_loss: 20.3904 - val_mse: 20.3177 - val_mae: 3.5128\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4963 - mse: 19.4236 - mae: 3.4627 - val_loss: 20.5632 - val_mse: 20.4889 - val_mae: 3.5281\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.4385 - mse: 19.3640 - mae: 3.4555 - val_loss: 20.2737 - val_mse: 20.1978 - val_mae: 3.5021\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.2919 - mse: 19.2158 - mae: 3.4422 - val_loss: 20.3614 - val_mse: 20.2838 - val_mae: 3.5102\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.2831 - mse: 19.2053 - mae: 3.4409 - val_loss: 20.2002 - val_mse: 20.1207 - val_mae: 3.4972\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.1515 - mse: 19.0718 - mae: 3.4306 - val_loss: 20.3982 - val_mse: 20.3169 - val_mae: 3.5115\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.1192 - mse: 19.0377 - mae: 3.4289 - val_loss: 20.1858 - val_mse: 20.1028 - val_mae: 3.4945\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.0653 - mse: 18.9821 - mae: 3.4203 - val_loss: 20.3559 - val_mse: 20.2713 - val_mae: 3.5071\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.9898 - mse: 18.9048 - mae: 3.4168 - val_loss: 19.9779 - val_mse: 19.8915 - val_mae: 3.4760\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.9694 - mse: 18.8827 - mae: 3.4114 - val_loss: 20.0115 - val_mse: 19.9233 - val_mae: 3.4786\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.8448 - mse: 18.7561 - mae: 3.4023 - val_loss: 20.2800 - val_mse: 20.1898 - val_mae: 3.4991\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.7693 - mse: 18.6785 - mae: 3.3960 - val_loss: 19.9510 - val_mse: 19.8585 - val_mae: 3.4692\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.6846 - mse: 18.5917 - mae: 3.3871 - val_loss: 20.2188 - val_mse: 20.1245 - val_mae: 3.4910\n",
      "bias 0.004748993\n",
      "si 0.57742697\n",
      "rmse 0.0448603\n",
      "kgeprime [0.70117838]\n",
      "rmse_95 0.0802619\n",
      "rmse_99 0.0984533\n",
      "pearson 0.7968231262893422\n",
      "pearson_95 0.30121928972428663\n",
      "pearson_99 0.1276688391761331\n",
      "rscore 0.6254812204145894\n",
      "rscore_95 -6.684936771625355\n",
      "rscore_99 -31.752220222270672\n",
      "nse [0.62548122]\n",
      "nse_95 [-6.68493677]\n",
      "nse_99 [-31.75222022]\n",
      "kge [0.60607769]\n",
      "ext_kge_95 [0.02614558]\n",
      "ext_kge_99 [-0.78889734]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([11544 11568 11592], shape=(3,), dtype=int64) Times out: tf.Tensor(11592, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([26096 26120 26144], shape=(3,), dtype=int64) Times out: tf.Tensor(26144, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34558 34582 34606], shape=(3,), dtype=int64) Times out: tf.Tensor(34606, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([39800 39824 39848], shape=(3,), dtype=int64) Times out: tf.Tensor(39848, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_305\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_306 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_610 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_611 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_305 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_610 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_305 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_611 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.5622 - mse: 26.5248 - mae: 4.0236 - val_loss: 19.4388 - val_mse: 19.3884 - val_mae: 3.3957\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.4509 - mse: 21.3964 - mae: 3.6390 - val_loss: 18.7013 - val_mse: 18.6406 - val_mae: 3.3316\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.7386 - mse: 20.6759 - mae: 3.5781 - val_loss: 18.4840 - val_mse: 18.4167 - val_mae: 3.3114\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 20.4559 - mse: 20.3873 - mae: 3.5530 - val_loss: 18.2440 - val_mse: 18.1710 - val_mae: 3.2863\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 20.1526 - mse: 20.0781 - mae: 3.5233 - val_loss: 18.1219 - val_mse: 18.0432 - val_mae: 3.2799\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.9911 - mse: 19.9116 - mae: 3.5116 - val_loss: 18.2290 - val_mse: 18.1463 - val_mae: 3.2935\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.8800 - mse: 19.7968 - mae: 3.5000 - val_loss: 18.0844 - val_mse: 17.9983 - val_mae: 3.2787\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.7612 - mse: 19.6746 - mae: 3.4883 - val_loss: 18.1756 - val_mse: 18.0865 - val_mae: 3.2903\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.6410 - mse: 19.5514 - mae: 3.4783 - val_loss: 18.0042 - val_mse: 17.9123 - val_mae: 3.2739\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.5053 - mse: 19.4130 - mae: 3.4677 - val_loss: 17.9944 - val_mse: 17.9002 - val_mae: 3.2724\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.4174 - mse: 19.3229 - mae: 3.4607 - val_loss: 17.9712 - val_mse: 17.8750 - val_mae: 3.2724\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3852 - mse: 19.2886 - mae: 3.4553 - val_loss: 18.0290 - val_mse: 17.9307 - val_mae: 3.2836\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2850 - mse: 19.1864 - mae: 3.4454 - val_loss: 17.9635 - val_mse: 17.8633 - val_mae: 3.2724\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.1812 - mse: 19.0806 - mae: 3.4371 - val_loss: 17.8827 - val_mse: 17.7804 - val_mae: 3.2671\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.1832 - mse: 19.0804 - mae: 3.4364 - val_loss: 17.8517 - val_mse: 17.7474 - val_mae: 3.2646\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.0456 - mse: 18.9409 - mae: 3.4237 - val_loss: 17.7580 - val_mse: 17.6519 - val_mae: 3.2565\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.9536 - mse: 18.8470 - mae: 3.4167 - val_loss: 17.7485 - val_mse: 17.6405 - val_mae: 3.2560\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.9141 - mse: 18.8057 - mae: 3.4119 - val_loss: 17.7785 - val_mse: 17.6689 - val_mae: 3.2606\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.8543 - mse: 18.7441 - mae: 3.4107 - val_loss: 17.6877 - val_mse: 17.5762 - val_mae: 3.2515\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.7545 - mse: 18.6425 - mae: 3.3994 - val_loss: 17.7949 - val_mse: 17.6817 - val_mae: 3.2618\n",
      "bias 0.003025545\n",
      "si 0.57271314\n",
      "rmse 0.042049576\n",
      "kgeprime [0.73887832]\n",
      "rmse_95 0.075886995\n",
      "rmse_99 0.08582593\n",
      "pearson 0.7990925094571308\n",
      "pearson_95 0.4734465742136489\n",
      "pearson_99 0.2773484378352886\n",
      "rscore 0.6363133964036489\n",
      "rscore_95 -5.566638245888676\n",
      "rscore_99 -15.620565833609195\n",
      "nse [0.6363134]\n",
      "nse_95 [-5.56663825]\n",
      "nse_99 [-15.62056583]\n",
      "kge [0.67305212]\n",
      "ext_kge_95 [0.22530128]\n",
      "ext_kge_99 [-0.16354218]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([11996 12020 12044], shape=(3,), dtype=int64) Times out: tf.Tensor(12044, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([15251 15275 15299], shape=(3,), dtype=int64) Times out: tf.Tensor(15299, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([13869 13893 13917], shape=(3,), dtype=int64) Times out: tf.Tensor(13917, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([3921 3945 3969], shape=(3,), dtype=int64) Times out: tf.Tensor(3969, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_306\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_307 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_612 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_613 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_306 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_612 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_306 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_613 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.1430 - mse: 26.0971 - mae: 3.9677 - val_loss: 23.1020 - val_mse: 23.0431 - val_mae: 3.8347\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.2046 - mse: 21.1421 - mae: 3.5820 - val_loss: 22.3664 - val_mse: 22.2996 - val_mae: 3.7674\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.5793 - mse: 20.5096 - mae: 3.5331 - val_loss: 21.9971 - val_mse: 21.9235 - val_mae: 3.7339\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0768 - mse: 20.0013 - mae: 3.4942 - val_loss: 21.8683 - val_mse: 21.7902 - val_mae: 3.7194\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.8944 - mse: 19.8149 - mae: 3.4773 - val_loss: 21.8373 - val_mse: 21.7558 - val_mae: 3.7161\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6804 - mse: 19.5978 - mae: 3.4624 - val_loss: 22.0115 - val_mse: 21.9273 - val_mae: 3.7253\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5402 - mse: 19.4551 - mae: 3.4486 - val_loss: 21.8376 - val_mse: 21.7514 - val_mae: 3.7067\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4537 - mse: 19.3667 - mae: 3.4441 - val_loss: 21.8624 - val_mse: 21.7744 - val_mae: 3.7095\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3380 - mse: 19.2492 - mae: 3.4330 - val_loss: 22.0612 - val_mse: 21.9715 - val_mae: 3.7254\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2933 - mse: 19.2028 - mae: 3.4264 - val_loss: 21.4832 - val_mse: 21.3918 - val_mae: 3.6749\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2113 - mse: 19.1193 - mae: 3.4235 - val_loss: 21.5898 - val_mse: 21.4970 - val_mae: 3.6850\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1776 - mse: 19.0841 - mae: 3.4173 - val_loss: 21.4725 - val_mse: 21.3782 - val_mae: 3.6753\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.0510 - mse: 18.9559 - mae: 3.4047 - val_loss: 21.4771 - val_mse: 21.3810 - val_mae: 3.6747\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9917 - mse: 18.8950 - mae: 3.4053 - val_loss: 21.3165 - val_mse: 21.2189 - val_mae: 3.6633\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9696 - mse: 18.8711 - mae: 3.4012 - val_loss: 21.3655 - val_mse: 21.2662 - val_mae: 3.6646\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8011 - mse: 18.7010 - mae: 3.3873 - val_loss: 21.2100 - val_mse: 21.1090 - val_mae: 3.6504\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8502 - mse: 18.7484 - mae: 3.3885 - val_loss: 21.3190 - val_mse: 21.2163 - val_mae: 3.6583\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7345 - mse: 18.6310 - mae: 3.3799 - val_loss: 21.2654 - val_mse: 21.1610 - val_mae: 3.6553\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6944 - mse: 18.5892 - mae: 3.3747 - val_loss: 21.1229 - val_mse: 21.0168 - val_mae: 3.6408\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6094 - mse: 18.5024 - mae: 3.3700 - val_loss: 21.4490 - val_mse: 21.3409 - val_mae: 3.6663\n",
      "bias 0.002993172\n",
      "si 0.6473929\n",
      "rmse 0.046196245\n",
      "kgeprime [0.70376115]\n",
      "rmse_95 0.075287\n",
      "rmse_99 0.09689305\n",
      "pearson 0.7399017214866971\n",
      "pearson_95 0.349218921927945\n",
      "pearson_99 0.27542746096924103\n",
      "rscore 0.536409241906618\n",
      "rscore_95 -7.183276060406248\n",
      "rscore_99 -22.066366441472063\n",
      "nse [0.53640924]\n",
      "nse_95 [-7.18327606]\n",
      "nse_99 [-22.06636644]\n",
      "kge [0.66241968]\n",
      "ext_kge_95 [0.04410231]\n",
      "ext_kge_99 [-0.61194463]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([158031 158055 158079], shape=(3,), dtype=int64) Times out: tf.Tensor(158079, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([140264 140288 140312], shape=(3,), dtype=int64) Times out: tf.Tensor(140312, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([74464 74488 74512], shape=(3,), dtype=int64) Times out: tf.Tensor(74512, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([150620 150644 150668], shape=(3,), dtype=int64) Times out: tf.Tensor(150668, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_307\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_308 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_614 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_615 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_307 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_614 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_307 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_615 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 9s 2ms/step - loss: 24.5688 - mse: 24.5207 - mae: 3.8603 - val_loss: 17.9543 - val_mse: 17.8971 - val_mae: 3.3356\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8536 - mse: 20.7904 - mae: 3.5705 - val_loss: 17.5660 - val_mse: 17.4975 - val_mae: 3.3027\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.2677 - mse: 20.1952 - mae: 3.5223 - val_loss: 17.3870 - val_mse: 17.3109 - val_mae: 3.2868\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9732 - mse: 19.8941 - mae: 3.4959 - val_loss: 17.0289 - val_mse: 16.9471 - val_mae: 3.2427\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8109 - mse: 19.7268 - mae: 3.4799 - val_loss: 16.9647 - val_mse: 16.8785 - val_mae: 3.2386\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6197 - mse: 19.5315 - mae: 3.4651 - val_loss: 16.8402 - val_mse: 16.7502 - val_mae: 3.2197\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.4652 - mse: 19.3737 - mae: 3.4489 - val_loss: 17.1876 - val_mse: 17.0948 - val_mae: 3.2598\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.4232 - mse: 19.3288 - mae: 3.4441 - val_loss: 16.8403 - val_mse: 16.7445 - val_mae: 3.2258\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.2991 - mse: 19.2019 - mae: 3.4354 - val_loss: 16.6623 - val_mse: 16.5637 - val_mae: 3.2048\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.1889 - mse: 19.0889 - mae: 3.4234 - val_loss: 16.6384 - val_mse: 16.5371 - val_mae: 3.2041\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.1302 - mse: 19.0278 - mae: 3.4186 - val_loss: 16.6735 - val_mse: 16.5698 - val_mae: 3.2076\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0673 - mse: 18.9621 - mae: 3.4161 - val_loss: 16.6135 - val_mse: 16.5069 - val_mae: 3.2077\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.9345 - mse: 18.8266 - mae: 3.4024 - val_loss: 16.5353 - val_mse: 16.4259 - val_mae: 3.1927\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.9152 - mse: 18.8043 - mae: 3.3994 - val_loss: 16.6036 - val_mse: 16.4914 - val_mae: 3.2064\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8009 - mse: 18.6871 - mae: 3.3880 - val_loss: 16.5299 - val_mse: 16.4146 - val_mae: 3.1971\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.7149 - mse: 18.5982 - mae: 3.3803 - val_loss: 16.3858 - val_mse: 16.2675 - val_mae: 3.1795\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6741 - mse: 18.5542 - mae: 3.3795 - val_loss: 16.3885 - val_mse: 16.2670 - val_mae: 3.1757\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6432 - mse: 18.5201 - mae: 3.3730 - val_loss: 16.4787 - val_mse: 16.3543 - val_mae: 3.1973\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.4855 - mse: 18.3594 - mae: 3.3613 - val_loss: 16.2823 - val_mse: 16.1546 - val_mae: 3.1711\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.4902 - mse: 18.3611 - mae: 3.3616 - val_loss: 16.3107 - val_mse: 16.1801 - val_mae: 3.1738\n",
      "bias 0.00076764834\n",
      "si 0.589218\n",
      "rmse 0.040224478\n",
      "kgeprime [0.7188673]\n",
      "rmse_95 0.063755915\n",
      "rmse_99 0.072285086\n",
      "pearson 0.7795829535868929\n",
      "pearson_95 0.544443553531209\n",
      "pearson_99 0.44793707099449676\n",
      "rscore 0.6071356495660376\n",
      "rscore_95 -3.754142021438744\n",
      "rscore_99 -3.6254242460303354\n",
      "nse [0.60713565]\n",
      "nse_95 [-3.75414202]\n",
      "nse_99 [-3.62542425]\n",
      "kge [0.70139777]\n",
      "ext_kge_95 [0.37377182]\n",
      "ext_kge_99 [0.34850483]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -44.49 -44.18 -43.87 ... -37.94 -37.62\n",
      "  * longitude       (longitude) float32 169.7 170.0 170.3 ... 175.6 175.9 176.2\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -3.22 -3.16 ... 1.184\n",
      "    vgrd10m         (time, latitude, longitude) float32 2.69 2.888 ... 1.839\n",
      "    uw2             (time, latitude, longitude) float32 10.37 9.984 ... 1.401\n",
      "    vw2             (time, latitude, longitude) float32 7.235 8.343 ... 3.384\n",
      "    wind_magnitude  (time, latitude, longitude) float32 4.195 4.281 ... 2.187\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([83272 83296 83320], shape=(3,), dtype=int64) Times out: tf.Tensor(83320, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([31830 31854 31878], shape=(3,), dtype=int64) Times out: tf.Tensor(31878, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([142229 142253 142277], shape=(3,), dtype=int64) Times out: tf.Tensor(142277, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([9746 9770 9794], shape=(3,), dtype=int64) Times out: tf.Tensor(9794, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_308\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_309 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_616 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_617 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_308 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_616 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_308 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_617 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.1416 - mse: 28.0975 - mae: 4.1538 - val_loss: 21.3430 - val_mse: 21.2900 - val_mae: 3.6302\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2992 - mse: 21.2403 - mae: 3.6241 - val_loss: 19.6744 - val_mse: 19.6106 - val_mae: 3.4648\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5022 - mse: 20.4337 - mae: 3.5501 - val_loss: 19.1744 - val_mse: 19.1024 - val_mae: 3.4484\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1676 - mse: 20.0926 - mae: 3.5205 - val_loss: 18.8261 - val_mse: 18.7484 - val_mae: 3.4135\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9850 - mse: 19.9053 - mae: 3.5002 - val_loss: 18.6505 - val_mse: 18.5693 - val_mae: 3.3998\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8333 - mse: 19.7503 - mae: 3.4872 - val_loss: 18.5018 - val_mse: 18.4173 - val_mae: 3.3868\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.7324 - mse: 19.6466 - mae: 3.4781 - val_loss: 18.4745 - val_mse: 18.3872 - val_mae: 3.3850\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6153 - mse: 19.5266 - mae: 3.4672 - val_loss: 18.4359 - val_mse: 18.3457 - val_mae: 3.3734\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.4966 - mse: 19.4051 - mae: 3.4557 - val_loss: 18.3672 - val_mse: 18.2742 - val_mae: 3.3771\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.4770 - mse: 19.3828 - mae: 3.4522 - val_loss: 18.2909 - val_mse: 18.1950 - val_mae: 3.3622\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3372 - mse: 19.2402 - mae: 3.4388 - val_loss: 18.3587 - val_mse: 18.2603 - val_mae: 3.3745\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2936 - mse: 19.1939 - mae: 3.4336 - val_loss: 18.1860 - val_mse: 18.0850 - val_mae: 3.3555\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2051 - mse: 19.1027 - mae: 3.4278 - val_loss: 18.2809 - val_mse: 18.1769 - val_mae: 3.3696\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.1093 - mse: 19.0043 - mae: 3.4187 - val_loss: 18.0734 - val_mse: 17.9668 - val_mae: 3.3401\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0710 - mse: 18.9634 - mae: 3.4124 - val_loss: 18.1067 - val_mse: 17.9977 - val_mae: 3.3408\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.9798 - mse: 18.8696 - mae: 3.4044 - val_loss: 17.9848 - val_mse: 17.8733 - val_mae: 3.3276\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8799 - mse: 18.7673 - mae: 3.3980 - val_loss: 18.1445 - val_mse: 18.0307 - val_mae: 3.3529\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8028 - mse: 18.6880 - mae: 3.3900 - val_loss: 17.9923 - val_mse: 17.8764 - val_mae: 3.3312\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.7660 - mse: 18.6489 - mae: 3.3878 - val_loss: 18.0273 - val_mse: 17.9093 - val_mae: 3.3328\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6614 - mse: 18.5425 - mae: 3.3780 - val_loss: 17.9362 - val_mse: 17.8163 - val_mae: 3.3246\n",
      "bias -0.0026523548\n",
      "si 0.5938934\n",
      "rmse 0.042209312\n",
      "kgeprime [0.58341773]\n",
      "rmse_95 0.07413671\n",
      "rmse_99 0.111090764\n",
      "pearson 0.7800277091565816\n",
      "pearson_95 0.4760659074792216\n",
      "pearson_99 0.21067749606968303\n",
      "rscore 0.6055917221856829\n",
      "rscore_95 -1.7861907743828587\n",
      "rscore_99 -16.88400107397214\n",
      "nse [0.60559172]\n",
      "nse_95 [-1.78619077]\n",
      "nse_99 [-16.88400107]\n",
      "kge [0.64281599]\n",
      "ext_kge_95 [0.37959414]\n",
      "ext_kge_99 [-0.00501345]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([90366 90390 90414], shape=(3,), dtype=int64) Times out: tf.Tensor(90414, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([59691 59715 59739], shape=(3,), dtype=int64) Times out: tf.Tensor(59739, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([68987 69011 69035], shape=(3,), dtype=int64) Times out: tf.Tensor(69035, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([26559 26583 26607], shape=(3,), dtype=int64) Times out: tf.Tensor(26607, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_309\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_310 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_618 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_619 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_309 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_618 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_619 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.3234 - mse: 25.2828 - mae: 3.9413 - val_loss: 22.4528 - val_mse: 22.3985 - val_mae: 3.7049\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.9115 - mse: 20.8544 - mae: 3.5954 - val_loss: 21.6171 - val_mse: 21.5545 - val_mae: 3.6299\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.3357 - mse: 20.2715 - mae: 3.5406 - val_loss: 20.9413 - val_mse: 20.8726 - val_mae: 3.5704\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.8441 - mse: 19.7747 - mae: 3.4967 - val_loss: 20.4588 - val_mse: 20.3861 - val_mae: 3.5349\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5564 - mse: 19.4835 - mae: 3.4691 - val_loss: 20.5286 - val_mse: 20.4527 - val_mae: 3.5366\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5023 - mse: 19.4265 - mae: 3.4639 - val_loss: 20.4912 - val_mse: 20.4125 - val_mae: 3.5336\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3206 - mse: 19.2420 - mae: 3.4480 - val_loss: 20.3354 - val_mse: 20.2540 - val_mae: 3.5183\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.2811 - mse: 19.1999 - mae: 3.4432 - val_loss: 20.1209 - val_mse: 20.0367 - val_mae: 3.5008\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1391 - mse: 19.0552 - mae: 3.4297 - val_loss: 20.1137 - val_mse: 20.0271 - val_mae: 3.4927\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1041 - mse: 19.0176 - mae: 3.4264 - val_loss: 20.0170 - val_mse: 19.9277 - val_mae: 3.4890\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.0149 - mse: 18.9261 - mae: 3.4189 - val_loss: 19.9934 - val_mse: 19.9018 - val_mae: 3.4894\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9316 - mse: 18.8403 - mae: 3.4101 - val_loss: 20.0259 - val_mse: 19.9318 - val_mae: 3.4826\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8897 - mse: 18.7958 - mae: 3.4019 - val_loss: 20.0603 - val_mse: 19.9639 - val_mae: 3.4847\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8369 - mse: 18.7405 - mae: 3.4018 - val_loss: 19.9171 - val_mse: 19.8181 - val_mae: 3.4722\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7516 - mse: 18.6525 - mae: 3.3961 - val_loss: 20.0131 - val_mse: 19.9117 - val_mae: 3.4787\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6233 - mse: 18.5219 - mae: 3.3805 - val_loss: 19.7622 - val_mse: 19.6584 - val_mae: 3.4632\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.5921 - mse: 18.4883 - mae: 3.3755 - val_loss: 19.7747 - val_mse: 19.6685 - val_mae: 3.4620\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.5277 - mse: 18.4215 - mae: 3.3679 - val_loss: 19.7428 - val_mse: 19.6343 - val_mae: 3.4571\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.4609 - mse: 18.3524 - mae: 3.3639 - val_loss: 19.7824 - val_mse: 19.6718 - val_mae: 3.4586\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.4028 - mse: 18.2919 - mae: 3.3573 - val_loss: 19.6874 - val_mse: 19.5741 - val_mae: 3.4514\n",
      "bias 0.0012984738\n",
      "si 0.5760003\n",
      "rmse 0.044242673\n",
      "kgeprime [0.69789399]\n",
      "rmse_95 0.076681376\n",
      "rmse_99 0.09484654\n",
      "pearson 0.7970642279343729\n",
      "pearson_95 0.39451067361663317\n",
      "pearson_99 0.24740753415262778\n",
      "rscore 0.6318006103992884\n",
      "rscore_95 -5.740438130858998\n",
      "rscore_99 -24.170719110367344\n",
      "nse [0.63180061]\n",
      "nse_95 [-5.74043813]\n",
      "nse_99 [-24.17071911]\n",
      "kge [0.66621795]\n",
      "ext_kge_95 [-0.00902593]\n",
      "ext_kge_99 [-0.89919624]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([77050 77074 77098], shape=(3,), dtype=int64) Times out: tf.Tensor(77098, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([1513 1537 1561], shape=(3,), dtype=int64) Times out: tf.Tensor(1561, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([26017 26041 26065], shape=(3,), dtype=int64) Times out: tf.Tensor(26065, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([53097 53121 53145], shape=(3,), dtype=int64) Times out: tf.Tensor(53145, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_310\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_311 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_620 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_621 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_310 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_620 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_621 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 25.4534 - mse: 25.4048 - mae: 3.9442 - val_loss: 18.9572 - val_mse: 18.8901 - val_mae: 3.3621\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.5878 - mse: 20.5154 - mae: 3.5612 - val_loss: 18.2395 - val_mse: 18.1591 - val_mae: 3.3025\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.8633 - mse: 19.7800 - mae: 3.5024 - val_loss: 17.8607 - val_mse: 17.7717 - val_mae: 3.2606\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.5084 - mse: 19.4174 - mae: 3.4733 - val_loss: 17.7736 - val_mse: 17.6783 - val_mae: 3.2522\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3175 - mse: 19.2209 - mae: 3.4550 - val_loss: 17.6922 - val_mse: 17.5919 - val_mae: 3.2401\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2158 - mse: 19.1148 - mae: 3.4395 - val_loss: 17.6732 - val_mse: 17.5692 - val_mae: 3.2411\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.1096 - mse: 19.0052 - mae: 3.4313 - val_loss: 17.6581 - val_mse: 17.5509 - val_mae: 3.2414\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.9875 - mse: 18.8802 - mae: 3.4199 - val_loss: 17.7002 - val_mse: 17.5904 - val_mae: 3.2479\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.8717 - mse: 18.7616 - mae: 3.4102 - val_loss: 17.6492 - val_mse: 17.5365 - val_mae: 3.2412\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.8395 - mse: 18.7267 - mae: 3.4051 - val_loss: 17.5403 - val_mse: 17.4250 - val_mae: 3.2313\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.6932 - mse: 18.5779 - mae: 3.3945 - val_loss: 17.4961 - val_mse: 17.3783 - val_mae: 3.2251\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.6654 - mse: 18.5473 - mae: 3.3913 - val_loss: 17.3965 - val_mse: 17.2761 - val_mae: 3.2165\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.6222 - mse: 18.5016 - mae: 3.3863 - val_loss: 17.3754 - val_mse: 17.2527 - val_mae: 3.2132\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.5017 - mse: 18.3788 - mae: 3.3773 - val_loss: 17.4377 - val_mse: 17.3128 - val_mae: 3.2263\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.4534 - mse: 18.3279 - mae: 3.3715 - val_loss: 17.3170 - val_mse: 17.1894 - val_mae: 3.2086\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.3767 - mse: 18.2486 - mae: 3.3639 - val_loss: 17.3666 - val_mse: 17.2364 - val_mae: 3.2150\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.3287 - mse: 18.1979 - mae: 3.3573 - val_loss: 17.4432 - val_mse: 17.3104 - val_mae: 3.2250\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.2357 - mse: 18.1021 - mae: 3.3513 - val_loss: 17.3117 - val_mse: 17.1761 - val_mae: 3.2097\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.1386 - mse: 18.0023 - mae: 3.3411 - val_loss: 17.1965 - val_mse: 17.0584 - val_mae: 3.1973\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.0742 - mse: 17.9353 - mae: 3.3352 - val_loss: 17.3649 - val_mse: 17.2238 - val_mae: 3.2163\n",
      "bias -0.0039604055\n",
      "si 0.5719065\n",
      "rmse 0.041501608\n",
      "kgeprime [0.55918102]\n",
      "rmse_95 0.06844434\n",
      "rmse_99 0.07326542\n",
      "pearson 0.8006499070395978\n",
      "pearson_95 0.502901869336762\n",
      "pearson_99 0.47421962130481343\n",
      "rscore 0.6367538857172517\n",
      "rscore_95 -4.575030700242177\n",
      "rscore_99 -13.729861375819159\n",
      "nse [0.63675389]\n",
      "nse_95 [-4.5750307]\n",
      "nse_99 [-13.72986138]\n",
      "kge [0.64523881]\n",
      "ext_kge_95 [0.20334363]\n",
      "ext_kge_99 [-0.24310414]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([12479 12503 12527], shape=(3,), dtype=int64) Times out: tf.Tensor(12527, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([8703 8727 8751], shape=(3,), dtype=int64) Times out: tf.Tensor(8751, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([20209 20233 20257], shape=(3,), dtype=int64) Times out: tf.Tensor(20257, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([18003 18027 18051], shape=(3,), dtype=int64) Times out: tf.Tensor(18051, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_311\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_312 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_622 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_623 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_311 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_622 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_311 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_623 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 23.4887 - mse: 23.4487 - mae: 3.7675 - val_loss: 21.8937 - val_mse: 21.8443 - val_mae: 3.7386\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.9003 - mse: 19.8459 - mae: 3.4803 - val_loss: 21.7522 - val_mse: 21.6923 - val_mae: 3.7158\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2431 - mse: 19.1798 - mae: 3.4267 - val_loss: 21.3113 - val_mse: 21.2444 - val_mae: 3.6705\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9598 - mse: 18.8906 - mae: 3.4052 - val_loss: 21.2462 - val_mse: 21.1741 - val_mae: 3.6607\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8280 - mse: 18.7540 - mae: 3.3927 - val_loss: 21.1577 - val_mse: 21.0812 - val_mae: 3.6503\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6642 - mse: 18.5862 - mae: 3.3734 - val_loss: 21.1098 - val_mse: 21.0293 - val_mae: 3.6464\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.5262 - mse: 18.4444 - mae: 3.3617 - val_loss: 21.1984 - val_mse: 21.1144 - val_mae: 3.6491\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.4270 - mse: 18.3418 - mae: 3.3543 - val_loss: 21.0105 - val_mse: 20.9232 - val_mae: 3.6394\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3459 - mse: 18.2575 - mae: 3.3470 - val_loss: 21.0086 - val_mse: 20.9183 - val_mae: 3.6359\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.2424 - mse: 18.1511 - mae: 3.3364 - val_loss: 20.9874 - val_mse: 20.8942 - val_mae: 3.6327\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.1739 - mse: 18.0795 - mae: 3.3311 - val_loss: 20.8547 - val_mse: 20.7585 - val_mae: 3.6177\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.0639 - mse: 17.9666 - mae: 3.3231 - val_loss: 20.7358 - val_mse: 20.6367 - val_mae: 3.6108\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.9823 - mse: 17.8821 - mae: 3.3109 - val_loss: 20.9578 - val_mse: 20.8560 - val_mae: 3.6243\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.9200 - mse: 17.8172 - mae: 3.3075 - val_loss: 20.9253 - val_mse: 20.8209 - val_mae: 3.6244\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.8629 - mse: 17.7572 - mae: 3.3010 - val_loss: 20.8286 - val_mse: 20.7213 - val_mae: 3.6169\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.7444 - mse: 17.6357 - mae: 3.2899 - val_loss: 20.6070 - val_mse: 20.4966 - val_mae: 3.5955\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.6648 - mse: 17.5532 - mae: 3.2830 - val_loss: 20.6309 - val_mse: 20.5178 - val_mae: 3.5974\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.5640 - mse: 17.4494 - mae: 3.2712 - val_loss: 20.4625 - val_mse: 20.3463 - val_mae: 3.5824\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.5096 - mse: 17.3921 - mae: 3.2676 - val_loss: 20.4113 - val_mse: 20.2923 - val_mae: 3.5768\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.5158 - mse: 17.3955 - mae: 3.2674 - val_loss: 20.6260 - val_mse: 20.5041 - val_mae: 3.5930\n",
      "bias -0.0054858644\n",
      "si 0.6374992\n",
      "rmse 0.045281406\n",
      "kgeprime [0.50018302]\n",
      "rmse_95 0.06565462\n",
      "rmse_99 0.08558362\n",
      "pearson 0.7478527069993393\n",
      "pearson_95 0.322496273439748\n",
      "pearson_99 0.23044364176472593\n",
      "rscore 0.5467103902788935\n",
      "rscore_95 -5.314111922453956\n",
      "rscore_99 -17.288781367812152\n",
      "nse [0.54671039]\n",
      "nse_95 [-5.31411192]\n",
      "nse_99 [-17.28878137]\n",
      "kge [0.59815047]\n",
      "ext_kge_95 [0.11008784]\n",
      "ext_kge_99 [-0.26412106]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([117830 117854 117878], shape=(3,), dtype=int64) Times out: tf.Tensor(117878, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([126555 126579 126603], shape=(3,), dtype=int64) Times out: tf.Tensor(126603, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([81793 81817 81841], shape=(3,), dtype=int64) Times out: tf.Tensor(81841, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([148996 149020 149044], shape=(3,), dtype=int64) Times out: tf.Tensor(149044, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_312\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_313 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_624 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_625 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_312 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_624 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_312 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_625 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 27.5404 - mse: 27.4901 - mae: 4.0878 - val_loss: 18.4067 - val_mse: 18.3428 - val_mae: 3.3660\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.7283 - mse: 21.6597 - mae: 3.6449 - val_loss: 17.5632 - val_mse: 17.4904 - val_mae: 3.2995\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.9567 - mse: 20.8802 - mae: 3.5814 - val_loss: 17.4823 - val_mse: 17.4029 - val_mae: 3.2970\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6366 - mse: 20.5550 - mae: 3.5548 - val_loss: 17.3200 - val_mse: 17.2370 - val_mae: 3.2847\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.4281 - mse: 20.3435 - mae: 3.5348 - val_loss: 16.9580 - val_mse: 16.8722 - val_mae: 3.2489\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.2665 - mse: 20.1797 - mae: 3.5211 - val_loss: 17.0087 - val_mse: 16.9212 - val_mae: 3.2539\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.1691 - mse: 20.0806 - mae: 3.5137 - val_loss: 17.3034 - val_mse: 17.2142 - val_mae: 3.2869\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1097 - mse: 20.0196 - mae: 3.5025 - val_loss: 17.3060 - val_mse: 17.2154 - val_mae: 3.2866\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.0014 - mse: 19.9102 - mae: 3.4995 - val_loss: 16.5947 - val_mse: 16.5029 - val_mae: 3.2102\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9394 - mse: 19.8470 - mae: 3.4910 - val_loss: 17.0179 - val_mse: 16.9248 - val_mae: 3.2559\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9174 - mse: 19.8236 - mae: 3.4896 - val_loss: 16.6844 - val_mse: 16.5900 - val_mae: 3.2210\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.7804 - mse: 19.6853 - mae: 3.4782 - val_loss: 16.9528 - val_mse: 16.8571 - val_mae: 3.2524\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.7478 - mse: 19.6515 - mae: 3.4733 - val_loss: 16.6900 - val_mse: 16.5932 - val_mae: 3.2206\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6440 - mse: 19.5464 - mae: 3.4641 - val_loss: 16.8155 - val_mse: 16.7173 - val_mae: 3.2338\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5724 - mse: 19.4734 - mae: 3.4588 - val_loss: 16.9335 - val_mse: 16.8339 - val_mae: 3.2489\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5146 - mse: 19.4141 - mae: 3.4540 - val_loss: 16.5594 - val_mse: 16.4584 - val_mae: 3.2039\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.5010 - mse: 19.3991 - mae: 3.4545 - val_loss: 16.7027 - val_mse: 16.6002 - val_mae: 3.2242\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.4041 - mse: 19.3008 - mae: 3.4455 - val_loss: 16.5228 - val_mse: 16.4187 - val_mae: 3.2059\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3635 - mse: 19.2586 - mae: 3.4390 - val_loss: 16.7465 - val_mse: 16.6411 - val_mae: 3.2327\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2325 - mse: 19.1261 - mae: 3.4283 - val_loss: 16.4846 - val_mse: 16.3776 - val_mae: 3.2016\n",
      "bias 0.006186173\n",
      "si 0.59635055\n",
      "rmse 0.040469244\n",
      "kgeprime [0.64477553]\n",
      "rmse_95 0.070744894\n",
      "rmse_99 0.07811413\n",
      "pearson 0.7741125069107554\n",
      "pearson_95 0.573267624501499\n",
      "pearson_99 0.4010537321944767\n",
      "rscore 0.5888753302635358\n",
      "rscore_95 -5.159604792004375\n",
      "rscore_99 -6.264302960819219\n",
      "nse [0.58887533]\n",
      "nse_95 [-5.15960479]\n",
      "nse_99 [-6.26430296]\n",
      "kge [0.56434147]\n",
      "ext_kge_95 [0.33977825]\n",
      "ext_kge_99 [0.26836632]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -44.18 -43.87 -43.56 ... -37.94 -37.62\n",
      "  * longitude       (longitude) float32 172.8 173.1 173.4 ... 179.1 179.4 179.7\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 3.88 2.638 ... 1.874\n",
      "    vgrd10m         (time, latitude, longitude) float32 4.668 4.499 ... 0.9593\n",
      "    uw2             (time, latitude, longitude) float32 15.05 6.962 ... 3.511\n",
      "    vw2             (time, latitude, longitude) float32 21.79 20.24 ... 0.9202\n",
      "    wind_magnitude  (time, latitude, longitude) float32 6.07 5.216 ... 2.105\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([92374 92398 92422], shape=(3,), dtype=int64) Times out: tf.Tensor(92422, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([128823 128847 128871], shape=(3,), dtype=int64) Times out: tf.Tensor(128871, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([98351 98375 98399], shape=(3,), dtype=int64) Times out: tf.Tensor(98399, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([118953 118977 119001], shape=(3,), dtype=int64) Times out: tf.Tensor(119001, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_313\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_314 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_626 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_627 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_313 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_626 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_313 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_627 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 25.7132 - mse: 25.6653 - mae: 3.9604 - val_loss: 19.6811 - val_mse: 19.6251 - val_mae: 3.4802\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0179 - mse: 21.9574 - mae: 3.6853 - val_loss: 19.2847 - val_mse: 19.2207 - val_mae: 3.4492\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.7054 - mse: 21.6391 - mae: 3.6538 - val_loss: 19.1773 - val_mse: 19.1089 - val_mae: 3.4424\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4537 - mse: 21.3835 - mae: 3.6321 - val_loss: 18.9660 - val_mse: 18.8939 - val_mae: 3.4275\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1307 - mse: 21.0572 - mae: 3.6049 - val_loss: 18.3921 - val_mse: 18.3168 - val_mae: 3.3663\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8619 - mse: 20.7853 - mae: 3.5798 - val_loss: 18.6774 - val_mse: 18.5992 - val_mae: 3.4071\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5680 - mse: 20.4888 - mae: 3.5558 - val_loss: 18.0513 - val_mse: 17.9707 - val_mae: 3.3452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4152 - mse: 20.3338 - mae: 3.5397 - val_loss: 17.9492 - val_mse: 17.8665 - val_mae: 3.3361\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.2929 - mse: 20.2093 - mae: 3.5300 - val_loss: 17.4273 - val_mse: 17.3425 - val_mae: 3.2800\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1621 - mse: 20.0764 - mae: 3.5177 - val_loss: 17.4844 - val_mse: 17.3977 - val_mae: 3.2917\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.0518 - mse: 19.9643 - mae: 3.5092 - val_loss: 17.3554 - val_mse: 17.2668 - val_mae: 3.2737\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9994 - mse: 19.9102 - mae: 3.5032 - val_loss: 17.4009 - val_mse: 17.3109 - val_mae: 3.2806\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9363 - mse: 19.8455 - mae: 3.4948 - val_loss: 17.2979 - val_mse: 17.2061 - val_mae: 3.2725\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8475 - mse: 19.7550 - mae: 3.4908 - val_loss: 17.3643 - val_mse: 17.2707 - val_mae: 3.2764\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8280 - mse: 19.7338 - mae: 3.4842 - val_loss: 17.3625 - val_mse: 17.2673 - val_mae: 3.2807\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8338 - mse: 19.7378 - mae: 3.4872 - val_loss: 17.3964 - val_mse: 17.2992 - val_mae: 3.2846\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.7037 - mse: 19.6058 - mae: 3.4763 - val_loss: 17.2562 - val_mse: 17.1574 - val_mae: 3.2632\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.7233 - mse: 19.6239 - mae: 3.4768 - val_loss: 17.3119 - val_mse: 17.2116 - val_mae: 3.2750\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6703 - mse: 19.5691 - mae: 3.4741 - val_loss: 17.1781 - val_mse: 17.0759 - val_mae: 3.2617\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6190 - mse: 19.5160 - mae: 3.4688 - val_loss: 17.2444 - val_mse: 17.1405 - val_mae: 3.2694\n",
      "bias -0.004475283\n",
      "si 0.5047161\n",
      "rmse 0.041401073\n",
      "kgeprime [0.62340516]\n",
      "rmse_95 0.06728452\n",
      "rmse_99 0.08622443\n",
      "pearson 0.8450077110789012\n",
      "pearson_95 0.4196908784249358\n",
      "pearson_99 0.6911871508291456\n",
      "rscore 0.7095937671456036\n",
      "rscore_95 -3.534844523061108\n",
      "rscore_99 -15.28647900030829\n",
      "nse [0.70959377]\n",
      "nse_95 [-3.53484452]\n",
      "nse_99 [-15.286479]\n",
      "kge [0.70686471]\n",
      "ext_kge_95 [0.31557273]\n",
      "ext_kge_99 [0.27655398]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([111649 111673 111697], shape=(3,), dtype=int64) Times out: tf.Tensor(111697, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([39579 39603 39627], shape=(3,), dtype=int64) Times out: tf.Tensor(39627, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([93286 93310 93334], shape=(3,), dtype=int64) Times out: tf.Tensor(93334, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([97290 97314 97338], shape=(3,), dtype=int64) Times out: tf.Tensor(97338, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_314\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_315 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_628 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_629 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_314 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_628 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_314 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_629 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.6360 - mse: 26.5862 - mae: 4.0353 - val_loss: 22.9645 - val_mse: 22.9040 - val_mae: 3.7192\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8936 - mse: 21.8278 - mae: 3.6806 - val_loss: 21.8744 - val_mse: 21.8037 - val_mae: 3.6256\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.1702 - mse: 21.0954 - mae: 3.6143 - val_loss: 21.2163 - val_mse: 21.1384 - val_mae: 3.5735\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.6102 - mse: 20.5300 - mae: 3.5672 - val_loss: 20.4362 - val_mse: 20.3547 - val_mae: 3.5048\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.2992 - mse: 20.2164 - mae: 3.5387 - val_loss: 19.9451 - val_mse: 19.8614 - val_mae: 3.4519\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0270 - mse: 19.9423 - mae: 3.5160 - val_loss: 20.2563 - val_mse: 20.1705 - val_mae: 3.4969\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.9094 - mse: 19.8229 - mae: 3.5051 - val_loss: 19.7934 - val_mse: 19.7060 - val_mae: 3.4508\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.7573 - mse: 19.6691 - mae: 3.4876 - val_loss: 19.7283 - val_mse: 19.6392 - val_mae: 3.4449\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5933 - mse: 19.5030 - mae: 3.4768 - val_loss: 19.6203 - val_mse: 19.5289 - val_mae: 3.4358\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4648 - mse: 19.3725 - mae: 3.4641 - val_loss: 19.4161 - val_mse: 19.3229 - val_mae: 3.4117\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3834 - mse: 19.2889 - mae: 3.4560 - val_loss: 19.0544 - val_mse: 18.9591 - val_mae: 3.3799\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2661 - mse: 19.1694 - mae: 3.4485 - val_loss: 19.0573 - val_mse: 18.9598 - val_mae: 3.3736\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1941 - mse: 19.0953 - mae: 3.4397 - val_loss: 18.9214 - val_mse: 18.8220 - val_mae: 3.3641\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1164 - mse: 19.0157 - mae: 3.4320 - val_loss: 19.4064 - val_mse: 19.3049 - val_mae: 3.4229\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.0163 - mse: 18.9137 - mae: 3.4218 - val_loss: 18.9797 - val_mse: 18.8766 - val_mae: 3.3705\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9627 - mse: 18.8584 - mae: 3.4193 - val_loss: 18.9145 - val_mse: 18.8097 - val_mae: 3.3702\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9411 - mse: 18.8350 - mae: 3.4165 - val_loss: 18.9450 - val_mse: 18.8384 - val_mae: 3.3744\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9267 - mse: 18.8187 - mae: 3.4119 - val_loss: 19.1550 - val_mse: 19.0466 - val_mae: 3.3948\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8589 - mse: 18.7491 - mae: 3.4088 - val_loss: 18.8930 - val_mse: 18.7827 - val_mae: 3.3667\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8364 - mse: 18.7248 - mae: 3.4052 - val_loss: 18.8802 - val_mse: 18.7680 - val_mae: 3.3647\n",
      "bias 0.0033048284\n",
      "si 0.48955387\n",
      "rmse 0.043322094\n",
      "kgeprime [0.78022877]\n",
      "rmse_95 0.0667297\n",
      "rmse_99 0.08667074\n",
      "pearson 0.8610275005690117\n",
      "pearson_95 0.33447230525716243\n",
      "pearson_99 -0.3687423938174912\n",
      "rscore 0.7321908691761883\n",
      "rscore_95 -5.729526471205747\n",
      "rscore_99 -33.175756229360715\n",
      "nse [0.73219087]\n",
      "nse_95 [-5.72952647]\n",
      "nse_99 [-33.17575623]\n",
      "kge [0.70921653]\n",
      "ext_kge_95 [0.15172294]\n",
      "ext_kge_99 [-1.23995895]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([51194 51218 51242], shape=(3,), dtype=int64) Times out: tf.Tensor(51242, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([42451 42475 42499], shape=(3,), dtype=int64) Times out: tf.Tensor(42499, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([20325 20349 20373], shape=(3,), dtype=int64) Times out: tf.Tensor(20373, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([60328 60352 60376], shape=(3,), dtype=int64) Times out: tf.Tensor(60376, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_315\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_316 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_630 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_631 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_315 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_630 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_315 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_631 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 25.8859 - mse: 25.8378 - mae: 3.9887 - val_loss: 20.0912 - val_mse: 20.0347 - val_mae: 3.4895\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.8377 - mse: 21.7770 - mae: 3.6822 - val_loss: 19.8332 - val_mse: 19.7680 - val_mae: 3.4653\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.5086 - mse: 21.4417 - mae: 3.6526 - val_loss: 19.5646 - val_mse: 19.4944 - val_mae: 3.4385\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.1081 - mse: 21.0366 - mae: 3.6164 - val_loss: 19.1882 - val_mse: 19.1135 - val_mae: 3.3963\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.9225 - mse: 20.8465 - mae: 3.5994 - val_loss: 18.9115 - val_mse: 18.8322 - val_mae: 3.3726\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.5351 - mse: 20.4544 - mae: 3.5666 - val_loss: 18.5671 - val_mse: 18.4832 - val_mae: 3.3456\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2091 - mse: 20.1240 - mae: 3.5385 - val_loss: 18.3209 - val_mse: 18.2330 - val_mae: 3.3241\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.0148 - mse: 19.9260 - mae: 3.5204 - val_loss: 18.1325 - val_mse: 18.0414 - val_mae: 3.3030\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.8598 - mse: 19.7678 - mae: 3.5086 - val_loss: 17.9166 - val_mse: 17.8223 - val_mae: 3.2799\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.7609 - mse: 19.6656 - mae: 3.4997 - val_loss: 17.8265 - val_mse: 17.7292 - val_mae: 3.2709\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.6372 - mse: 19.5390 - mae: 3.4876 - val_loss: 17.7289 - val_mse: 17.6287 - val_mae: 3.2627\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.5613 - mse: 19.4606 - mae: 3.4809 - val_loss: 17.8252 - val_mse: 17.7229 - val_mae: 3.2862\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.4882 - mse: 19.3851 - mae: 3.4728 - val_loss: 17.6354 - val_mse: 17.5306 - val_mae: 3.2538\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3849 - mse: 19.2796 - mae: 3.4629 - val_loss: 17.6136 - val_mse: 17.5067 - val_mae: 3.2465\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3774 - mse: 19.2701 - mae: 3.4605 - val_loss: 17.6025 - val_mse: 17.4936 - val_mae: 3.2540\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3949 - mse: 19.2857 - mae: 3.4630 - val_loss: 17.5578 - val_mse: 17.4472 - val_mae: 3.2488\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2444 - mse: 19.1338 - mae: 3.4505 - val_loss: 17.5762 - val_mse: 17.4643 - val_mae: 3.2507\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2787 - mse: 19.1665 - mae: 3.4542 - val_loss: 17.5738 - val_mse: 17.4604 - val_mae: 3.2515\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.1421 - mse: 19.0284 - mae: 3.4414 - val_loss: 17.5855 - val_mse: 17.4705 - val_mae: 3.2558\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.1728 - mse: 19.0577 - mae: 3.4435 - val_loss: 17.6327 - val_mse: 17.5166 - val_mae: 3.2448\n",
      "bias -0.0027976795\n",
      "si 0.5051759\n",
      "rmse 0.041852783\n",
      "kgeprime [0.69192264]\n",
      "rmse_95 0.06611535\n",
      "rmse_99 0.1116413\n",
      "pearson 0.8456179705885126\n",
      "pearson_95 0.20194570243293092\n",
      "pearson_99 -0.11876618552020587\n",
      "rscore 0.7136473920531181\n",
      "rscore_95 -2.778428638436495\n",
      "rscore_99 -21.305866732058266\n",
      "nse [0.71364739]\n",
      "nse_95 [-2.77842864]\n",
      "nse_99 [-21.30586673]\n",
      "kge [0.75013512]\n",
      "ext_kge_95 [0.15211705]\n",
      "ext_kge_99 [-0.8927248]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([8129 8153 8177], shape=(3,), dtype=int64) Times out: tf.Tensor(8177, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([19495 19519 19543], shape=(3,), dtype=int64) Times out: tf.Tensor(19543, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([11950 11974 11998], shape=(3,), dtype=int64) Times out: tf.Tensor(11998, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([29507 29531 29555], shape=(3,), dtype=int64) Times out: tf.Tensor(29555, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_316\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_317 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_632 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_633 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_316 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_632 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_316 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_633 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.6293 - mse: 25.5847 - mae: 3.9464 - val_loss: 22.6494 - val_mse: 22.5977 - val_mae: 3.7796\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.4147 - mse: 21.3593 - mae: 3.6250 - val_loss: 22.1691 - val_mse: 22.1103 - val_mae: 3.7224\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.8633 - mse: 20.8018 - mae: 3.5814 - val_loss: 22.2490 - val_mse: 22.1848 - val_mae: 3.7343\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.5915 - mse: 20.5251 - mae: 3.5562 - val_loss: 21.8982 - val_mse: 21.8294 - val_mae: 3.6993\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.2746 - mse: 20.2038 - mae: 3.5298 - val_loss: 21.4857 - val_mse: 21.4126 - val_mae: 3.6589\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0108 - mse: 19.9358 - mae: 3.5046 - val_loss: 21.0495 - val_mse: 20.9725 - val_mae: 3.6166\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.8347 - mse: 19.7559 - mae: 3.4896 - val_loss: 21.3306 - val_mse: 21.2499 - val_mae: 3.6562\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6056 - mse: 19.5233 - mae: 3.4685 - val_loss: 20.8346 - val_mse: 20.7507 - val_mae: 3.6062\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4361 - mse: 19.3508 - mae: 3.4531 - val_loss: 20.6935 - val_mse: 20.6068 - val_mae: 3.5929\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3226 - mse: 19.2346 - mae: 3.4429 - val_loss: 20.6955 - val_mse: 20.6062 - val_mae: 3.5951\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2942 - mse: 19.2038 - mae: 3.4383 - val_loss: 20.8448 - val_mse: 20.7535 - val_mae: 3.6160\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1948 - mse: 19.1021 - mae: 3.4313 - val_loss: 20.5930 - val_mse: 20.4994 - val_mae: 3.5896\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1276 - mse: 19.0328 - mae: 3.4247 - val_loss: 20.7854 - val_mse: 20.6897 - val_mae: 3.6118\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.0464 - mse: 18.9497 - mae: 3.4133 - val_loss: 20.3905 - val_mse: 20.2928 - val_mae: 3.5657\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9912 - mse: 18.8924 - mae: 3.4064 - val_loss: 20.4246 - val_mse: 20.3249 - val_mae: 3.5733\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9719 - mse: 18.8714 - mae: 3.4067 - val_loss: 20.2241 - val_mse: 20.1228 - val_mae: 3.5506\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8976 - mse: 18.7952 - mae: 3.4016 - val_loss: 20.2042 - val_mse: 20.1010 - val_mae: 3.5501\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9275 - mse: 18.8232 - mae: 3.4026 - val_loss: 20.0121 - val_mse: 19.9069 - val_mae: 3.5271\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7739 - mse: 18.6678 - mae: 3.3877 - val_loss: 20.3813 - val_mse: 20.2744 - val_mae: 3.5700\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7568 - mse: 18.6490 - mae: 3.3901 - val_loss: 20.2540 - val_mse: 20.1453 - val_mae: 3.5555\n",
      "bias 0.004626032\n",
      "si 0.54996836\n",
      "rmse 0.044883538\n",
      "kgeprime [0.73832718]\n",
      "rmse_95 0.06689509\n",
      "rmse_99 0.082110636\n",
      "pearson 0.8159006024622617\n",
      "pearson_95 0.3878244244316979\n",
      "pearson_99 0.015213489971459422\n",
      "rscore 0.6606150873186636\n",
      "rscore_95 -6.678837462880258\n",
      "rscore_99 -34.77149342047079\n",
      "nse [0.66061509]\n",
      "nse_95 [-6.67883746]\n",
      "nse_99 [-34.77149342]\n",
      "kge [0.70359222]\n",
      "ext_kge_95 [0.13120179]\n",
      "ext_kge_99 [-0.80328812]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([139366 139390 139414], shape=(3,), dtype=int64) Times out: tf.Tensor(139414, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([65432 65456 65480], shape=(3,), dtype=int64) Times out: tf.Tensor(65480, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([193251 193275 193299], shape=(3,), dtype=int64) Times out: tf.Tensor(193299, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([55145 55169 55193], shape=(3,), dtype=int64) Times out: tf.Tensor(55193, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_317\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_318 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_634 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_635 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_317 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_634 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_317 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_635 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 27.3012 - mse: 27.2499 - mae: 4.0669 - val_loss: 20.2488 - val_mse: 20.1886 - val_mae: 3.6106\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6004 - mse: 22.5341 - mae: 3.7164 - val_loss: 19.2171 - val_mse: 19.1459 - val_mae: 3.5144\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0747 - mse: 22.0003 - mae: 3.6713 - val_loss: 19.2834 - val_mse: 19.2063 - val_mae: 3.5274\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.8496 - mse: 21.7708 - mae: 3.6512 - val_loss: 18.7099 - val_mse: 18.6294 - val_mae: 3.4657\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5991 - mse: 21.5175 - mae: 3.6265 - val_loss: 18.7834 - val_mse: 18.7005 - val_mae: 3.4831\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.3585 - mse: 21.2746 - mae: 3.6048 - val_loss: 18.5738 - val_mse: 18.4884 - val_mae: 3.4703\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.0045 - mse: 20.9180 - mae: 3.5709 - val_loss: 17.9766 - val_mse: 17.8887 - val_mae: 3.4163\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7721 - mse: 20.6834 - mae: 3.5510 - val_loss: 17.6498 - val_mse: 17.5601 - val_mae: 3.3907\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5334 - mse: 20.4429 - mae: 3.5337 - val_loss: 17.5543 - val_mse: 17.4629 - val_mae: 3.3847\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6151 - mse: 20.5231 - mae: 3.5379 - val_loss: 17.5370 - val_mse: 17.4440 - val_mae: 3.3880\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4051 - mse: 20.3114 - mae: 3.5201 - val_loss: 17.3556 - val_mse: 17.2611 - val_mae: 3.3663\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3850 - mse: 20.2898 - mae: 3.5178 - val_loss: 17.1871 - val_mse: 17.0913 - val_mae: 3.3515\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3058 - mse: 20.2096 - mae: 3.5051 - val_loss: 17.1800 - val_mse: 17.0830 - val_mae: 3.3515\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.2749 - mse: 20.1773 - mae: 3.5090 - val_loss: 17.2023 - val_mse: 17.1039 - val_mae: 3.3519\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.1342 - mse: 20.0353 - mae: 3.4993 - val_loss: 17.2786 - val_mse: 17.1790 - val_mae: 3.3658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1865 - mse: 20.0863 - mae: 3.4986 - val_loss: 17.5543 - val_mse: 17.4533 - val_mae: 3.3925\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.0545 - mse: 19.9528 - mae: 3.4926 - val_loss: 16.9431 - val_mse: 16.8407 - val_mae: 3.3242\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.0825 - mse: 19.9794 - mae: 3.4945 - val_loss: 17.2402 - val_mse: 17.1362 - val_mae: 3.3526\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9744 - mse: 19.8701 - mae: 3.4812 - val_loss: 17.3223 - val_mse: 17.2174 - val_mae: 3.3644\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9491 - mse: 19.8434 - mae: 3.4821 - val_loss: 16.8913 - val_mse: 16.7853 - val_mae: 3.3178\n",
      "bias 0.0035849388\n",
      "si 0.50675744\n",
      "rmse 0.040969875\n",
      "kgeprime [0.78365573]\n",
      "rmse_95 0.059003063\n",
      "rmse_99 0.072660446\n",
      "pearson 0.8426908671645472\n",
      "pearson_95 0.7008328707990804\n",
      "pearson_99 0.6184389464009005\n",
      "rscore 0.7066279528902732\n",
      "rscore_95 -2.1786077255120024\n",
      "rscore_99 -7.352283706375291\n",
      "nse [0.70662795]\n",
      "nse_95 [-2.17860773]\n",
      "nse_99 [-7.35228371]\n",
      "kge [0.71947851]\n",
      "ext_kge_95 [0.57621343]\n",
      "ext_kge_99 [0.43608611]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -44.18 -43.87 -43.56 ... -37.62 -37.31\n",
      "  * longitude       (longitude) float32 169.7 170.0 170.3 ... 175.6 175.9 176.2\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -1.818 -2.589 ... 0.6046\n",
      "    vgrd10m         (time, latitude, longitude) float32 1.889 2.329 ... 2.408\n",
      "    uw2             (time, latitude, longitude) float32 3.305 6.705 ... 0.3656\n",
      "    vw2             (time, latitude, longitude) float32 3.567 5.423 ... 5.797\n",
      "    wind_magnitude  (time, latitude, longitude) float32 2.622 3.482 ... 2.483\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([66035 66059 66083], shape=(3,), dtype=int64) Times out: tf.Tensor(66083, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([51678 51702 51726], shape=(3,), dtype=int64) Times out: tf.Tensor(51726, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([25460 25484 25508], shape=(3,), dtype=int64) Times out: tf.Tensor(25508, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([147113 147137 147161], shape=(3,), dtype=int64) Times out: tf.Tensor(147161, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_318\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_319 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_636 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_637 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_318 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_636 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_318 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_637 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 25.4611 - mse: 25.4147 - mae: 3.9213 - val_loss: 21.8922 - val_mse: 21.8330 - val_mae: 3.6409\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6858 - mse: 20.6207 - mae: 3.5560 - val_loss: 20.6216 - val_mse: 20.5513 - val_mae: 3.5222\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1743 - mse: 20.1004 - mae: 3.5086 - val_loss: 20.1077 - val_mse: 20.0299 - val_mae: 3.4692\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9246 - mse: 19.8437 - mae: 3.4868 - val_loss: 19.9427 - val_mse: 19.8584 - val_mae: 3.4717\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6655 - mse: 19.5787 - mae: 3.4647 - val_loss: 19.4321 - val_mse: 19.3425 - val_mae: 3.4191\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5754 - mse: 19.4838 - mae: 3.4538 - val_loss: 19.3359 - val_mse: 19.2419 - val_mae: 3.4197\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.4006 - mse: 19.3048 - mae: 3.4384 - val_loss: 19.1151 - val_mse: 19.0172 - val_mae: 3.3993\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2252 - mse: 19.1261 - mae: 3.4237 - val_loss: 19.0915 - val_mse: 18.9906 - val_mae: 3.4082\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0954 - mse: 18.9929 - mae: 3.4129 - val_loss: 18.7231 - val_mse: 18.6188 - val_mae: 3.3674\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0067 - mse: 18.9013 - mae: 3.4044 - val_loss: 18.8051 - val_mse: 18.6981 - val_mae: 3.3862\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8659 - mse: 18.7580 - mae: 3.3868 - val_loss: 18.5053 - val_mse: 18.3960 - val_mae: 3.3499\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8560 - mse: 18.7460 - mae: 3.3929 - val_loss: 18.4593 - val_mse: 18.3480 - val_mae: 3.3542\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8245 - mse: 18.7124 - mae: 3.3876 - val_loss: 18.3038 - val_mse: 18.1908 - val_mae: 3.3397\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6671 - mse: 18.5533 - mae: 3.3753 - val_loss: 18.3585 - val_mse: 18.2435 - val_mae: 3.3511\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6779 - mse: 18.5623 - mae: 3.3736 - val_loss: 18.1854 - val_mse: 18.0687 - val_mae: 3.3260\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6329 - mse: 18.5155 - mae: 3.3696 - val_loss: 18.4161 - val_mse: 18.2975 - val_mae: 3.3672\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.5193 - mse: 18.4001 - mae: 3.3601 - val_loss: 18.0815 - val_mse: 17.9611 - val_mae: 3.3304\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.4522 - mse: 18.3311 - mae: 3.3519 - val_loss: 18.0791 - val_mse: 17.9569 - val_mae: 3.3335\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.3245 - mse: 18.2014 - mae: 3.3426 - val_loss: 17.8748 - val_mse: 17.7505 - val_mae: 3.3030\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.2811 - mse: 18.1559 - mae: 3.3394 - val_loss: 17.9189 - val_mse: 17.7927 - val_mae: 3.3186\n",
      "bias -0.004320797\n",
      "si 0.5720063\n",
      "rmse 0.04218135\n",
      "kgeprime [0.54081316]\n",
      "rmse_95 0.07524415\n",
      "rmse_99 0.112693325\n",
      "pearson 0.7983177452025776\n",
      "pearson_95 0.6346251838368049\n",
      "pearson_99 0.533126243190544\n",
      "rscore 0.6304815709496727\n",
      "rscore_95 -1.0089326422141394\n",
      "rscore_99 -8.264665986788104\n",
      "nse [0.63048157]\n",
      "nse_95 [-1.00893264]\n",
      "nse_99 [-8.26466599]\n",
      "kge [0.62688528]\n",
      "ext_kge_95 [0.50416717]\n",
      "ext_kge_99 [0.26286933]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([107927 107951 107975], shape=(3,), dtype=int64) Times out: tf.Tensor(107975, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([83451 83475 83499], shape=(3,), dtype=int64) Times out: tf.Tensor(83499, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([83770 83794 83818], shape=(3,), dtype=int64) Times out: tf.Tensor(83818, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([15573 15597 15621], shape=(3,), dtype=int64) Times out: tf.Tensor(15621, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_319\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_320 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_638 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_639 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_319 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_638 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_319 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_639 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 24.3680 - mse: 24.3226 - mae: 3.8556 - val_loss: 22.3556 - val_mse: 22.2934 - val_mae: 3.6996\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.2694 - mse: 20.2044 - mae: 3.5294 - val_loss: 21.5557 - val_mse: 21.4819 - val_mae: 3.6267\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5521 - mse: 19.4762 - mae: 3.4700 - val_loss: 20.7660 - val_mse: 20.6823 - val_mae: 3.5572\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1044 - mse: 19.0196 - mae: 3.4292 - val_loss: 20.7064 - val_mse: 20.6150 - val_mae: 3.5547\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.8931 - mse: 18.8016 - mae: 3.4095 - val_loss: 20.5230 - val_mse: 20.4261 - val_mae: 3.5365\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7622 - mse: 18.6653 - mae: 3.3958 - val_loss: 20.3341 - val_mse: 20.2320 - val_mae: 3.5200\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6047 - mse: 18.5030 - mae: 3.3796 - val_loss: 20.3437 - val_mse: 20.2371 - val_mae: 3.5218\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3981 - mse: 18.2920 - mae: 3.3602 - val_loss: 20.1266 - val_mse: 20.0157 - val_mae: 3.4994\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3607 - mse: 18.2505 - mae: 3.3560 - val_loss: 20.2245 - val_mse: 20.1095 - val_mae: 3.5120\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.2521 - mse: 18.1378 - mae: 3.3470 - val_loss: 20.1193 - val_mse: 20.0005 - val_mae: 3.4951\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.1583 - mse: 18.0401 - mae: 3.3369 - val_loss: 19.8583 - val_mse: 19.7356 - val_mae: 3.4767\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.0744 - mse: 17.9524 - mae: 3.3258 - val_loss: 19.8506 - val_mse: 19.7243 - val_mae: 3.4739\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.9299 - mse: 17.8045 - mae: 3.3115 - val_loss: 19.7856 - val_mse: 19.6562 - val_mae: 3.4691\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.8910 - mse: 17.7624 - mae: 3.3079 - val_loss: 19.7882 - val_mse: 19.6557 - val_mae: 3.4742\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.7586 - mse: 17.6264 - mae: 3.2935 - val_loss: 19.7294 - val_mse: 19.5934 - val_mae: 3.4646\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.6699 - mse: 17.5342 - mae: 3.2857 - val_loss: 19.5697 - val_mse: 19.4301 - val_mae: 3.4513\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.6485 - mse: 17.5096 - mae: 3.2825 - val_loss: 19.4789 - val_mse: 19.3365 - val_mae: 3.4370\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.5388 - mse: 17.3967 - mae: 3.2757 - val_loss: 19.5411 - val_mse: 19.3955 - val_mae: 3.4448\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.4610 - mse: 17.3159 - mae: 3.2654 - val_loss: 19.4344 - val_mse: 19.2862 - val_mae: 3.4379\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.4157 - mse: 17.2676 - mae: 3.2597 - val_loss: 19.3453 - val_mse: 19.1942 - val_mae: 3.4293\n",
      "bias -0.0026718623\n",
      "si 0.55984336\n",
      "rmse 0.04381116\n",
      "kgeprime [0.62955619]\n",
      "rmse_95 0.0730344\n",
      "rmse_99 0.080221154\n",
      "pearson 0.8079844376873689\n",
      "pearson_95 0.5886358527732256\n",
      "pearson_99 0.44617328424421915\n",
      "rscore 0.6506733062933097\n",
      "rscore_95 -3.14180415986094\n",
      "rscore_99 -7.916391590780133\n",
      "nse [0.65067331]\n",
      "nse_95 [-3.14180416]\n",
      "nse_99 [-7.91639159]\n",
      "kge [0.68746574]\n",
      "ext_kge_95 [0.11637938]\n",
      "ext_kge_99 [-0.66071648]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([62991 63015 63039], shape=(3,), dtype=int64) Times out: tf.Tensor(63039, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([20445 20469 20493], shape=(3,), dtype=int64) Times out: tf.Tensor(20493, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([74820 74844 74868], shape=(3,), dtype=int64) Times out: tf.Tensor(74868, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([43768 43792 43816], shape=(3,), dtype=int64) Times out: tf.Tensor(43816, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_320\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_321 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_640 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_641 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_320 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_640 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_320 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_641 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.0560 - mse: 26.0049 - mae: 3.9792 - val_loss: 18.9835 - val_mse: 18.9142 - val_mae: 3.3775\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.3041 - mse: 21.2306 - mae: 3.6114 - val_loss: 18.3494 - val_mse: 18.2672 - val_mae: 3.3195\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 20.6168 - mse: 20.5323 - mae: 3.5492 - val_loss: 18.1712 - val_mse: 18.0793 - val_mae: 3.3164\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 20.2845 - mse: 20.1910 - mae: 3.5226 - val_loss: 17.9968 - val_mse: 17.8969 - val_mae: 3.3062\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.0693 - mse: 19.9687 - mae: 3.5064 - val_loss: 17.9153 - val_mse: 17.8097 - val_mae: 3.3026\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.8141 - mse: 19.7081 - mae: 3.4831 - val_loss: 17.8951 - val_mse: 17.7852 - val_mae: 3.3039\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.5755 - mse: 19.4656 - mae: 3.4622 - val_loss: 17.5857 - val_mse: 17.4723 - val_mae: 3.2659\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.4158 - mse: 19.3028 - mae: 3.4461 - val_loss: 17.5783 - val_mse: 17.4625 - val_mae: 3.2654\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.3201 - mse: 19.2048 - mae: 3.4374 - val_loss: 17.5870 - val_mse: 17.4698 - val_mae: 3.2697\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.1837 - mse: 19.0667 - mae: 3.4287 - val_loss: 17.4792 - val_mse: 17.3598 - val_mae: 3.2517\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.0912 - mse: 18.9723 - mae: 3.4192 - val_loss: 17.5462 - val_mse: 17.4256 - val_mae: 3.2608\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.0292 - mse: 18.9092 - mae: 3.4140 - val_loss: 17.3979 - val_mse: 17.2762 - val_mae: 3.2325\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.9503 - mse: 18.8288 - mae: 3.4042 - val_loss: 17.4321 - val_mse: 17.3089 - val_mae: 3.2482\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.9032 - mse: 18.7803 - mae: 3.4024 - val_loss: 17.3487 - val_mse: 17.2245 - val_mae: 3.2287\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.8551 - mse: 18.7310 - mae: 3.3945 - val_loss: 17.4162 - val_mse: 17.2907 - val_mae: 3.2425\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.7176 - mse: 18.5922 - mae: 3.3872 - val_loss: 17.2875 - val_mse: 17.1606 - val_mae: 3.2238\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.6236 - mse: 18.4968 - mae: 3.3780 - val_loss: 17.3561 - val_mse: 17.2278 - val_mae: 3.2394\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.5476 - mse: 18.4192 - mae: 3.3699 - val_loss: 17.3021 - val_mse: 17.1720 - val_mae: 3.2313\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.5030 - mse: 18.3729 - mae: 3.3665 - val_loss: 17.2036 - val_mse: 17.0722 - val_mae: 3.2207\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.4195 - mse: 18.2879 - mae: 3.3540 - val_loss: 17.2444 - val_mse: 17.1114 - val_mae: 3.2261\n",
      "bias -0.0044467305\n",
      "si 0.572123\n",
      "rmse 0.041365955\n",
      "kgeprime [0.54901102]\n",
      "rmse_95 0.066171534\n",
      "rmse_99 0.06775732\n",
      "pearson 0.7998347956771991\n",
      "pearson_95 0.5033518899937302\n",
      "pearson_99 0.37669832319774155\n",
      "rscore 0.6352627459448581\n",
      "rscore_95 -4.447649078112635\n",
      "rscore_99 -10.27068013306636\n",
      "nse [0.63526275]\n",
      "nse_95 [-4.44764908]\n",
      "nse_99 [-10.27068013]\n",
      "kge [0.64169866]\n",
      "ext_kge_95 [0.22792632]\n",
      "ext_kge_99 [-0.13087076]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([31709 31733 31757], shape=(3,), dtype=int64) Times out: tf.Tensor(31757, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([19632 19656 19680], shape=(3,), dtype=int64) Times out: tf.Tensor(19680, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([36539 36563 36587], shape=(3,), dtype=int64) Times out: tf.Tensor(36587, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([28174 28198 28222], shape=(3,), dtype=int64) Times out: tf.Tensor(28222, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_321\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_322 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_642 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_643 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_321 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_642 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_321 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_643 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.4082 - mse: 25.3690 - mae: 3.8994 - val_loss: 21.4722 - val_mse: 21.4210 - val_mae: 3.6997\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.7765 - mse: 20.7210 - mae: 3.5335 - val_loss: 21.1152 - val_mse: 21.0550 - val_mae: 3.6618\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.1584 - mse: 20.0948 - mae: 3.4864 - val_loss: 20.9098 - val_mse: 20.8418 - val_mae: 3.6421\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7567 - mse: 19.6857 - mae: 3.4523 - val_loss: 20.7011 - val_mse: 20.6262 - val_mae: 3.6247\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4778 - mse: 19.3999 - mae: 3.4318 - val_loss: 20.7835 - val_mse: 20.7023 - val_mae: 3.6255\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.2805 - mse: 19.1972 - mae: 3.4175 - val_loss: 20.5329 - val_mse: 20.4471 - val_mae: 3.6052\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1703 - mse: 19.0828 - mae: 3.4056 - val_loss: 20.7270 - val_mse: 20.6373 - val_mae: 3.6242\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.9959 - mse: 18.9049 - mae: 3.3918 - val_loss: 20.3547 - val_mse: 20.2619 - val_mae: 3.5864\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.9253 - mse: 18.8315 - mae: 3.3861 - val_loss: 20.2647 - val_mse: 20.1695 - val_mae: 3.5798\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.8476 - mse: 18.7515 - mae: 3.3755 - val_loss: 20.4287 - val_mse: 20.3311 - val_mae: 3.5897\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.7314 - mse: 18.6331 - mae: 3.3676 - val_loss: 20.3123 - val_mse: 20.2128 - val_mae: 3.5777\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6374 - mse: 18.5371 - mae: 3.3613 - val_loss: 20.1537 - val_mse: 20.0520 - val_mae: 3.5645\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.5799 - mse: 18.4773 - mae: 3.3573 - val_loss: 20.2540 - val_mse: 20.1502 - val_mae: 3.5758\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.5277 - mse: 18.4230 - mae: 3.3526 - val_loss: 20.2201 - val_mse: 20.1141 - val_mae: 3.5731\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.4185 - mse: 18.3116 - mae: 3.3427 - val_loss: 20.1691 - val_mse: 20.0609 - val_mae: 3.5647\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3608 - mse: 18.2519 - mae: 3.3347 - val_loss: 20.0804 - val_mse: 19.9704 - val_mae: 3.5603\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3305 - mse: 18.2196 - mae: 3.3324 - val_loss: 20.1033 - val_mse: 19.9913 - val_mae: 3.5671\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.2310 - mse: 18.1181 - mae: 3.3215 - val_loss: 20.0610 - val_mse: 19.9470 - val_mae: 3.5561\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.1488 - mse: 18.0339 - mae: 3.3156 - val_loss: 19.9711 - val_mse: 19.8552 - val_mae: 3.5515\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.1348 - mse: 18.0179 - mae: 3.3150 - val_loss: 19.9767 - val_mse: 19.8587 - val_mae: 3.5489\n",
      "bias -0.0015391001\n",
      "si 0.63024765\n",
      "rmse 0.044563048\n",
      "kgeprime [0.64599429]\n",
      "rmse_95 0.06697208\n",
      "rmse_99 0.07606281\n",
      "pearson 0.7527313912192709\n",
      "pearson_95 0.38573838532381693\n",
      "pearson_99 0.2317790819247154\n",
      "rscore 0.5625805303694373\n",
      "rscore_95 -5.666876600710375\n",
      "rscore_99 -12.338906871454562\n",
      "nse [0.56258053]\n",
      "nse_95 [-5.6668766]\n",
      "nse_99 [-12.33890687]\n",
      "kge [0.68103045]\n",
      "ext_kge_95 [0.05918477]\n",
      "ext_kge_99 [-0.26229972]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([86963 86987 87011], shape=(3,), dtype=int64) Times out: tf.Tensor(87011, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([107114 107138 107162], shape=(3,), dtype=int64) Times out: tf.Tensor(107162, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([56233 56257 56281], shape=(3,), dtype=int64) Times out: tf.Tensor(56281, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([61611 61635 61659], shape=(3,), dtype=int64) Times out: tf.Tensor(61659, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_322\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_323 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_644 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_645 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_322 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_644 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_322 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_645 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 25.8150 - mse: 25.7720 - mae: 3.9553 - val_loss: 17.4897 - val_mse: 17.4334 - val_mae: 3.2835\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2310 - mse: 21.1672 - mae: 3.5986 - val_loss: 17.1451 - val_mse: 17.0747 - val_mae: 3.2496\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6299 - mse: 20.5548 - mae: 3.5463 - val_loss: 16.6055 - val_mse: 16.5260 - val_mae: 3.1911\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1833 - mse: 20.1004 - mae: 3.5120 - val_loss: 16.5456 - val_mse: 16.4591 - val_mae: 3.1794\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9728 - mse: 19.8835 - mae: 3.4917 - val_loss: 16.3209 - val_mse: 16.2286 - val_mae: 3.1553\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8155 - mse: 19.7209 - mae: 3.4781 - val_loss: 16.1769 - val_mse: 16.0799 - val_mae: 3.1399\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.6414 - mse: 19.5427 - mae: 3.4656 - val_loss: 16.1375 - val_mse: 16.0368 - val_mae: 3.1339\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5485 - mse: 19.4463 - mae: 3.4532 - val_loss: 16.0776 - val_mse: 15.9737 - val_mae: 3.1379\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3694 - mse: 19.2643 - mae: 3.4392 - val_loss: 15.9954 - val_mse: 15.8888 - val_mae: 3.1193\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3120 - mse: 19.2041 - mae: 3.4328 - val_loss: 15.9026 - val_mse: 15.7934 - val_mae: 3.1165\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.1681 - mse: 19.0575 - mae: 3.4187 - val_loss: 15.8641 - val_mse: 15.7522 - val_mae: 3.1146\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0567 - mse: 18.9435 - mae: 3.4125 - val_loss: 15.7958 - val_mse: 15.6812 - val_mae: 3.0976\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.9637 - mse: 18.8481 - mae: 3.3997 - val_loss: 15.7766 - val_mse: 15.6597 - val_mae: 3.1089\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8390 - mse: 18.7208 - mae: 3.3921 - val_loss: 15.6594 - val_mse: 15.5399 - val_mae: 3.0859\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.7803 - mse: 18.6596 - mae: 3.3837 - val_loss: 15.6128 - val_mse: 15.4908 - val_mae: 3.0863\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6114 - mse: 18.4882 - mae: 3.3684 - val_loss: 15.5540 - val_mse: 15.4295 - val_mae: 3.0853\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.5645 - mse: 18.4387 - mae: 3.3655 - val_loss: 15.4691 - val_mse: 15.3424 - val_mae: 3.0746\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.4749 - mse: 18.3469 - mae: 3.3592 - val_loss: 15.4711 - val_mse: 15.3420 - val_mae: 3.0705\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.4273 - mse: 18.2969 - mae: 3.3506 - val_loss: 15.5221 - val_mse: 15.3905 - val_mae: 3.0829\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.2732 - mse: 18.1405 - mae: 3.3414 - val_loss: 15.3524 - val_mse: 15.2185 - val_mae: 3.0627\n",
      "bias -0.00022766792\n",
      "si 0.5866017\n",
      "rmse 0.039010935\n",
      "kgeprime [0.6961948]\n",
      "rmse_95 0.05694109\n",
      "rmse_99 0.06943671\n",
      "pearson 0.7815754705592852\n",
      "pearson_95 0.5208580290962616\n",
      "pearson_99 0.4321457529935458\n",
      "rscore 0.6106089901672731\n",
      "rscore_95 -2.7516727203689095\n",
      "rscore_99 -7.845994201972653\n",
      "nse [0.61060899]\n",
      "nse_95 [-2.75167272]\n",
      "nse_99 [-7.8459942]\n",
      "kge [0.70164029]\n",
      "ext_kge_95 [0.36248647]\n",
      "ext_kge_99 [0.12775404]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -42.93 -42.62 -42.31 ... -36.37 -36.06\n",
      "  * longitude       (longitude) float32 173.4 173.8 174.1 ... 179.7 180.0 180.3\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 4.298 3.888 ... 3.017\n",
      "    vgrd10m         (time, latitude, longitude) float32 -0.05982 ... -0.1495\n",
      "    uw2             (time, latitude, longitude) float32 18.48 15.12 ... 12.1 9.1\n",
      "    vw2             (time, latitude, longitude) float32 0.003578 ... 0.02237\n",
      "    wind_magnitude  (time, latitude, longitude) float32 4.299 3.91 ... 3.02\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([140216 140240 140264], shape=(3,), dtype=int64) Times out: tf.Tensor(140264, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([48976 49000 49024], shape=(3,), dtype=int64) Times out: tf.Tensor(49024, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([98445 98469 98493], shape=(3,), dtype=int64) Times out: tf.Tensor(98493, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([59373 59397 59421], shape=(3,), dtype=int64) Times out: tf.Tensor(59421, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_323\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_324 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_646 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_647 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_323 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_646 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_323 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_647 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 26.4783 - mse: 26.4446 - mae: 4.0049 - val_loss: 18.5718 - val_mse: 18.5355 - val_mae: 3.3931\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0296 - mse: 21.9925 - mae: 3.6751 - val_loss: 17.8064 - val_mse: 17.7688 - val_mae: 3.3139\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5141 - mse: 21.4758 - mae: 3.6306 - val_loss: 17.6733 - val_mse: 17.6343 - val_mae: 3.3018\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4044 - mse: 21.3646 - mae: 3.6195 - val_loss: 17.5218 - val_mse: 17.4813 - val_mae: 3.2864\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2928 - mse: 21.2516 - mae: 3.6093 - val_loss: 17.2839 - val_mse: 17.2420 - val_mae: 3.2625\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2364 - mse: 21.1939 - mae: 3.6032 - val_loss: 17.5215 - val_mse: 17.4783 - val_mae: 3.2717\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1916 - mse: 21.1477 - mae: 3.6018 - val_loss: 17.1268 - val_mse: 17.0823 - val_mae: 3.2419\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1219 - mse: 21.0767 - mae: 3.5942 - val_loss: 17.0720 - val_mse: 17.0262 - val_mae: 3.2378\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.0282 - mse: 20.9817 - mae: 3.5859 - val_loss: 17.0292 - val_mse: 16.9819 - val_mae: 3.2358\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.9463 - mse: 20.8983 - mae: 3.5784 - val_loss: 16.8732 - val_mse: 16.8245 - val_mae: 3.2147\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8316 - mse: 20.7820 - mae: 3.5717 - val_loss: 16.7585 - val_mse: 16.7080 - val_mae: 3.2144\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7281 - mse: 20.6768 - mae: 3.5626 - val_loss: 16.7897 - val_mse: 16.7375 - val_mae: 3.2060\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8083 - mse: 20.7552 - mae: 3.5644 - val_loss: 16.7160 - val_mse: 16.6621 - val_mae: 3.2090\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6019 - mse: 20.5471 - mae: 3.5515 - val_loss: 16.5581 - val_mse: 16.5024 - val_mae: 3.1997\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6075 - mse: 20.5508 - mae: 3.5494 - val_loss: 16.6322 - val_mse: 16.5746 - val_mae: 3.2059\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5517 - mse: 20.4932 - mae: 3.5420 - val_loss: 16.4601 - val_mse: 16.4008 - val_mae: 3.1909\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4440 - mse: 20.3839 - mae: 3.5327 - val_loss: 16.4195 - val_mse: 16.3585 - val_mae: 3.1808\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4338 - mse: 20.3720 - mae: 3.5276 - val_loss: 16.4638 - val_mse: 16.4012 - val_mae: 3.1875\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4980 - mse: 20.4345 - mae: 3.5366 - val_loss: 16.3816 - val_mse: 16.3174 - val_mae: 3.1858\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4065 - mse: 20.3416 - mae: 3.5261 - val_loss: 16.3837 - val_mse: 16.3180 - val_mae: 3.1923\n",
      "bias -0.00064376206\n",
      "si 0.54399323\n",
      "rmse 0.04039555\n",
      "kgeprime [0.6651687]\n",
      "rmse_95 0.06799899\n",
      "rmse_99 0.08545295\n",
      "pearson 0.8234257230681833\n",
      "pearson_95 0.4425307440617832\n",
      "pearson_99 0.3422447138926236\n",
      "rscore 0.6703488038405843\n",
      "rscore_95 -4.462311439775919\n",
      "rscore_99 -17.444072505254418\n",
      "nse [0.6703488]\n",
      "nse_95 [-4.46231144]\n",
      "nse_99 [-17.44407251]\n",
      "kge [0.68145262]\n",
      "ext_kge_95 [0.30906185]\n",
      "ext_kge_99 [0.14273548]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([107140 107164 107188], shape=(3,), dtype=int64) Times out: tf.Tensor(107188, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([11650 11674 11698], shape=(3,), dtype=int64) Times out: tf.Tensor(11698, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([6994 7018 7042], shape=(3,), dtype=int64) Times out: tf.Tensor(7042, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77350 77374 77398], shape=(3,), dtype=int64) Times out: tf.Tensor(77398, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_324\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_325 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_648 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_649 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_324 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_648 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_324 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_649 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 8s 2ms/step - loss: 25.5932 - mse: 25.5536 - mae: 3.9394 - val_loss: 20.5528 - val_mse: 20.5082 - val_mae: 3.5410\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.7947 - mse: 20.7497 - mae: 3.5775 - val_loss: 20.1179 - val_mse: 20.0718 - val_mae: 3.4973\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.3600 - mse: 20.3130 - mae: 3.5376 - val_loss: 19.8278 - val_mse: 19.7790 - val_mae: 3.4734\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0929 - mse: 20.0433 - mae: 3.5119 - val_loss: 19.7020 - val_mse: 19.6510 - val_mae: 3.4625\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.1361 - mse: 20.0848 - mae: 3.5132 - val_loss: 19.8921 - val_mse: 19.8395 - val_mae: 3.4798\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0150 - mse: 19.9621 - mae: 3.5038 - val_loss: 19.4595 - val_mse: 19.4053 - val_mae: 3.4396\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.9055 - mse: 19.8512 - mae: 3.4956 - val_loss: 19.6417 - val_mse: 19.5862 - val_mae: 3.4575\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.7504 - mse: 19.6946 - mae: 3.4832 - val_loss: 19.2634 - val_mse: 19.2066 - val_mae: 3.4219\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.7482 - mse: 19.6911 - mae: 3.4804 - val_loss: 19.2075 - val_mse: 19.1493 - val_mae: 3.4154\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6496 - mse: 19.5911 - mae: 3.4743 - val_loss: 18.8489 - val_mse: 18.7893 - val_mae: 3.3815\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5985 - mse: 19.5386 - mae: 3.4696 - val_loss: 18.8445 - val_mse: 18.7836 - val_mae: 3.3836\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.6108 - mse: 19.5495 - mae: 3.4687 - val_loss: 19.1363 - val_mse: 19.0738 - val_mae: 3.4139\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3831 - mse: 19.3203 - mae: 3.4515 - val_loss: 18.6789 - val_mse: 18.6150 - val_mae: 3.3648\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4561 - mse: 19.3919 - mae: 3.4568 - val_loss: 18.6910 - val_mse: 18.6259 - val_mae: 3.3660\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2260 - mse: 19.1605 - mae: 3.4376 - val_loss: 18.7374 - val_mse: 18.6706 - val_mae: 3.3718\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2818 - mse: 19.2148 - mae: 3.4390 - val_loss: 18.8905 - val_mse: 18.8226 - val_mae: 3.3886\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3217 - mse: 19.2533 - mae: 3.4452 - val_loss: 18.6568 - val_mse: 18.5873 - val_mae: 3.3634\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1991 - mse: 19.1291 - mae: 3.4315 - val_loss: 18.5287 - val_mse: 18.4576 - val_mae: 3.3502\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1764 - mse: 19.1050 - mae: 3.4331 - val_loss: 18.7096 - val_mse: 18.6370 - val_mae: 3.3685\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1684 - mse: 19.0955 - mae: 3.4300 - val_loss: 18.4899 - val_mse: 18.4159 - val_mae: 3.3446\n",
      "bias -0.0013924148\n",
      "si 0.526922\n",
      "rmse 0.042913783\n",
      "kgeprime [0.66082597]\n",
      "rmse_95 0.0684028\n",
      "rmse_99 0.091552794\n",
      "pearson 0.8377544296140497\n",
      "pearson_95 0.44792152773298766\n",
      "pearson_99 0.7203655845582195\n",
      "rscore 0.6932582964209233\n",
      "rscore_95 -4.063924878306032\n",
      "rscore_99 -13.33766853080777\n",
      "nse [0.6932583]\n",
      "nse_95 [-4.06392488]\n",
      "nse_99 [-13.33766853]\n",
      "kge [0.69423793]\n",
      "ext_kge_95 [0.3102893]\n",
      "ext_kge_99 [0.13203864]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([52449 52473 52497], shape=(3,), dtype=int64) Times out: tf.Tensor(52497, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77290 77314 77338], shape=(3,), dtype=int64) Times out: tf.Tensor(77338, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([66761 66785 66809], shape=(3,), dtype=int64) Times out: tf.Tensor(66809, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([58456 58480 58504], shape=(3,), dtype=int64) Times out: tf.Tensor(58504, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_325\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_326 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_650 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_651 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_325 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_650 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_325 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_651 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 24.4773 - mse: 24.4254 - mae: 3.8681 - val_loss: 19.1217 - val_mse: 19.0560 - val_mae: 3.3552\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.7313 - mse: 19.6590 - mae: 3.4997 - val_loss: 18.6480 - val_mse: 18.5682 - val_mae: 3.3126\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2621 - mse: 19.1792 - mae: 3.4564 - val_loss: 18.1981 - val_mse: 18.1102 - val_mae: 3.2736\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.9932 - mse: 18.9041 - mae: 3.4339 - val_loss: 18.0103 - val_mse: 17.9174 - val_mae: 3.2586\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.7844 - mse: 18.6905 - mae: 3.4132 - val_loss: 17.7886 - val_mse: 17.6919 - val_mae: 3.2357\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.5123 - mse: 18.4153 - mae: 3.3879 - val_loss: 17.6958 - val_mse: 17.5968 - val_mae: 3.2245\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.4294 - mse: 18.3299 - mae: 3.3795 - val_loss: 17.6211 - val_mse: 17.5195 - val_mae: 3.2197\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.2494 - mse: 18.1476 - mae: 3.3627 - val_loss: 17.7082 - val_mse: 17.6047 - val_mae: 3.2253\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.1411 - mse: 18.0370 - mae: 3.3575 - val_loss: 17.5086 - val_mse: 17.4029 - val_mae: 3.2101\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 18.0889 - mse: 17.9830 - mae: 3.3512 - val_loss: 17.4119 - val_mse: 17.3045 - val_mae: 3.1942\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.9697 - mse: 17.8617 - mae: 3.3402 - val_loss: 17.5201 - val_mse: 17.4104 - val_mae: 3.2125\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.9355 - mse: 17.8251 - mae: 3.3349 - val_loss: 17.4550 - val_mse: 17.3433 - val_mae: 3.1979\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.8420 - mse: 17.7295 - mae: 3.3283 - val_loss: 17.3135 - val_mse: 17.1997 - val_mae: 3.1892\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.7745 - mse: 17.6596 - mae: 3.3203 - val_loss: 17.3084 - val_mse: 17.1925 - val_mae: 3.1866\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.6689 - mse: 17.5522 - mae: 3.3120 - val_loss: 17.2132 - val_mse: 17.0954 - val_mae: 3.1784\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.7061 - mse: 17.5871 - mae: 3.3124 - val_loss: 17.2406 - val_mse: 17.1205 - val_mae: 3.1841\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.6352 - mse: 17.5142 - mae: 3.3062 - val_loss: 17.3056 - val_mse: 17.1835 - val_mae: 3.1901\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.5463 - mse: 17.4230 - mae: 3.2964 - val_loss: 17.1931 - val_mse: 17.0686 - val_mae: 3.1747\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.5605 - mse: 17.4348 - mae: 3.3007 - val_loss: 17.1798 - val_mse: 17.0530 - val_mae: 3.1747\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 17.4693 - mse: 17.3413 - mae: 3.2873 - val_loss: 17.1186 - val_mse: 16.9892 - val_mae: 3.1684\n",
      "bias 0.0002681627\n",
      "si 0.535381\n",
      "rmse 0.04121796\n",
      "kgeprime [0.75355932]\n",
      "rmse_95 0.07149434\n",
      "rmse_99 0.11216918\n",
      "pearson 0.827012491000633\n",
      "pearson_95 0.27247664870707866\n",
      "pearson_99 -0.15594112189983336\n",
      "rscore 0.6838025677846375\n",
      "rscore_95 -3.4251217229031514\n",
      "rscore_99 -33.705195161389675\n",
      "nse [0.68380257]\n",
      "nse_95 [-3.42512172]\n",
      "nse_99 [-33.70519516]\n",
      "kge [0.74679351]\n",
      "ext_kge_95 [0.17826246]\n",
      "ext_kge_99 [-1.35207754]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([10960 10984 11008], shape=(3,), dtype=int64) Times out: tf.Tensor(11008, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([20969 20993 21017], shape=(3,), dtype=int64) Times out: tf.Tensor(21017, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([3999 4023 4047], shape=(3,), dtype=int64) Times out: tf.Tensor(4047, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([12263 12287 12311], shape=(3,), dtype=int64) Times out: tf.Tensor(12311, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_326\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_327 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_652 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_653 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_326 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_652 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_653 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 24.3683 - mse: 24.3237 - mae: 3.8328 - val_loss: 20.3506 - val_mse: 20.2959 - val_mae: 3.5978\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0128 - mse: 19.9545 - mae: 3.4938 - val_loss: 20.2828 - val_mse: 20.2208 - val_mae: 3.5919\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6366 - mse: 19.5728 - mae: 3.4557 - val_loss: 19.9767 - val_mse: 19.9105 - val_mae: 3.5665\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4310 - mse: 19.3636 - mae: 3.4370 - val_loss: 19.8304 - val_mse: 19.7610 - val_mae: 3.5531\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3640 - mse: 19.2936 - mae: 3.4305 - val_loss: 19.4776 - val_mse: 19.4058 - val_mae: 3.5187\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2407 - mse: 19.1683 - mae: 3.4205 - val_loss: 19.4892 - val_mse: 19.4154 - val_mae: 3.5214\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1398 - mse: 19.0652 - mae: 3.4120 - val_loss: 19.2244 - val_mse: 19.1484 - val_mae: 3.4980\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9430 - mse: 18.8659 - mae: 3.3947 - val_loss: 19.3208 - val_mse: 19.2426 - val_mae: 3.5071\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7826 - mse: 18.7033 - mae: 3.3827 - val_loss: 18.9898 - val_mse: 18.9095 - val_mae: 3.4754\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7062 - mse: 18.6247 - mae: 3.3746 - val_loss: 19.0863 - val_mse: 19.0040 - val_mae: 3.4845\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.6246 - mse: 18.5411 - mae: 3.3668 - val_loss: 19.1000 - val_mse: 19.0155 - val_mae: 3.4853\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.6427 - mse: 18.5571 - mae: 3.3690 - val_loss: 18.8212 - val_mse: 18.7350 - val_mae: 3.4559\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.5037 - mse: 18.4163 - mae: 3.3539 - val_loss: 18.8691 - val_mse: 18.7811 - val_mae: 3.4587\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3767 - mse: 18.2876 - mae: 3.3444 - val_loss: 18.8252 - val_mse: 18.7356 - val_mae: 3.4545\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.3915 - mse: 18.3010 - mae: 3.3463 - val_loss: 18.8507 - val_mse: 18.7595 - val_mae: 3.4538\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.3794 - mse: 18.2874 - mae: 3.3407 - val_loss: 18.7924 - val_mse: 18.6997 - val_mae: 3.4490\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.2581 - mse: 18.1645 - mae: 3.3311 - val_loss: 18.7046 - val_mse: 18.6103 - val_mae: 3.4400\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.2711 - mse: 18.1756 - mae: 3.3314 - val_loss: 18.8303 - val_mse: 18.7340 - val_mae: 3.4529\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.1157 - mse: 18.0183 - mae: 3.3188 - val_loss: 18.6851 - val_mse: 18.5871 - val_mae: 3.4389\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.1676 - mse: 18.0686 - mae: 3.3250 - val_loss: 18.8255 - val_mse: 18.7256 - val_mae: 3.4490\n",
      "bias 0.00042821694\n",
      "si 0.57778716\n",
      "rmse 0.043273136\n",
      "kgeprime [0.77016563]\n",
      "rmse_95 0.05621768\n",
      "rmse_99 0.06962581\n",
      "pearson 0.7988621370641332\n",
      "pearson_95 0.3677894575881875\n",
      "pearson_99 0.2576915320791006\n",
      "rscore 0.6325638961039715\n",
      "rscore_95 -5.233736944320206\n",
      "rscore_99 -14.560808023663618\n",
      "nse [0.6325639]\n",
      "nse_95 [-5.23373694]\n",
      "nse_99 [-14.56080802]\n",
      "kge [0.76167327]\n",
      "ext_kge_95 [0.19752883]\n",
      "ext_kge_99 [0.13508913]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([67499 67523 67547], shape=(3,), dtype=int64) Times out: tf.Tensor(67547, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([159352 159376 159400], shape=(3,), dtype=int64) Times out: tf.Tensor(159400, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([102562 102586 102610], shape=(3,), dtype=int64) Times out: tf.Tensor(102610, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([188989 189013 189037], shape=(3,), dtype=int64) Times out: tf.Tensor(189037, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_327\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_328 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_654 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_655 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_327 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_654 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_655 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 26.4895 - mse: 26.4550 - mae: 4.0024 - val_loss: 18.3751 - val_mse: 18.3347 - val_mae: 3.4039\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4722 - mse: 21.4289 - mae: 3.6257 - val_loss: 17.9415 - val_mse: 17.8948 - val_mae: 3.3828\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6232 - mse: 20.5729 - mae: 3.5471 - val_loss: 17.3002 - val_mse: 17.2467 - val_mae: 3.3071\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.3101 - mse: 20.2538 - mae: 3.5218 - val_loss: 17.2267 - val_mse: 17.1680 - val_mae: 3.3018\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.1810 - mse: 20.1202 - mae: 3.5069 - val_loss: 17.2180 - val_mse: 17.1552 - val_mae: 3.3002\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.0391 - mse: 19.9744 - mae: 3.4947 - val_loss: 17.1582 - val_mse: 17.0920 - val_mae: 3.2961\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.0255 - mse: 19.9575 - mae: 3.4956 - val_loss: 17.0106 - val_mse: 16.9409 - val_mae: 3.2808\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.8880 - mse: 19.8166 - mae: 3.4829 - val_loss: 16.9890 - val_mse: 16.9160 - val_mae: 3.2807\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.7063 - mse: 19.6316 - mae: 3.4658 - val_loss: 16.8697 - val_mse: 16.7936 - val_mae: 3.2679\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.6656 - mse: 19.5878 - mae: 3.4625 - val_loss: 16.6285 - val_mse: 16.5491 - val_mae: 3.2418\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.6007 - mse: 19.5198 - mae: 3.4521 - val_loss: 16.5037 - val_mse: 16.4213 - val_mae: 3.2279\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.4526 - mse: 19.3689 - mae: 3.4425 - val_loss: 16.4133 - val_mse: 16.3284 - val_mae: 3.2222\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.4012 - mse: 19.3151 - mae: 3.4339 - val_loss: 16.2782 - val_mse: 16.1909 - val_mae: 3.2056\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.3595 - mse: 19.2711 - mae: 3.4343 - val_loss: 16.5600 - val_mse: 16.4706 - val_mae: 3.2429\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.3305 - mse: 19.2401 - mae: 3.4281 - val_loss: 16.1178 - val_mse: 16.0262 - val_mae: 3.1934\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.2228 - mse: 19.1303 - mae: 3.4206 - val_loss: 16.5822 - val_mse: 16.4888 - val_mae: 3.2511\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.2321 - mse: 19.1378 - mae: 3.4199 - val_loss: 16.1894 - val_mse: 16.0941 - val_mae: 3.2038\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.1737 - mse: 19.0776 - mae: 3.4190 - val_loss: 16.5312 - val_mse: 16.4343 - val_mae: 3.2483\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.1316 - mse: 19.0337 - mae: 3.4113 - val_loss: 16.1042 - val_mse: 16.0054 - val_mae: 3.1919\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 19.0448 - mse: 18.9451 - mae: 3.4053 - val_loss: 16.1938 - val_mse: 16.0932 - val_mae: 3.2066\n",
      "bias 0.004992828\n",
      "si 0.5398664\n",
      "rmse 0.04011631\n",
      "kgeprime [0.72766305]\n",
      "rmse_95 0.06102256\n",
      "rmse_99 0.0806678\n",
      "pearson 0.8204275799348212\n",
      "pearson_95 0.4876927329275815\n",
      "pearson_99 0.33656577163241064\n",
      "rscore 0.6675563198879437\n",
      "rscore_95 -3.4611274294216194\n",
      "rscore_99 -9.938729265202065\n",
      "nse [0.66755632]\n",
      "nse_95 [-3.46112743]\n",
      "nse_99 [-9.93872927]\n",
      "kge [0.66249518]\n",
      "ext_kge_95 [0.39685271]\n",
      "ext_kge_99 [0.22374722]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -42.31 -41.99 -41.68 ... -36.06 -35.75\n",
      "  * longitude       (longitude) float32 170.6 170.9 171.2 ... 176.9 177.2 177.5\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 0.72 0.5491 ... 4.895\n",
      "    vgrd10m         (time, latitude, longitude) float32 -0.03846 ... -0.2307\n",
      "    uw2             (time, latitude, longitude) float32 0.5184 0.3015 ... 23.96\n",
      "    vw2             (time, latitude, longitude) float32 0.001479 ... 0.05324\n",
      "    wind_magnitude  (time, latitude, longitude) float32 0.721 0.5638 ... 4.9\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([42372 42396 42420], shape=(3,), dtype=int64) Times out: tf.Tensor(42420, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([33907 33931 33955], shape=(3,), dtype=int64) Times out: tf.Tensor(33955, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34272 34296 34320], shape=(3,), dtype=int64) Times out: tf.Tensor(34320, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([154627 154651 154675], shape=(3,), dtype=int64) Times out: tf.Tensor(154675, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_328\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_329 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_656 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_657 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_328 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_656 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_657 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 37.2698 - mse: 37.2281 - mae: 4.6479 - val_loss: 23.5417 - val_mse: 23.4904 - val_mae: 3.7353\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 27.3906 - mse: 27.3351 - mae: 4.0267 - val_loss: 22.5854 - val_mse: 22.5262 - val_mae: 3.6773\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.4906 - mse: 26.4283 - mae: 3.9567 - val_loss: 22.0925 - val_mse: 22.0272 - val_mae: 3.6343\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.0310 - mse: 25.9635 - mae: 3.9243 - val_loss: 21.7711 - val_mse: 21.7014 - val_mae: 3.6085\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.7618 - mse: 25.6901 - mae: 3.9075 - val_loss: 21.7960 - val_mse: 21.7223 - val_mae: 3.6217\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.5826 - mse: 25.5072 - mae: 3.8938 - val_loss: 21.5248 - val_mse: 21.4477 - val_mae: 3.5914\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.3974 - mse: 25.3184 - mae: 3.8762 - val_loss: 21.4047 - val_mse: 21.3244 - val_mae: 3.5797\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.3436 - mse: 25.2614 - mae: 3.8738 - val_loss: 21.6612 - val_mse: 21.5773 - val_mae: 3.6069\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.1767 - mse: 25.0913 - mae: 3.8604 - val_loss: 21.5090 - val_mse: 21.4220 - val_mae: 3.5951\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.9882 - mse: 24.8998 - mae: 3.8472 - val_loss: 21.1753 - val_mse: 21.0852 - val_mae: 3.5580\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.9192 - mse: 24.8276 - mae: 3.8454 - val_loss: 21.1771 - val_mse: 21.0840 - val_mae: 3.5673\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.6875 - mse: 24.5927 - mae: 3.8265 - val_loss: 20.8893 - val_mse: 20.7930 - val_mae: 3.5380\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.6117 - mse: 24.5137 - mae: 3.8206 - val_loss: 20.7716 - val_mse: 20.6719 - val_mae: 3.5307\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.3660 - mse: 24.2648 - mae: 3.7997 - val_loss: 20.4512 - val_mse: 20.3484 - val_mae: 3.5047\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.2893 - mse: 24.1849 - mae: 3.7980 - val_loss: 20.2207 - val_mse: 20.1150 - val_mae: 3.4814\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.2313 - mse: 24.1240 - mae: 3.7860 - val_loss: 20.2413 - val_mse: 20.1325 - val_mae: 3.4868\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.9799 - mse: 23.8696 - mae: 3.7712 - val_loss: 19.9246 - val_mse: 19.8126 - val_mae: 3.4580\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.8726 - mse: 23.7591 - mae: 3.7605 - val_loss: 19.8819 - val_mse: 19.7669 - val_mae: 3.4548\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.8783 - mse: 23.7619 - mae: 3.7584 - val_loss: 19.6677 - val_mse: 19.5499 - val_mae: 3.4343\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.7561 - mse: 23.6370 - mae: 3.7487 - val_loss: 19.6235 - val_mse: 19.5030 - val_mae: 3.4309\n",
      "bias -0.005227003\n",
      "si 0.46627975\n",
      "rmse 0.04416225\n",
      "kgeprime [0.62968723]\n",
      "rmse_95 0.06999903\n",
      "rmse_99 0.114811294\n",
      "pearson 0.8721193147239816\n",
      "pearson_95 0.48017218627837305\n",
      "pearson_99 0.6573987970302221\n",
      "rscore 0.7545874741300977\n",
      "rscore_95 -0.8073328114967293\n",
      "rscore_99 -3.5940363744204413\n",
      "nse [0.75458747]\n",
      "nse_95 [-0.80733281]\n",
      "nse_99 [-3.59403637]\n",
      "kge [0.71952148]\n",
      "ext_kge_95 [0.4447167]\n",
      "ext_kge_99 [0.51578461]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([8345 8369 8393], shape=(3,), dtype=int64) Times out: tf.Tensor(8393, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([57034 57058 57082], shape=(3,), dtype=int64) Times out: tf.Tensor(57082, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([98029 98053 98077], shape=(3,), dtype=int64) Times out: tf.Tensor(98077, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([99410 99434 99458], shape=(3,), dtype=int64) Times out: tf.Tensor(99458, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_329\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_330 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_658 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_659 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_329 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_658 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_659 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 33.5676 - mse: 33.5216 - mae: 4.4039 - val_loss: 26.1762 - val_mse: 26.1163 - val_mae: 3.9464\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.0532 - mse: 24.9878 - mae: 3.8547 - val_loss: 25.5335 - val_mse: 25.4629 - val_mae: 3.9166\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.3734 - mse: 24.2993 - mae: 3.7998 - val_loss: 24.8880 - val_mse: 24.8102 - val_mae: 3.8557\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.0936 - mse: 24.0126 - mae: 3.7771 - val_loss: 24.6274 - val_mse: 24.5425 - val_mae: 3.8438\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.7412 - mse: 23.6538 - mae: 3.7509 - val_loss: 24.1225 - val_mse: 24.0315 - val_mae: 3.8023\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.5218 - mse: 23.4285 - mae: 3.7333 - val_loss: 23.9162 - val_mse: 23.8196 - val_mae: 3.7893\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.2792 - mse: 23.1806 - mae: 3.7180 - val_loss: 23.5085 - val_mse: 23.4068 - val_mae: 3.7637\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.0998 - mse: 22.9966 - mae: 3.6996 - val_loss: 23.1800 - val_mse: 23.0738 - val_mae: 3.7364\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.9157 - mse: 22.8080 - mae: 3.6823 - val_loss: 23.0226 - val_mse: 22.9122 - val_mae: 3.7217\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.7402 - mse: 22.6284 - mae: 3.6713 - val_loss: 22.9039 - val_mse: 22.7897 - val_mae: 3.7076\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5415 - mse: 22.4261 - mae: 3.6537 - val_loss: 22.6295 - val_mse: 22.5117 - val_mae: 3.6910\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.4270 - mse: 22.3079 - mae: 3.6480 - val_loss: 22.4281 - val_mse: 22.3065 - val_mae: 3.6725\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2809 - mse: 22.1581 - mae: 3.6340 - val_loss: 22.4503 - val_mse: 22.3251 - val_mae: 3.6680\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2037 - mse: 22.0775 - mae: 3.6270 - val_loss: 22.0883 - val_mse: 21.9599 - val_mae: 3.6390\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0620 - mse: 21.9326 - mae: 3.6138 - val_loss: 21.9532 - val_mse: 21.8216 - val_mae: 3.6332\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.9238 - mse: 21.7912 - mae: 3.6081 - val_loss: 21.7422 - val_mse: 21.6075 - val_mae: 3.6165\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8265 - mse: 21.6908 - mae: 3.5977 - val_loss: 21.7848 - val_mse: 21.6468 - val_mae: 3.6180\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7765 - mse: 21.6379 - mae: 3.5921 - val_loss: 21.7849 - val_mse: 21.6441 - val_mae: 3.6156\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.6860 - mse: 21.5445 - mae: 3.5835 - val_loss: 21.6581 - val_mse: 21.5145 - val_mae: 3.6031\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5491 - mse: 21.4049 - mae: 3.5751 - val_loss: 21.6492 - val_mse: 21.5030 - val_mae: 3.6008\n",
      "bias 0.001948968\n",
      "si 0.45875007\n",
      "rmse 0.046371333\n",
      "kgeprime [0.8262559]\n",
      "rmse_95 0.07254428\n",
      "rmse_99 0.07742237\n",
      "pearson 0.877752929504148\n",
      "pearson_95 0.6697246765711021\n",
      "pearson_99 0.6125634941624808\n",
      "rscore 0.7683840141072786\n",
      "rscore_95 -2.533146456661588\n",
      "rscore_99 -11.489819452313847\n",
      "nse [0.76838401]\n",
      "nse_95 [-2.53314646]\n",
      "nse_99 [-11.48981945]\n",
      "kge [0.78634854]\n",
      "ext_kge_95 [0.26636924]\n",
      "ext_kge_99 [-0.76593208]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([25641 25665 25689], shape=(3,), dtype=int64) Times out: tf.Tensor(25689, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([56742 56766 56790], shape=(3,), dtype=int64) Times out: tf.Tensor(56790, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([15748 15772 15796], shape=(3,), dtype=int64) Times out: tf.Tensor(15796, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([27606 27630 27654], shape=(3,), dtype=int64) Times out: tf.Tensor(27654, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_330\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_331 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_660 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_661 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_330 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_660 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_661 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 33.1991 - mse: 33.1538 - mae: 4.4142 - val_loss: 23.9724 - val_mse: 23.9154 - val_mae: 3.6716\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 25.4078 - mse: 25.3461 - mae: 3.9141 - val_loss: 23.6077 - val_mse: 23.5411 - val_mae: 3.6410\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.8384 - mse: 24.7694 - mae: 3.8733 - val_loss: 23.5792 - val_mse: 23.5064 - val_mae: 3.6379\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.6704 - mse: 24.5954 - mae: 3.8612 - val_loss: 23.1863 - val_mse: 23.1079 - val_mae: 3.6014\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.3736 - mse: 24.2933 - mae: 3.8394 - val_loss: 23.0979 - val_mse: 23.0143 - val_mae: 3.5925\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.1293 - mse: 24.0439 - mae: 3.8208 - val_loss: 22.9824 - val_mse: 22.8938 - val_mae: 3.5872\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.0428 - mse: 23.9523 - mae: 3.8144 - val_loss: 22.9006 - val_mse: 22.8068 - val_mae: 3.5835\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.8151 - mse: 23.7191 - mae: 3.7944 - val_loss: 22.7708 - val_mse: 22.6711 - val_mae: 3.5737\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.6358 - mse: 23.5340 - mae: 3.7808 - val_loss: 22.6134 - val_mse: 22.5076 - val_mae: 3.5622\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.3274 - mse: 23.2195 - mae: 3.7576 - val_loss: 22.4878 - val_mse: 22.3761 - val_mae: 3.5539\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.2501 - mse: 23.1364 - mae: 3.7474 - val_loss: 22.3103 - val_mse: 22.1930 - val_mae: 3.5427\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.9555 - mse: 22.8364 - mae: 3.7213 - val_loss: 22.2819 - val_mse: 22.1595 - val_mae: 3.5391\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.7764 - mse: 22.6524 - mae: 3.7054 - val_loss: 22.0179 - val_mse: 21.8906 - val_mae: 3.5181\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.6241 - mse: 22.4956 - mae: 3.6945 - val_loss: 21.9193 - val_mse: 21.7880 - val_mae: 3.5144\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.4580 - mse: 22.3258 - mae: 3.6812 - val_loss: 21.7206 - val_mse: 21.5856 - val_mae: 3.4944\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.3490 - mse: 22.2132 - mae: 3.6731 - val_loss: 21.7678 - val_mse: 21.6293 - val_mae: 3.4987\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.2375 - mse: 22.0985 - mae: 3.6615 - val_loss: 21.8256 - val_mse: 21.6844 - val_mae: 3.4989\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.0586 - mse: 21.9169 - mae: 3.6518 - val_loss: 21.7371 - val_mse: 21.5930 - val_mae: 3.4937\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.9388 - mse: 21.7943 - mae: 3.6389 - val_loss: 21.7296 - val_mse: 21.5831 - val_mae: 3.4908\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 21.8997 - mse: 21.7528 - mae: 3.6334 - val_loss: 21.6688 - val_mse: 21.5197 - val_mae: 3.4958\n",
      "bias 0.0031961242\n",
      "si 0.49868274\n",
      "rmse 0.04638939\n",
      "kgeprime [0.80714847]\n",
      "rmse_95 0.09120711\n",
      "rmse_99 0.16282015\n",
      "pearson 0.8527908163414746\n",
      "pearson_95 0.17004364239870243\n",
      "pearson_99 -0.16818535882923746\n",
      "rscore 0.7258649230315942\n",
      "rscore_95 -2.406125595050696\n",
      "rscore_99 -9.403027068186134\n",
      "nse [0.72586492]\n",
      "nse_95 [-2.4061256]\n",
      "nse_99 [-9.40302707]\n",
      "kge [0.75595655]\n",
      "ext_kge_95 [0.12671953]\n",
      "ext_kge_99 [-0.3084448]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([34978 35002 35026], shape=(3,), dtype=int64) Times out: tf.Tensor(35026, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([25328 25352 25376], shape=(3,), dtype=int64) Times out: tf.Tensor(25376, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([9553 9577 9601], shape=(3,), dtype=int64) Times out: tf.Tensor(9601, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([25752 25776 25800], shape=(3,), dtype=int64) Times out: tf.Tensor(25800, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_331\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_332 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_662 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_663 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_331 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_662 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_331 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_663 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 31.6616 - mse: 31.6086 - mae: 4.2903 - val_loss: 23.0335 - val_mse: 22.9737 - val_mae: 3.7304\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 25.5824 - mse: 25.5196 - mae: 3.9003 - val_loss: 23.1565 - val_mse: 23.0904 - val_mae: 3.7296\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.8955 - mse: 24.8258 - mae: 3.8470 - val_loss: 22.3157 - val_mse: 22.2426 - val_mae: 3.6692\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.4524 - mse: 24.3756 - mae: 3.8145 - val_loss: 21.8300 - val_mse: 21.7499 - val_mae: 3.6326\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.0442 - mse: 23.9611 - mae: 3.7854 - val_loss: 21.5317 - val_mse: 21.4456 - val_mae: 3.6112\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.8414 - mse: 23.7526 - mae: 3.7725 - val_loss: 21.4342 - val_mse: 21.3427 - val_mae: 3.5937\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.4790 - mse: 23.3844 - mae: 3.7401 - val_loss: 21.0106 - val_mse: 20.9133 - val_mae: 3.5721\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.2056 - mse: 23.1052 - mae: 3.7205 - val_loss: 21.0786 - val_mse: 20.9756 - val_mae: 3.5586\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 22.9642 - mse: 22.8583 - mae: 3.6988 - val_loss: 20.5970 - val_mse: 20.4885 - val_mae: 3.5335\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.7628 - mse: 22.6515 - mae: 3.6858 - val_loss: 20.4317 - val_mse: 20.3181 - val_mae: 3.5190\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5987 - mse: 22.4824 - mae: 3.6710 - val_loss: 20.2690 - val_mse: 20.1505 - val_mae: 3.4974\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3907 - mse: 22.2698 - mae: 3.6526 - val_loss: 20.2131 - val_mse: 20.0900 - val_mae: 3.4930\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2106 - mse: 22.0851 - mae: 3.6380 - val_loss: 20.1115 - val_mse: 19.9841 - val_mae: 3.4835\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0890 - mse: 21.9593 - mae: 3.6294 - val_loss: 19.9285 - val_mse: 19.7969 - val_mae: 3.4692\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8817 - mse: 21.7482 - mae: 3.6085 - val_loss: 19.8688 - val_mse: 19.7335 - val_mae: 3.4664\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7776 - mse: 21.6403 - mae: 3.6000 - val_loss: 19.7849 - val_mse: 19.6458 - val_mae: 3.4653\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7481 - mse: 21.6073 - mae: 3.6003 - val_loss: 19.7216 - val_mse: 19.5790 - val_mae: 3.4556\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5684 - mse: 21.4243 - mae: 3.5797 - val_loss: 19.5715 - val_mse: 19.4255 - val_mae: 3.4494\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.6074 - mse: 21.4600 - mae: 3.5826 - val_loss: 19.7572 - val_mse: 19.6081 - val_mae: 3.4469\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.4838 - mse: 21.3334 - mae: 3.5768 - val_loss: 19.6902 - val_mse: 19.5382 - val_mae: 3.4426\n",
      "bias -0.0038539346\n",
      "si 0.49249685\n",
      "rmse 0.0442021\n",
      "kgeprime [0.70852525]\n",
      "rmse_95 0.055205\n",
      "rmse_99 0.06869499\n",
      "pearson 0.8566128110476148\n",
      "pearson_95 0.5229578952095031\n",
      "pearson_99 0.5993517003055105\n",
      "rscore 0.7299683188262439\n",
      "rscore_95 -1.4307642841387045\n",
      "rscore_99 -5.324230839731495\n",
      "nse [0.72996832]\n",
      "nse_95 [-1.43076428]\n",
      "nse_99 [-5.32423084]\n",
      "kge [0.77545423]\n",
      "ext_kge_95 [0.32751933]\n",
      "ext_kge_99 [0.05698903]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([117795 117819 117843], shape=(3,), dtype=int64) Times out: tf.Tensor(117843, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([160262 160286 160310], shape=(3,), dtype=int64) Times out: tf.Tensor(160310, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([48305 48329 48353], shape=(3,), dtype=int64) Times out: tf.Tensor(48353, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([178421 178445 178469], shape=(3,), dtype=int64) Times out: tf.Tensor(178469, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_332\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_333 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_664 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_665 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_332 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_664 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_332 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_665 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 34.6529 - mse: 34.6055 - mae: 4.4724 - val_loss: 22.5862 - val_mse: 22.5333 - val_mae: 3.7329\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.9736 - mse: 26.9193 - mae: 3.9880 - val_loss: 21.7071 - val_mse: 21.6517 - val_mae: 3.6681\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.3068 - mse: 26.2500 - mae: 3.9401 - val_loss: 21.3229 - val_mse: 21.2648 - val_mae: 3.6311\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.1782 - mse: 26.1185 - mae: 3.9299 - val_loss: 21.2636 - val_mse: 21.2028 - val_mae: 3.6286\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.8698 - mse: 25.8077 - mae: 3.9065 - val_loss: 21.2180 - val_mse: 21.1547 - val_mae: 3.6390\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.8217 - mse: 25.7571 - mae: 3.9019 - val_loss: 20.7843 - val_mse: 20.7185 - val_mae: 3.5871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.5956 - mse: 25.5282 - mae: 3.8873 - val_loss: 20.5970 - val_mse: 20.5280 - val_mae: 3.5755\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.4236 - mse: 25.3526 - mae: 3.8759 - val_loss: 20.3991 - val_mse: 20.3260 - val_mae: 3.5558\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.2020 - mse: 25.1269 - mae: 3.8572 - val_loss: 20.2276 - val_mse: 20.1503 - val_mae: 3.5385\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.8795 - mse: 24.8001 - mae: 3.8340 - val_loss: 20.5782 - val_mse: 20.4964 - val_mae: 3.5915\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.8470 - mse: 24.7632 - mae: 3.8298 - val_loss: 19.7266 - val_mse: 19.6406 - val_mae: 3.5058\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.4576 - mse: 24.3695 - mae: 3.7997 - val_loss: 19.6538 - val_mse: 19.5638 - val_mae: 3.5005\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.2984 - mse: 24.2064 - mae: 3.7870 - val_loss: 19.3693 - val_mse: 19.2754 - val_mae: 3.4687\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.1444 - mse: 24.0484 - mae: 3.7733 - val_loss: 19.2549 - val_mse: 19.1570 - val_mae: 3.4611\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.9938 - mse: 23.8940 - mae: 3.7599 - val_loss: 19.1945 - val_mse: 19.0931 - val_mae: 3.4492\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.9828 - mse: 23.8794 - mae: 3.7586 - val_loss: 19.0881 - val_mse: 18.9829 - val_mae: 3.4413\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.8411 - mse: 23.7343 - mae: 3.7486 - val_loss: 19.1821 - val_mse: 19.0737 - val_mae: 3.4531\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.6972 - mse: 23.5874 - mae: 3.7417 - val_loss: 18.9943 - val_mse: 18.8832 - val_mae: 3.4301\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.6594 - mse: 23.5468 - mae: 3.7353 - val_loss: 18.9067 - val_mse: 18.7928 - val_mae: 3.4273\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.5982 - mse: 23.4832 - mae: 3.7283 - val_loss: 18.9548 - val_mse: 18.8385 - val_mae: 3.4274\n",
      "bias -0.0016620957\n",
      "si 0.46576667\n",
      "rmse 0.043403395\n",
      "kgeprime [0.77160819]\n",
      "rmse_95 0.0663501\n",
      "rmse_99 0.09381489\n",
      "pearson 0.8715963182869136\n",
      "pearson_95 0.6305993710798252\n",
      "pearson_99 0.6165778024918583\n",
      "rscore 0.7593050606670002\n",
      "rscore_95 -0.8206044941951391\n",
      "rscore_99 -2.875611441337254\n",
      "nse [0.75930506]\n",
      "nse_95 [-0.82060449]\n",
      "nse_99 [-2.87561144]\n",
      "kge [0.80641119]\n",
      "ext_kge_95 [0.57215581]\n",
      "ext_kge_99 [0.53227258]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -41.99 -41.68 -41.37 ... -35.75 -35.44\n",
      "  * longitude       (longitude) float32 174.7 175.0 175.3 ... 180.6 180.9 181.2\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 3.69 2.918 ... 5.655\n",
      "    vgrd10m         (time, latitude, longitude) float32 -5.589 -5.999 ... -1.613\n",
      "    uw2             (time, latitude, longitude) float32 13.61 8.517 ... 31.98\n",
      "    vw2             (time, latitude, longitude) float32 31.24 35.99 ... 2.602\n",
      "    wind_magnitude  (time, latitude, longitude) float32 6.697 6.671 ... 5.881\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([80324 80348 80372], shape=(3,), dtype=int64) Times out: tf.Tensor(80372, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([29593 29617 29641], shape=(3,), dtype=int64) Times out: tf.Tensor(29641, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([11879 11903 11927], shape=(3,), dtype=int64) Times out: tf.Tensor(11927, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([75546 75570 75594], shape=(3,), dtype=int64) Times out: tf.Tensor(75594, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_333\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_334 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_666 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_667 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_333 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_666 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_333 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_667 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 29.2012 - mse: 29.1609 - mae: 4.2004 - val_loss: 20.0588 - val_mse: 20.0091 - val_mae: 3.5515\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6620 - mse: 22.6074 - mae: 3.7363 - val_loss: 19.2367 - val_mse: 19.1784 - val_mae: 3.4681\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.1401 - mse: 22.0787 - mae: 3.6909 - val_loss: 19.0041 - val_mse: 18.9401 - val_mae: 3.4383\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.8497 - mse: 21.7834 - mae: 3.6696 - val_loss: 18.7791 - val_mse: 18.7108 - val_mae: 3.4220\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.6657 - mse: 21.5954 - mae: 3.6561 - val_loss: 18.5320 - val_mse: 18.4600 - val_mae: 3.4029\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5910 - mse: 21.5175 - mae: 3.6455 - val_loss: 18.2377 - val_mse: 18.1625 - val_mae: 3.3827\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4199 - mse: 21.3429 - mae: 3.6331 - val_loss: 18.1232 - val_mse: 18.0444 - val_mae: 3.3646\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.2853 - mse: 21.2047 - mae: 3.6197 - val_loss: 18.0311 - val_mse: 17.9488 - val_mae: 3.3569\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1341 - mse: 21.0499 - mae: 3.6043 - val_loss: 17.7622 - val_mse: 17.6762 - val_mae: 3.3436\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.9484 - mse: 20.8605 - mae: 3.5921 - val_loss: 17.8040 - val_mse: 17.7145 - val_mae: 3.3428\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8777 - mse: 20.7864 - mae: 3.5836 - val_loss: 17.4988 - val_mse: 17.4056 - val_mae: 3.3129\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7294 - mse: 20.6346 - mae: 3.5741 - val_loss: 17.4987 - val_mse: 17.4021 - val_mae: 3.3166\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6432 - mse: 20.5453 - mae: 3.5666 - val_loss: 17.4496 - val_mse: 17.3501 - val_mae: 3.3058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5697 - mse: 20.4691 - mae: 3.5593 - val_loss: 17.3636 - val_mse: 17.2615 - val_mae: 3.3008\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4737 - mse: 20.3703 - mae: 3.5521 - val_loss: 17.3805 - val_mse: 17.2758 - val_mae: 3.3064\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4139 - mse: 20.3078 - mae: 3.5470 - val_loss: 17.3195 - val_mse: 17.2121 - val_mae: 3.2923\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3529 - mse: 20.2443 - mae: 3.5390 - val_loss: 17.3002 - val_mse: 17.1903 - val_mae: 3.2928\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3926 - mse: 20.2818 - mae: 3.5408 - val_loss: 17.4536 - val_mse: 17.3415 - val_mae: 3.3002\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.2474 - mse: 20.1342 - mae: 3.5295 - val_loss: 17.3875 - val_mse: 17.2733 - val_mae: 3.2975\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1558 - mse: 20.0403 - mae: 3.5223 - val_loss: 17.3739 - val_mse: 17.2571 - val_mae: 3.3069\n",
      "bias -0.0015179592\n",
      "si 0.5417008\n",
      "rmse 0.0415417\n",
      "kgeprime [0.63430573]\n",
      "rmse_95 0.067060925\n",
      "rmse_99 0.07858047\n",
      "pearson 0.8282820222368035\n",
      "pearson_95 0.5944791611195127\n",
      "pearson_99 0.5406507205294606\n",
      "rscore 0.675493964527025\n",
      "rscore_95 -3.190933659661983\n",
      "rscore_99 -9.141450646294917\n",
      "nse [0.67549396]\n",
      "nse_95 [-3.19093366]\n",
      "nse_99 [-9.14145065]\n",
      "kge [0.6716646]\n",
      "ext_kge_95 [0.44961764]\n",
      "ext_kge_99 [0.31032504]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([112457 112481 112505], shape=(3,), dtype=int64) Times out: tf.Tensor(112505, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([69313 69337 69361], shape=(3,), dtype=int64) Times out: tf.Tensor(69361, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([492 516 540], shape=(3,), dtype=int64) Times out: tf.Tensor(540, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([34424 34448 34472], shape=(3,), dtype=int64) Times out: tf.Tensor(34472, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_334\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_335 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_668 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_669 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_334 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_668 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_669 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.8050 - mse: 27.7550 - mae: 4.1171 - val_loss: 24.1804 - val_mse: 24.1234 - val_mae: 3.8563\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.8152 - mse: 22.7526 - mae: 3.7574 - val_loss: 22.5248 - val_mse: 22.4565 - val_mae: 3.7334\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.8482 - mse: 21.7762 - mae: 3.6749 - val_loss: 21.7939 - val_mse: 21.7183 - val_mae: 3.6700\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.5530 - mse: 21.4749 - mae: 3.6449 - val_loss: 21.6105 - val_mse: 21.5298 - val_mae: 3.6492\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.2109 - mse: 21.1286 - mae: 3.6170 - val_loss: 21.0480 - val_mse: 20.9634 - val_mae: 3.6001\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.9290 - mse: 20.8431 - mae: 3.5936 - val_loss: 21.0410 - val_mse: 20.9531 - val_mae: 3.5994\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.6356 - mse: 20.5469 - mae: 3.5736 - val_loss: 20.4424 - val_mse: 20.3518 - val_mae: 3.5463\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.4655 - mse: 20.3742 - mae: 3.5577 - val_loss: 20.5459 - val_mse: 20.4531 - val_mae: 3.5581\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.3970 - mse: 20.3036 - mae: 3.5499 - val_loss: 20.1632 - val_mse: 20.0684 - val_mae: 3.5222\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.2318 - mse: 20.1363 - mae: 3.5340 - val_loss: 19.8837 - val_mse: 19.7868 - val_mae: 3.4949\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.2032 - mse: 20.1056 - mae: 3.5304 - val_loss: 19.9868 - val_mse: 19.8876 - val_mae: 3.5061\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.1022 - mse: 20.0025 - mae: 3.5243 - val_loss: 19.6859 - val_mse: 19.5851 - val_mae: 3.4768\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.9962 - mse: 19.8947 - mae: 3.5111 - val_loss: 19.8553 - val_mse: 19.7526 - val_mae: 3.4923\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.9247 - mse: 19.8214 - mae: 3.5050 - val_loss: 19.7054 - val_mse: 19.6010 - val_mae: 3.4758\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.8380 - mse: 19.7330 - mae: 3.4976 - val_loss: 19.7268 - val_mse: 19.6207 - val_mae: 3.4780\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7990 - mse: 19.6923 - mae: 3.4964 - val_loss: 19.7248 - val_mse: 19.6169 - val_mae: 3.4806\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7335 - mse: 19.6252 - mae: 3.4917 - val_loss: 19.3734 - val_mse: 19.2640 - val_mae: 3.4382\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7173 - mse: 19.6074 - mae: 3.4881 - val_loss: 19.6327 - val_mse: 19.5216 - val_mae: 3.4708\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.7136 - mse: 19.6021 - mae: 3.4872 - val_loss: 19.6532 - val_mse: 19.5406 - val_mae: 3.4728\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5913 - mse: 19.4782 - mae: 3.4751 - val_loss: 19.6152 - val_mse: 19.5010 - val_mae: 3.4677\n",
      "bias 0.001162563\n",
      "si 0.5370256\n",
      "rmse 0.044159904\n",
      "kgeprime [0.72652447]\n",
      "rmse_95 0.06996534\n",
      "rmse_99 0.075603835\n",
      "pearson 0.8302311117421705\n",
      "pearson_95 0.5969566115844329\n",
      "pearson_99 0.49948951646465206\n",
      "rscore 0.6831659368526297\n",
      "rscore_95 -3.046625499443282\n",
      "rscore_99 -3.5305754917337007\n",
      "nse [0.68316594]\n",
      "nse_95 [-3.0466255]\n",
      "nse_99 [-3.53057549]\n",
      "kge [0.69691301]\n",
      "ext_kge_95 [0.38238959]\n",
      "ext_kge_99 [0.4172481]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([63318 63342 63366], shape=(3,), dtype=int64) Times out: tf.Tensor(63366, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([12435 12459 12483], shape=(3,), dtype=int64) Times out: tf.Tensor(12483, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([50433 50457 50481], shape=(3,), dtype=int64) Times out: tf.Tensor(50481, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([52894 52918 52942], shape=(3,), dtype=int64) Times out: tf.Tensor(52942, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_335\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_336 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_670 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_671 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_335 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_670 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_335 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_671 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 26.5132 - mse: 26.4660 - mae: 4.0324 - val_loss: 21.4162 - val_mse: 21.3593 - val_mae: 3.5513\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.7878 - mse: 21.7244 - mae: 3.6835 - val_loss: 20.6126 - val_mse: 20.5427 - val_mae: 3.4871\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.2968 - mse: 21.2226 - mae: 3.6407 - val_loss: 20.1674 - val_mse: 20.0885 - val_mae: 3.4530\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.9040 - mse: 20.8218 - mae: 3.6108 - val_loss: 20.5296 - val_mse: 20.4434 - val_mae: 3.4880\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.7140 - mse: 20.6252 - mae: 3.5965 - val_loss: 19.8511 - val_mse: 19.7589 - val_mae: 3.4242\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.5916 - mse: 20.4978 - mae: 3.5820 - val_loss: 20.3366 - val_mse: 20.2399 - val_mae: 3.4779\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 20.3813 - mse: 20.2833 - mae: 3.5652 - val_loss: 19.2317 - val_mse: 19.1306 - val_mae: 3.3705\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2548 - mse: 20.1527 - mae: 3.5557 - val_loss: 19.1974 - val_mse: 19.0928 - val_mae: 3.3668\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.0987 - mse: 19.9932 - mae: 3.5416 - val_loss: 19.5136 - val_mse: 19.4057 - val_mae: 3.3966\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.8922 - mse: 19.7837 - mae: 3.5232 - val_loss: 19.4966 - val_mse: 19.3859 - val_mae: 3.3939\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.8526 - mse: 19.7412 - mae: 3.5156 - val_loss: 19.3752 - val_mse: 19.2620 - val_mae: 3.3845\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.7666 - mse: 19.6525 - mae: 3.5140 - val_loss: 19.1483 - val_mse: 19.0323 - val_mae: 3.3670\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.6907 - mse: 19.5740 - mae: 3.5063 - val_loss: 19.0823 - val_mse: 18.9637 - val_mae: 3.3579\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.5774 - mse: 19.4583 - mae: 3.4952 - val_loss: 19.1818 - val_mse: 19.0608 - val_mae: 3.3653\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.5176 - mse: 19.3961 - mae: 3.4889 - val_loss: 18.8813 - val_mse: 18.7580 - val_mae: 3.3404\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.4793 - mse: 19.3556 - mae: 3.4866 - val_loss: 19.0381 - val_mse: 18.9129 - val_mae: 3.3520\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.4604 - mse: 19.3348 - mae: 3.4827 - val_loss: 19.0161 - val_mse: 18.8888 - val_mae: 3.3490\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2955 - mse: 19.1679 - mae: 3.4718 - val_loss: 18.6932 - val_mse: 18.5640 - val_mae: 3.3235\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.3049 - mse: 19.1752 - mae: 3.4707 - val_loss: 18.8229 - val_mse: 18.6915 - val_mae: 3.3351\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 19.2690 - mse: 19.1370 - mae: 3.4686 - val_loss: 19.4109 - val_mse: 19.2773 - val_mae: 3.3838\n",
      "bias -0.010365747\n",
      "si 0.5380543\n",
      "rmse 0.043905836\n",
      "kgeprime [0.35287558]\n",
      "rmse_95 0.06759157\n",
      "rmse_99 0.103181176\n",
      "pearson 0.8268308261983219\n",
      "pearson_95 0.5403595936547074\n",
      "pearson_99 0.5326383017727951\n",
      "rscore 0.6649393626961926\n",
      "rscore_95 -1.4385995893909538\n",
      "rscore_99 -8.621383512519474\n",
      "nse [0.66493936]\n",
      "nse_95 [-1.43859959]\n",
      "nse_99 [-8.62138351]\n",
      "kge [0.48701537]\n",
      "ext_kge_95 [0.42872118]\n",
      "ext_kge_99 [-0.47989752]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([26574 26598 26622], shape=(3,), dtype=int64) Times out: tf.Tensor(26622, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([29414 29438 29462], shape=(3,), dtype=int64) Times out: tf.Tensor(29462, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([579 603 627], shape=(3,), dtype=int64) Times out: tf.Tensor(627, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([19897 19921 19945], shape=(3,), dtype=int64) Times out: tf.Tensor(19945, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_336\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_337 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_672 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_673 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_336 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_672 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_673 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 26.3525 - mse: 26.3026 - mae: 3.9774 - val_loss: 21.4085 - val_mse: 21.3459 - val_mae: 3.6960\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.6720 - mse: 21.6022 - mae: 3.6346 - val_loss: 20.7827 - val_mse: 20.7063 - val_mae: 3.6457\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.0798 - mse: 20.9983 - mae: 3.5865 - val_loss: 20.6574 - val_mse: 20.5701 - val_mae: 3.6428\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.7144 - mse: 20.6232 - mae: 3.5575 - val_loss: 20.3309 - val_mse: 20.2352 - val_mae: 3.6140\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.4712 - mse: 20.3726 - mae: 3.5355 - val_loss: 20.4918 - val_mse: 20.3898 - val_mae: 3.6350\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.2431 - mse: 20.1388 - mae: 3.5160 - val_loss: 20.1084 - val_mse: 20.0012 - val_mae: 3.6005\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.1557 - mse: 20.0470 - mae: 3.5085 - val_loss: 20.0341 - val_mse: 19.9235 - val_mae: 3.5936\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.0214 - mse: 19.9091 - mae: 3.4937 - val_loss: 19.7328 - val_mse: 19.6191 - val_mae: 3.5653\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.8838 - mse: 19.7684 - mae: 3.4852 - val_loss: 19.8974 - val_mse: 19.7806 - val_mae: 3.5819\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.7864 - mse: 19.6681 - mae: 3.4744 - val_loss: 20.0150 - val_mse: 19.8954 - val_mae: 3.5940\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.7354 - mse: 19.6146 - mae: 3.4701 - val_loss: 19.9185 - val_mse: 19.7969 - val_mae: 3.5828\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6862 - mse: 19.5632 - mae: 3.4631 - val_loss: 19.5073 - val_mse: 19.3835 - val_mae: 3.5434\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.6130 - mse: 19.4883 - mae: 3.4616 - val_loss: 19.4531 - val_mse: 19.3275 - val_mae: 3.5387\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.5431 - mse: 19.4164 - mae: 3.4506 - val_loss: 19.3856 - val_mse: 19.2581 - val_mae: 3.5313\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4492 - mse: 19.3203 - mae: 3.4440 - val_loss: 19.5509 - val_mse: 19.4212 - val_mae: 3.5476\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4028 - mse: 19.2720 - mae: 3.4428 - val_loss: 19.4282 - val_mse: 19.2965 - val_mae: 3.5364\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3430 - mse: 19.2101 - mae: 3.4371 - val_loss: 19.4679 - val_mse: 19.3344 - val_mae: 3.5404\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.3287 - mse: 19.1939 - mae: 3.4379 - val_loss: 19.5178 - val_mse: 19.3822 - val_mae: 3.5453\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2488 - mse: 19.1120 - mae: 3.4284 - val_loss: 19.4452 - val_mse: 19.3076 - val_mae: 3.5356\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.1771 - mse: 19.0384 - mae: 3.4244 - val_loss: 19.3231 - val_mse: 19.1837 - val_mae: 3.5241\n",
      "bias -0.0028049115\n",
      "si 0.56980604\n",
      "rmse 0.043799218\n",
      "kgeprime [0.66726135]\n",
      "rmse_95 0.05619214\n",
      "rmse_99 0.07166305\n",
      "pearson 0.8048226881220657\n",
      "pearson_95 0.3334831882326077\n",
      "pearson_99 0.4691966132407743\n",
      "rscore 0.643586587232616\n",
      "rscore_95 -4.7131678943575706\n",
      "rscore_99 -20.309973910648154\n",
      "nse [0.64358659]\n",
      "nse_95 [-4.71316789]\n",
      "nse_99 [-20.30997391]\n",
      "kge [0.72740878]\n",
      "ext_kge_95 [0.06349291]\n",
      "ext_kge_99 [-0.30855423]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([155287 155311 155335], shape=(3,), dtype=int64) Times out: tf.Tensor(155335, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([187217 187241 187265], shape=(3,), dtype=int64) Times out: tf.Tensor(187265, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([106174 106198 106222], shape=(3,), dtype=int64) Times out: tf.Tensor(106222, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([90979 91003 91027], shape=(3,), dtype=int64) Times out: tf.Tensor(91027, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_337\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_338 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_674 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_675 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_337 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_674 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_675 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 27.5387 - mse: 27.4954 - mae: 4.0900 - val_loss: 21.0350 - val_mse: 20.9837 - val_mae: 3.6356\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.5763 - mse: 22.5199 - mae: 3.7307 - val_loss: 19.7452 - val_mse: 19.6841 - val_mae: 3.5155\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.9999 - mse: 21.9345 - mae: 3.6829 - val_loss: 19.2998 - val_mse: 19.2304 - val_mae: 3.4750\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.5934 - mse: 21.5203 - mae: 3.6485 - val_loss: 19.3948 - val_mse: 19.3186 - val_mae: 3.4966\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4113 - mse: 21.3319 - mae: 3.6362 - val_loss: 18.9395 - val_mse: 18.8576 - val_mae: 3.4489\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1794 - mse: 21.0950 - mae: 3.6160 - val_loss: 18.6692 - val_mse: 18.5826 - val_mae: 3.4221\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.0272 - mse: 20.9387 - mae: 3.6009 - val_loss: 18.5817 - val_mse: 18.4916 - val_mae: 3.4170\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8764 - mse: 20.7845 - mae: 3.5878 - val_loss: 18.5504 - val_mse: 18.4572 - val_mae: 3.4165\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.7905 - mse: 20.6956 - mae: 3.5759 - val_loss: 18.5098 - val_mse: 18.4136 - val_mae: 3.4186\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6231 - mse: 20.5254 - mae: 3.5661 - val_loss: 18.2002 - val_mse: 18.1014 - val_mae: 3.3868\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5409 - mse: 20.4406 - mae: 3.5553 - val_loss: 17.8859 - val_mse: 17.7845 - val_mae: 3.3481\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4810 - mse: 20.3785 - mae: 3.5531 - val_loss: 17.8316 - val_mse: 17.7282 - val_mae: 3.3414\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.4278 - mse: 20.3234 - mae: 3.5456 - val_loss: 18.0577 - val_mse: 17.9523 - val_mae: 3.3780\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3389 - mse: 20.2323 - mae: 3.5379 - val_loss: 17.9317 - val_mse: 17.8243 - val_mae: 3.3615\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.2491 - mse: 20.1407 - mae: 3.5333 - val_loss: 18.1768 - val_mse: 18.0678 - val_mae: 3.3959\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1371 - mse: 20.0271 - mae: 3.5228 - val_loss: 17.9330 - val_mse: 17.8225 - val_mae: 3.3661\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.1320 - mse: 20.0205 - mae: 3.5211 - val_loss: 18.2403 - val_mse: 18.1281 - val_mae: 3.4056\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.0355 - mse: 19.9223 - mae: 3.5123 - val_loss: 17.6184 - val_mse: 17.5043 - val_mae: 3.3276\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.0473 - mse: 19.9323 - mae: 3.5136 - val_loss: 18.1594 - val_mse: 18.0438 - val_mae: 3.3913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.9476 - mse: 19.8309 - mae: 3.5061 - val_loss: 17.8286 - val_mse: 17.7115 - val_mae: 3.3570\n",
      "bias 0.0068888194\n",
      "si 0.5489242\n",
      "rmse 0.042085037\n",
      "kgeprime [0.63777692]\n",
      "rmse_95 0.0663169\n",
      "rmse_99 0.08381747\n",
      "pearson 0.8147284585229495\n",
      "pearson_95 0.5644249040332745\n",
      "pearson_99 0.49626809273344225\n",
      "rscore 0.6545105300471239\n",
      "rscore_95 -3.216668426097942\n",
      "rscore_99 -9.569928024302413\n",
      "nse [0.65451053]\n",
      "nse_95 [-3.21666843]\n",
      "nse_99 [-9.56992802]\n",
      "kge [0.61271751]\n",
      "ext_kge_95 [0.41111621]\n",
      "ext_kge_99 [0.22891037]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -41.37 -41.06 -40.75 ... -35.13 -34.81\n",
      "  * longitude       (longitude) float32 171.6 171.9 172.2 ... 177.5 177.8 178.1\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 1.169 0.9293 ... 7.251\n",
      "    vgrd10m         (time, latitude, longitude) float32 0.109 -0.2692 ... -2.079\n",
      "    uw2             (time, latitude, longitude) float32 1.366 0.8637 ... 52.58\n",
      "    vw2             (time, latitude, longitude) float32 0.01187 ... 4.321\n",
      "    wind_magnitude  (time, latitude, longitude) float32 1.174 0.9675 ... 7.543\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([106535 106559 106583], shape=(3,), dtype=int64) Times out: tf.Tensor(106583, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([89146 89170 89194], shape=(3,), dtype=int64) Times out: tf.Tensor(89194, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([73799 73823 73847], shape=(3,), dtype=int64) Times out: tf.Tensor(73847, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([36629 36653 36677], shape=(3,), dtype=int64) Times out: tf.Tensor(36677, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_338\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_339 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_676 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_677 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_338 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_676 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_677 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 32.6914 - mse: 32.6429 - mae: 4.3353 - val_loss: 22.2638 - val_mse: 22.2021 - val_mae: 3.6357\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.9983 - mse: 23.9306 - mae: 3.7652 - val_loss: 21.4252 - val_mse: 21.3530 - val_mae: 3.5658\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.3658 - mse: 23.2900 - mae: 3.7150 - val_loss: 20.8939 - val_mse: 20.8146 - val_mae: 3.5296\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0464 - mse: 22.9642 - mae: 3.6892 - val_loss: 20.5827 - val_mse: 20.4976 - val_mae: 3.5093\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.8529 - mse: 22.7653 - mae: 3.6772 - val_loss: 20.1530 - val_mse: 20.0626 - val_mae: 3.4672\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.6116 - mse: 22.5185 - mae: 3.6567 - val_loss: 20.0863 - val_mse: 19.9905 - val_mae: 3.4618\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.3139 - mse: 22.2152 - mae: 3.6334 - val_loss: 19.7020 - val_mse: 19.6002 - val_mae: 3.4370\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0365 - mse: 21.9322 - mae: 3.6125 - val_loss: 19.3744 - val_mse: 19.2672 - val_mae: 3.4094\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.6828 - mse: 21.5731 - mae: 3.5837 - val_loss: 18.9747 - val_mse: 18.8624 - val_mae: 3.3735\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.4692 - mse: 21.3543 - mae: 3.5658 - val_loss: 18.7780 - val_mse: 18.6606 - val_mae: 3.3631\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.3160 - mse: 21.1963 - mae: 3.5521 - val_loss: 18.5447 - val_mse: 18.4227 - val_mae: 3.3422\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.1162 - mse: 20.9921 - mae: 3.5362 - val_loss: 18.5063 - val_mse: 18.3802 - val_mae: 3.3373\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.9329 - mse: 20.8045 - mae: 3.5201 - val_loss: 18.3079 - val_mse: 18.1776 - val_mae: 3.3215\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.8471 - mse: 20.7148 - mae: 3.5123 - val_loss: 17.9174 - val_mse: 17.7834 - val_mae: 3.2844\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6705 - mse: 20.5347 - mae: 3.4988 - val_loss: 18.3565 - val_mse: 18.2192 - val_mae: 3.3288\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.6563 - mse: 20.5173 - mae: 3.4945 - val_loss: 17.8996 - val_mse: 17.7592 - val_mae: 3.2894\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5404 - mse: 20.3986 - mae: 3.4846 - val_loss: 17.8563 - val_mse: 17.7131 - val_mae: 3.2876\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5145 - mse: 20.3699 - mae: 3.4816 - val_loss: 17.6509 - val_mse: 17.5046 - val_mae: 3.2645\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3394 - mse: 20.1917 - mae: 3.4705 - val_loss: 17.6567 - val_mse: 17.5078 - val_mae: 3.2660\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3019 - mse: 20.1517 - mae: 3.4638 - val_loss: 17.6936 - val_mse: 17.5421 - val_mae: 3.2713\n",
      "bias 0.0022219582\n",
      "si 0.44818902\n",
      "rmse 0.041883256\n",
      "kgeprime [0.80864676]\n",
      "rmse_95 0.07006141\n",
      "rmse_99 0.10156646\n",
      "pearson 0.8844028247403463\n",
      "pearson_95 0.6804109401190271\n",
      "pearson_99 0.6768117628097271\n",
      "rscore 0.7748673232669212\n",
      "rscore_95 -0.46212904614877615\n",
      "rscore_99 -1.1088432338288778\n",
      "nse [0.77486732]\n",
      "nse_95 [-0.46212905]\n",
      "nse_99 [-1.10884323]\n",
      "kge [0.75987904]\n",
      "ext_kge_95 [0.58798672]\n",
      "ext_kge_99 [0.52023783]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([89000 89024 89048], shape=(3,), dtype=int64) Times out: tf.Tensor(89048, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([38500 38524 38548], shape=(3,), dtype=int64) Times out: tf.Tensor(38548, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([23144 23168 23192], shape=(3,), dtype=int64) Times out: tf.Tensor(23192, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([19683 19707 19731], shape=(3,), dtype=int64) Times out: tf.Tensor(19731, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_339\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_340 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_678 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_679 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_339 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_678 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_679 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 32.9339 - mse: 32.8871 - mae: 4.3431 - val_loss: 24.6165 - val_mse: 24.5567 - val_mae: 3.8012\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.0476 - mse: 23.9838 - mae: 3.7584 - val_loss: 23.5530 - val_mse: 23.4845 - val_mae: 3.7229\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.4138 - mse: 23.3440 - mae: 3.7075 - val_loss: 22.7677 - val_mse: 22.6948 - val_mae: 3.6784\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.0465 - mse: 22.9729 - mae: 3.6840 - val_loss: 22.7274 - val_mse: 22.6514 - val_mae: 3.6707\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.0457 - mse: 22.9697 - mae: 3.6834 - val_loss: 22.7968 - val_mse: 22.7189 - val_mae: 3.6734\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.8604 - mse: 22.7822 - mae: 3.6725 - val_loss: 22.6647 - val_mse: 22.5846 - val_mae: 3.6654\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.6875 - mse: 22.6069 - mae: 3.6553 - val_loss: 22.4669 - val_mse: 22.3840 - val_mae: 3.6527\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.5017 - mse: 22.4185 - mae: 3.6494 - val_loss: 22.1179 - val_mse: 22.0324 - val_mae: 3.6323\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.4043 - mse: 22.3187 - mae: 3.6359 - val_loss: 21.9402 - val_mse: 21.8520 - val_mae: 3.6285\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.2652 - mse: 22.1767 - mae: 3.6250 - val_loss: 22.0039 - val_mse: 21.9131 - val_mae: 3.6226\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0122 - mse: 21.9210 - mae: 3.6058 - val_loss: 21.3539 - val_mse: 21.2603 - val_mae: 3.5696\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.8593 - mse: 21.7653 - mae: 3.5932 - val_loss: 21.2700 - val_mse: 21.1736 - val_mae: 3.5712\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7379 - mse: 21.6410 - mae: 3.5830 - val_loss: 21.2825 - val_mse: 21.1830 - val_mae: 3.5595\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5236 - mse: 21.4237 - mae: 3.5693 - val_loss: 21.0513 - val_mse: 20.9491 - val_mae: 3.5338\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3608 - mse: 21.2585 - mae: 3.5519 - val_loss: 20.7934 - val_mse: 20.6887 - val_mae: 3.5252\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.3451 - mse: 21.2404 - mae: 3.5476 - val_loss: 20.9350 - val_mse: 20.8280 - val_mae: 3.5271\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.2623 - mse: 21.1553 - mae: 3.5435 - val_loss: 20.6505 - val_mse: 20.5413 - val_mae: 3.5038\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.1516 - mse: 21.0424 - mae: 3.5332 - val_loss: 20.7818 - val_mse: 20.6704 - val_mae: 3.5065\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.1071 - mse: 20.9956 - mae: 3.5273 - val_loss: 20.3565 - val_mse: 20.2429 - val_mae: 3.4855\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.9959 - mse: 20.8823 - mae: 3.5238 - val_loss: 20.2976 - val_mse: 20.1820 - val_mae: 3.4771\n",
      "bias -0.000110474524\n",
      "si 0.45770308\n",
      "rmse 0.0449244\n",
      "kgeprime [0.80563916]\n",
      "rmse_95 0.06574231\n",
      "rmse_99 0.054644868\n",
      "pearson 0.8769004639024308\n",
      "pearson_95 0.7021959607500032\n",
      "pearson_99 0.6527669984282984\n",
      "rscore 0.7683687729935399\n",
      "rscore_95 -2.092410978108777\n",
      "rscore_99 -7.377043322612886\n",
      "nse [0.76836877]\n",
      "nse_95 [-2.09241098]\n",
      "nse_99 [-7.37704332]\n",
      "kge [0.80802229]\n",
      "ext_kge_95 [0.20306879]\n",
      "ext_kge_99 [-0.99751637]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([62560 62584 62608], shape=(3,), dtype=int64) Times out: tf.Tensor(62608, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([17707 17731 17755], shape=(3,), dtype=int64) Times out: tf.Tensor(17755, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([6487 6511 6535], shape=(3,), dtype=int64) Times out: tf.Tensor(6535, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([28147 28171 28195], shape=(3,), dtype=int64) Times out: tf.Tensor(28195, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_340\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_341 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_680 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_681 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_340 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_680 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_681 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 33.5300 - mse: 33.4860 - mae: 4.4377 - val_loss: 23.0093 - val_mse: 22.9551 - val_mae: 3.5885\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 24.2922 - mse: 24.2338 - mae: 3.8214 - val_loss: 22.1083 - val_mse: 22.0446 - val_mae: 3.5071\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.4257 - mse: 23.3593 - mae: 3.7463 - val_loss: 22.0185 - val_mse: 21.9481 - val_mae: 3.5036\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 23.0309 - mse: 22.9592 - mae: 3.7203 - val_loss: 21.8945 - val_mse: 21.8193 - val_mae: 3.5041\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.7274 - mse: 22.6514 - mae: 3.6981 - val_loss: 21.4169 - val_mse: 21.3378 - val_mae: 3.4634\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.4552 - mse: 22.3754 - mae: 3.6772 - val_loss: 21.1524 - val_mse: 21.0692 - val_mae: 3.4396\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 22.2979 - mse: 22.2141 - mae: 3.6644 - val_loss: 20.9471 - val_mse: 20.8600 - val_mae: 3.4242\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.9540 - mse: 21.8656 - mae: 3.6359 - val_loss: 20.6598 - val_mse: 20.5677 - val_mae: 3.4021\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.6498 - mse: 21.5567 - mae: 3.6152 - val_loss: 20.4918 - val_mse: 20.3948 - val_mae: 3.3910\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.5597 - mse: 21.4615 - mae: 3.6037 - val_loss: 20.4410 - val_mse: 20.3389 - val_mae: 3.3920\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.1817 - mse: 21.0786 - mae: 3.5697 - val_loss: 20.2741 - val_mse: 20.1679 - val_mae: 3.3755\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.0278 - mse: 20.9203 - mae: 3.5617 - val_loss: 20.0312 - val_mse: 19.9202 - val_mae: 3.3474\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.8482 - mse: 20.7362 - mae: 3.5427 - val_loss: 19.9533 - val_mse: 19.8384 - val_mae: 3.3430\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.7187 - mse: 20.6029 - mae: 3.5320 - val_loss: 20.0757 - val_mse: 19.9570 - val_mae: 3.3532\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.6295 - mse: 20.5097 - mae: 3.5228 - val_loss: 19.8244 - val_mse: 19.7017 - val_mae: 3.3297\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.5675 - mse: 20.4437 - mae: 3.5146 - val_loss: 19.8019 - val_mse: 19.6752 - val_mae: 3.3281\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.3515 - mse: 20.2238 - mae: 3.5019 - val_loss: 19.7636 - val_mse: 19.6334 - val_mae: 3.3268\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2054 - mse: 20.0743 - mae: 3.4881 - val_loss: 19.7445 - val_mse: 19.6108 - val_mae: 3.3235\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.1681 - mse: 20.0338 - mae: 3.4838 - val_loss: 19.8112 - val_mse: 19.6743 - val_mae: 3.3264\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.1050 - mse: 19.9674 - mae: 3.4797 - val_loss: 19.7642 - val_mse: 19.6242 - val_mae: 3.3235\n",
      "bias 0.001198578\n",
      "si 0.49045464\n",
      "rmse 0.04429922\n",
      "kgeprime [0.80789237]\n",
      "rmse_95 0.08786454\n",
      "rmse_99 0.15751642\n",
      "pearson 0.8572476648592526\n",
      "pearson_95 0.2742192449172148\n",
      "pearson_99 -0.13013700146969165\n",
      "rscore 0.7344102677045805\n",
      "rscore_95 -1.7854131910948632\n",
      "rscore_99 -9.75014651677285\n",
      "nse [0.73441027]\n",
      "nse_95 [-1.78541319]\n",
      "nse_99 [-9.75014652]\n",
      "kge [0.7818514]\n",
      "ext_kge_95 [0.22874938]\n",
      "ext_kge_99 [-0.39928454]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([21925 21949 21973], shape=(3,), dtype=int64) Times out: tf.Tensor(21973, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([28061 28085 28109], shape=(3,), dtype=int64) Times out: tf.Tensor(28109, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([37104 37128 37152], shape=(3,), dtype=int64) Times out: tf.Tensor(37152, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([8334 8358 8382], shape=(3,), dtype=int64) Times out: tf.Tensor(8382, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_341\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_342 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_682 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_683 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_341 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_682 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_683 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 34.0151 - mse: 33.9629 - mae: 4.4020 - val_loss: 21.8358 - val_mse: 21.7718 - val_mae: 3.6505\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 24.3773 - mse: 24.3082 - mae: 3.7888 - val_loss: 21.2332 - val_mse: 21.1591 - val_mae: 3.5941\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 23.6151 - mse: 23.5372 - mae: 3.7280 - val_loss: 21.0693 - val_mse: 20.9880 - val_mae: 3.5816\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 23.2752 - mse: 23.1912 - mae: 3.6985 - val_loss: 20.9324 - val_mse: 20.8457 - val_mae: 3.5713\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 23.0370 - mse: 22.9483 - mae: 3.6840 - val_loss: 20.5818 - val_mse: 20.4908 - val_mae: 3.5375\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.7466 - mse: 22.6535 - mae: 3.6624 - val_loss: 20.4403 - val_mse: 20.3447 - val_mae: 3.5234\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 22.5001 - mse: 22.4023 - mae: 3.6470 - val_loss: 20.1691 - val_mse: 20.0686 - val_mae: 3.5008\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.3993 - mse: 22.2962 - mae: 3.6407 - val_loss: 20.0651 - val_mse: 19.9586 - val_mae: 3.4956\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 22.0775 - mse: 21.9682 - mae: 3.6127 - val_loss: 19.9254 - val_mse: 19.8127 - val_mae: 3.4872\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.7905 - mse: 21.6752 - mae: 3.5914 - val_loss: 19.6888 - val_mse: 19.5702 - val_mae: 3.4593\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.5160 - mse: 21.3953 - mae: 3.5689 - val_loss: 19.5307 - val_mse: 19.4073 - val_mae: 3.4474\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.3801 - mse: 21.2551 - mae: 3.5584 - val_loss: 19.4344 - val_mse: 19.3067 - val_mae: 3.4420\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 21.3179 - mse: 21.1885 - mae: 3.5517 - val_loss: 19.1977 - val_mse: 19.0659 - val_mae: 3.4222\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.1203 - mse: 20.9872 - mae: 3.5337 - val_loss: 19.2494 - val_mse: 19.1142 - val_mae: 3.4207\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 21.0408 - mse: 20.9042 - mae: 3.5276 - val_loss: 19.1543 - val_mse: 19.0154 - val_mae: 3.4172\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.9279 - mse: 20.7882 - mae: 3.5170 - val_loss: 19.1847 - val_mse: 19.0433 - val_mae: 3.4240\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.8861 - mse: 20.7436 - mae: 3.5130 - val_loss: 18.9187 - val_mse: 18.7747 - val_mae: 3.3936\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.8848 - mse: 20.7396 - mae: 3.5142 - val_loss: 18.7865 - val_mse: 18.6396 - val_mae: 3.3814\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.7048 - mse: 20.5570 - mae: 3.5007 - val_loss: 18.9165 - val_mse: 18.7672 - val_mae: 3.4018\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.6142 - mse: 20.4640 - mae: 3.4914 - val_loss: 18.6357 - val_mse: 18.4842 - val_mae: 3.3693\n",
      "bias 8.0049555e-05\n",
      "si 0.49596778\n",
      "rmse 0.04299327\n",
      "kgeprime [0.81381054]\n",
      "rmse_95 0.06294636\n",
      "rmse_99 0.07554923\n",
      "pearson 0.8531862216894962\n",
      "pearson_95 0.547637830365553\n",
      "pearson_99 0.4763236933211074\n",
      "rscore 0.7270412256116352\n",
      "rscore_95 -2.5929144456350293\n",
      "rscore_99 -6.676718686686516\n",
      "nse [0.72704123]\n",
      "nse_95 [-2.59291445]\n",
      "nse_99 [-6.67671869]\n",
      "kge [0.81220001]\n",
      "ext_kge_95 [0.30185622]\n",
      "ext_kge_99 [0.01705334]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([179960 179984 180008], shape=(3,), dtype=int64) Times out: tf.Tensor(180008, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([180751 180775 180799], shape=(3,), dtype=int64) Times out: tf.Tensor(180799, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([183021 183045 183069], shape=(3,), dtype=int64) Times out: tf.Tensor(183069, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([50480 50504 50528], shape=(3,), dtype=int64) Times out: tf.Tensor(50528, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_342\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_343 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_684 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_685 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_342 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_684 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_685 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 38.3216 - mse: 38.2732 - mae: 4.7302 - val_loss: 23.2723 - val_mse: 23.2134 - val_mae: 3.7609\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 28.0101 - mse: 27.9453 - mae: 4.0735 - val_loss: 22.1781 - val_mse: 22.1064 - val_mae: 3.6877\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 26.4057 - mse: 26.3265 - mae: 3.9489 - val_loss: 21.1013 - val_mse: 21.0152 - val_mae: 3.5974\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 25.2381 - mse: 25.1460 - mae: 3.8540 - val_loss: 20.6535 - val_mse: 20.5562 - val_mae: 3.5546\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.7527 - mse: 24.6518 - mae: 3.8133 - val_loss: 19.7359 - val_mse: 19.6318 - val_mae: 3.4850\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.4204 - mse: 24.3140 - mae: 3.7849 - val_loss: 19.4845 - val_mse: 19.3758 - val_mae: 3.4633\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 24.1620 - mse: 24.0513 - mae: 3.7587 - val_loss: 19.3402 - val_mse: 19.2274 - val_mae: 3.4466\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.9485 - mse: 23.8338 - mae: 3.7450 - val_loss: 18.8828 - val_mse: 18.7661 - val_mae: 3.4153\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.7104 - mse: 23.5920 - mae: 3.7260 - val_loss: 18.6891 - val_mse: 18.5691 - val_mae: 3.3949\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.5397 - mse: 23.4181 - mae: 3.7122 - val_loss: 18.5206 - val_mse: 18.3974 - val_mae: 3.3785\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.2350 - mse: 23.1104 - mae: 3.6855 - val_loss: 18.3659 - val_mse: 18.2401 - val_mae: 3.3655\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0686 - mse: 22.9414 - mae: 3.6724 - val_loss: 18.3138 - val_mse: 18.1853 - val_mae: 3.3588\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 23.0434 - mse: 22.9136 - mae: 3.6701 - val_loss: 17.9593 - val_mse: 17.8279 - val_mae: 3.3336\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.9104 - mse: 22.7776 - mae: 3.6543 - val_loss: 18.1006 - val_mse: 17.9662 - val_mae: 3.3453\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.7197 - mse: 22.5839 - mae: 3.6417 - val_loss: 17.6065 - val_mse: 17.4693 - val_mae: 3.3016\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.5797 - mse: 22.4405 - mae: 3.6342 - val_loss: 17.5670 - val_mse: 17.4262 - val_mae: 3.3035\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4902 - mse: 22.3478 - mae: 3.6175 - val_loss: 17.5929 - val_mse: 17.4488 - val_mae: 3.3012\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.4239 - mse: 22.2783 - mae: 3.6165 - val_loss: 17.4803 - val_mse: 17.3331 - val_mae: 3.2949\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2409 - mse: 22.0922 - mae: 3.6044 - val_loss: 17.5117 - val_mse: 17.3616 - val_mae: 3.3018\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.2209 - mse: 22.0697 - mae: 3.6034 - val_loss: 17.2606 - val_mse: 17.1082 - val_mae: 3.2774\n",
      "bias 0.0013308497\n",
      "si 0.461202\n",
      "rmse 0.04136209\n",
      "kgeprime [0.81004983]\n",
      "rmse_95 0.06318931\n",
      "rmse_99 0.08208435\n",
      "pearson 0.874573736672551\n",
      "pearson_95 0.6650029334162094\n",
      "pearson_99 0.7051769071801495\n",
      "rscore 0.7621983167305608\n",
      "rscore_95 -0.5335826759113393\n",
      "rscore_99 -1.4979913223638635\n",
      "nse [0.76219832]\n",
      "nse_95 [-0.53358268]\n",
      "nse_99 [-1.49799132]\n",
      "kge [0.78003767]\n",
      "ext_kge_95 [0.60695827]\n",
      "ext_kge_99 [0.60864197]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 23, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -41.06 -40.75 -40.43 ... -34.5 -34.19\n",
      "  * longitude       (longitude) float32 172.8 173.1 173.4 ... 179.1 179.4 179.7\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -2.318 -1.799 ... 7.456\n",
      "    vgrd10m         (time, latitude, longitude) float32 -0.5683 ... -3.384\n",
      "    uw2             (time, latitude, longitude) float32 5.373 3.236 ... 55.59\n",
      "    vw2             (time, latitude, longitude) float32 0.323 0.1154 ... 11.45\n",
      "    wind_magnitude  (time, latitude, longitude) float32 2.387 1.831 ... 8.188\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([149173 149197 149221], shape=(3,), dtype=int64) Times out: tf.Tensor(149221, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([55312 55336 55360], shape=(3,), dtype=int64) Times out: tf.Tensor(55360, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([82612 82636 82660], shape=(3,), dtype=int64) Times out: tf.Tensor(82660, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([30592 30616 30640], shape=(3,), dtype=int64) Times out: tf.Tensor(30640, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_343\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_344 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_686 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_687 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_343 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_686 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_343 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_687 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.3643 - mse: 20.3128 - mae: 3.4870 - val_loss: 12.5418 - val_mse: 12.4744 - val_mae: 2.7571\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.7725 - mse: 15.6965 - mae: 3.1048 - val_loss: 12.1080 - val_mse: 12.0249 - val_mae: 2.7105\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.2839 - mse: 15.1950 - mae: 3.0573 - val_loss: 11.8747 - val_mse: 11.7806 - val_mae: 2.6670\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.0011 - mse: 14.9024 - mae: 3.0276 - val_loss: 11.7673 - val_mse: 11.6645 - val_mae: 2.6614\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.7926 - mse: 14.6861 - mae: 3.0050 - val_loss: 11.7233 - val_mse: 11.6131 - val_mae: 2.6598\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.6393 - mse: 14.5256 - mae: 2.9872 - val_loss: 11.7124 - val_mse: 11.5955 - val_mae: 2.6589\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.4876 - mse: 14.3678 - mae: 2.9727 - val_loss: 11.6834 - val_mse: 11.5608 - val_mae: 2.6570\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.3466 - mse: 14.2211 - mae: 2.9551 - val_loss: 11.5650 - val_mse: 11.4374 - val_mae: 2.6398\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.2757 - mse: 14.1458 - mae: 2.9474 - val_loss: 11.4807 - val_mse: 11.3487 - val_mae: 2.6329\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1415 - mse: 14.0073 - mae: 2.9327 - val_loss: 11.3998 - val_mse: 11.2640 - val_mae: 2.6192\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.0007 - mse: 13.8632 - mae: 2.9181 - val_loss: 11.2936 - val_mse: 11.1549 - val_mae: 2.6042\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.9744 - mse: 13.8343 - mae: 2.9162 - val_loss: 11.2192 - val_mse: 11.0781 - val_mae: 2.5965\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.8484 - mse: 13.7060 - mae: 2.9057 - val_loss: 11.1259 - val_mse: 10.9825 - val_mae: 2.5882\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7665 - mse: 13.6218 - mae: 2.8933 - val_loss: 11.1172 - val_mse: 10.9716 - val_mae: 2.5848\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6839 - mse: 13.5372 - mae: 2.8845 - val_loss: 11.2222 - val_mse: 11.0745 - val_mae: 2.6080\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5723 - mse: 13.4234 - mae: 2.8749 - val_loss: 10.9819 - val_mse: 10.8322 - val_mae: 2.5738\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5202 - mse: 13.3693 - mae: 2.8689 - val_loss: 10.9164 - val_mse: 10.7647 - val_mae: 2.5667\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4626 - mse: 13.3098 - mae: 2.8593 - val_loss: 10.8410 - val_mse: 10.6873 - val_mae: 2.5574\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.3646 - mse: 13.2099 - mae: 2.8485 - val_loss: 10.8451 - val_mse: 10.6897 - val_mae: 2.5576\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.2943 - mse: 13.1379 - mae: 2.8426 - val_loss: 10.8088 - val_mse: 10.6516 - val_mae: 2.5588\n",
      "bias -0.0023082315\n",
      "si 0.5286662\n",
      "rmse 0.03263679\n",
      "kgeprime [0.65700406]\n",
      "rmse_95 0.045443956\n",
      "rmse_99 0.04504461\n",
      "pearson 0.8258075846544233\n",
      "pearson_95 0.7368989397817953\n",
      "pearson_99 0.6432131467689488\n",
      "rscore 0.6797938169029721\n",
      "rscore_95 -1.5507736600081041\n",
      "rscore_99 -2.9592864962415995\n",
      "nse [0.67979382]\n",
      "nse_95 [-1.55077366]\n",
      "nse_99 [-2.9592865]\n",
      "kge [0.71547125]\n",
      "ext_kge_95 [0.28268118]\n",
      "ext_kge_99 [-0.34365251]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([79302 79326 79350], shape=(3,), dtype=int64) Times out: tf.Tensor(79350, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([77142 77166 77190], shape=(3,), dtype=int64) Times out: tf.Tensor(77190, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([60462 60486 60510], shape=(3,), dtype=int64) Times out: tf.Tensor(60510, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([113435 113459 113483], shape=(3,), dtype=int64) Times out: tf.Tensor(113483, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_344\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_345 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_688 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_689 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_344 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_688 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_344 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_689 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.4287 - mse: 19.3836 - mae: 3.4130 - val_loss: 15.5976 - val_mse: 15.5428 - val_mae: 3.0499\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 15.2113 - mse: 15.1496 - mae: 3.0498 - val_loss: 14.7723 - val_mse: 14.7055 - val_mae: 2.9732\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.5235 - mse: 14.4498 - mae: 2.9818 - val_loss: 14.5518 - val_mse: 14.4737 - val_mae: 2.9501\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.2122 - mse: 14.1280 - mae: 2.9452 - val_loss: 14.3874 - val_mse: 14.2998 - val_mae: 2.9316\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.9783 - mse: 13.8853 - mae: 2.9224 - val_loss: 14.4310 - val_mse: 14.3354 - val_mae: 2.9337\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.8454 - mse: 13.7450 - mae: 2.9094 - val_loss: 14.2585 - val_mse: 14.1560 - val_mae: 2.9193\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7214 - mse: 13.6148 - mae: 2.8936 - val_loss: 14.3672 - val_mse: 14.2590 - val_mae: 2.9298\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6376 - mse: 13.5255 - mae: 2.8808 - val_loss: 14.2545 - val_mse: 14.1413 - val_mae: 2.9167\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.5090 - mse: 13.3920 - mae: 2.8683 - val_loss: 14.2887 - val_mse: 14.1709 - val_mae: 2.9222\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4580 - mse: 13.3368 - mae: 2.8620 - val_loss: 14.4163 - val_mse: 14.2946 - val_mae: 2.9325\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4181 - mse: 13.2932 - mae: 2.8583 - val_loss: 14.3611 - val_mse: 14.2360 - val_mae: 2.9231\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.3294 - mse: 13.2008 - mae: 2.8485 - val_loss: 14.1812 - val_mse: 14.0521 - val_mae: 2.9087\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.2502 - mse: 13.1181 - mae: 2.8406 - val_loss: 14.2371 - val_mse: 14.1048 - val_mae: 2.9115\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.1984 - mse: 13.0632 - mae: 2.8311 - val_loss: 14.1487 - val_mse: 14.0135 - val_mae: 2.8992\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.1429 - mse: 13.0047 - mae: 2.8262 - val_loss: 14.0935 - val_mse: 13.9551 - val_mae: 2.8961\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.0142 - mse: 12.8731 - mae: 2.8122 - val_loss: 14.2225 - val_mse: 14.0814 - val_mae: 2.9094\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.9900 - mse: 12.8462 - mae: 2.8098 - val_loss: 14.0724 - val_mse: 13.9287 - val_mae: 2.8930\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.8936 - mse: 12.7474 - mae: 2.7993 - val_loss: 13.9502 - val_mse: 13.8042 - val_mae: 2.8846\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.8718 - mse: 12.7232 - mae: 2.7937 - val_loss: 13.8361 - val_mse: 13.6876 - val_mae: 2.8753\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.7939 - mse: 12.6431 - mae: 2.7881 - val_loss: 13.8843 - val_mse: 13.7337 - val_mae: 2.8753\n",
      "bias -0.0031522077\n",
      "si 0.5334334\n",
      "rmse 0.037059046\n",
      "kgeprime [0.60547458]\n",
      "rmse_95 0.05781773\n",
      "rmse_99 0.084254585\n",
      "pearson 0.8282103248552017\n",
      "pearson_95 0.6144815722299647\n",
      "pearson_99 0.5395544582638954\n",
      "rscore 0.6801642517448692\n",
      "rscore_95 -0.32505018267598396\n",
      "rscore_99 -0.287403137219274\n",
      "nse [0.68016425]\n",
      "nse_95 [-0.32505018]\n",
      "nse_99 [-0.28740314]\n",
      "kge [0.67962348]\n",
      "ext_kge_95 [0.56242128]\n",
      "ext_kge_99 [0.43058946]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([60238 60262 60286], shape=(3,), dtype=int64) Times out: tf.Tensor(60286, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([36322 36346 36370], shape=(3,), dtype=int64) Times out: tf.Tensor(36370, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([40909 40933 40957], shape=(3,), dtype=int64) Times out: tf.Tensor(40957, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([3572 3596 3620], shape=(3,), dtype=int64) Times out: tf.Tensor(3620, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_345\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_346 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_690 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_691 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_345 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_690 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_345 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_691 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.3387 - mse: 18.2900 - mae: 3.3038 - val_loss: 15.4908 - val_mse: 15.4328 - val_mae: 3.0420\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.8778 - mse: 14.8135 - mae: 3.0121 - val_loss: 14.6295 - val_mse: 14.5608 - val_mae: 2.9768\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.4384 - mse: 14.3645 - mae: 2.9645 - val_loss: 14.4612 - val_mse: 14.3839 - val_mae: 2.9465\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.1167 - mse: 14.0346 - mae: 2.9317 - val_loss: 14.0392 - val_mse: 13.9543 - val_mae: 2.9080\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.9300 - mse: 13.8406 - mae: 2.9129 - val_loss: 13.8856 - val_mse: 13.7941 - val_mae: 2.8853\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.7803 - mse: 13.6847 - mae: 2.8954 - val_loss: 13.7923 - val_mse: 13.6949 - val_mae: 2.8723\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.6492 - mse: 13.5481 - mae: 2.8813 - val_loss: 13.7552 - val_mse: 13.6527 - val_mae: 2.8715\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.5405 - mse: 13.4349 - mae: 2.8692 - val_loss: 13.6927 - val_mse: 13.5856 - val_mae: 2.8546\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.4231 - mse: 13.3129 - mae: 2.8574 - val_loss: 13.4367 - val_mse: 13.3253 - val_mae: 2.8284\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.3164 - mse: 13.2022 - mae: 2.8462 - val_loss: 13.3823 - val_mse: 13.2674 - val_mae: 2.8204\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.2198 - mse: 13.1025 - mae: 2.8348 - val_loss: 13.3261 - val_mse: 13.2078 - val_mae: 2.8127\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.1193 - mse: 12.9987 - mae: 2.8259 - val_loss: 13.3365 - val_mse: 13.2153 - val_mae: 2.8153\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.0179 - mse: 12.8943 - mae: 2.8156 - val_loss: 13.0845 - val_mse: 12.9605 - val_mae: 2.7858\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.9550 - mse: 12.8287 - mae: 2.8067 - val_loss: 13.0878 - val_mse: 12.9608 - val_mae: 2.7825\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.8826 - mse: 12.7533 - mae: 2.8005 - val_loss: 13.0478 - val_mse: 12.9180 - val_mae: 2.7756\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.8145 - mse: 12.6824 - mae: 2.7956 - val_loss: 13.0320 - val_mse: 12.8992 - val_mae: 2.7731\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.7477 - mse: 12.6127 - mae: 2.7837 - val_loss: 13.1520 - val_mse: 13.0166 - val_mae: 2.7820\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.7012 - mse: 12.5634 - mae: 2.7802 - val_loss: 12.7740 - val_mse: 12.6356 - val_mae: 2.7449\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.6425 - mse: 12.5019 - mae: 2.7715 - val_loss: 13.0796 - val_mse: 12.9385 - val_mae: 2.7730\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 12.5299 - mse: 12.3866 - mae: 2.7610 - val_loss: 13.1242 - val_mse: 12.9803 - val_mae: 2.7715\n",
      "bias 0.004228681\n",
      "si 0.53467506\n",
      "rmse 0.03602823\n",
      "kgeprime [0.7303583]\n",
      "rmse_95 0.07355796\n",
      "rmse_99 0.10830082\n",
      "pearson 0.8295850698970726\n",
      "pearson_95 0.1991918969145397\n",
      "pearson_99 -0.43961601674783907\n",
      "rscore 0.6770480863897442\n",
      "rscore_95 -5.265110258061786\n",
      "rscore_99 -50.20633281754098\n",
      "nse [0.67704809]\n",
      "nse_95 [-5.26511026]\n",
      "nse_99 [-50.20633282]\n",
      "kge [0.63708329]\n",
      "ext_kge_95 [0.03217183]\n",
      "ext_kge_99 [-2.38621326]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([2398 2422 2446], shape=(3,), dtype=int64) Times out: tf.Tensor(2446, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([13794 13818 13842], shape=(3,), dtype=int64) Times out: tf.Tensor(13842, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([11778 11802 11826], shape=(3,), dtype=int64) Times out: tf.Tensor(11826, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([11364 11388 11412], shape=(3,), dtype=int64) Times out: tf.Tensor(11412, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_346\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_347 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_692 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_693 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_346 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_692 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_346 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_693 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.9689 - mse: 20.9252 - mae: 3.5626 - val_loss: 19.0873 - val_mse: 19.0373 - val_mae: 3.4852\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.1676 - mse: 17.1155 - mae: 3.2390 - val_loss: 16.1192 - val_mse: 16.0655 - val_mae: 3.1906\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 15.3836 - mse: 15.3267 - mae: 3.0522 - val_loss: 15.4267 - val_mse: 15.3669 - val_mae: 3.1158\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.8921 - mse: 14.8295 - mae: 3.0003 - val_loss: 15.4011 - val_mse: 15.3365 - val_mae: 3.1183\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.6355 - mse: 14.5686 - mae: 2.9726 - val_loss: 15.1393 - val_mse: 15.0706 - val_mae: 3.0913\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.4384 - mse: 14.3676 - mae: 2.9517 - val_loss: 15.1212 - val_mse: 15.0491 - val_mae: 3.0888\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.3063 - mse: 14.2323 - mae: 2.9370 - val_loss: 14.8610 - val_mse: 14.7857 - val_mae: 3.0608\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.1466 - mse: 14.0696 - mae: 2.9195 - val_loss: 14.7812 - val_mse: 14.7028 - val_mae: 3.0550\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.0560 - mse: 13.9760 - mae: 2.9105 - val_loss: 14.6541 - val_mse: 14.5731 - val_mae: 3.0408\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.9383 - mse: 13.8556 - mae: 2.8957 - val_loss: 14.5415 - val_mse: 14.4578 - val_mae: 3.0294\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.8487 - mse: 13.7634 - mae: 2.8878 - val_loss: 14.4651 - val_mse: 14.3791 - val_mae: 3.0233\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7095 - mse: 13.6222 - mae: 2.8734 - val_loss: 14.5563 - val_mse: 14.4682 - val_mae: 3.0254\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7011 - mse: 13.6119 - mae: 2.8702 - val_loss: 14.3898 - val_mse: 14.3002 - val_mae: 3.0152\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.5902 - mse: 13.4993 - mae: 2.8619 - val_loss: 14.2393 - val_mse: 14.1479 - val_mae: 2.9971\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4985 - mse: 13.4060 - mae: 2.8496 - val_loss: 14.0811 - val_mse: 13.9881 - val_mae: 2.9828\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4323 - mse: 13.3382 - mae: 2.8433 - val_loss: 14.0437 - val_mse: 13.9491 - val_mae: 2.9796\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.3652 - mse: 13.2696 - mae: 2.8366 - val_loss: 13.9701 - val_mse: 13.8740 - val_mae: 2.9727\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.3431 - mse: 13.2459 - mae: 2.8332 - val_loss: 13.8748 - val_mse: 13.7770 - val_mae: 2.9629\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.2371 - mse: 13.1383 - mae: 2.8227 - val_loss: 13.7955 - val_mse: 13.6962 - val_mae: 2.9547\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.1876 - mse: 13.0872 - mae: 2.8155 - val_loss: 13.8868 - val_mse: 13.7860 - val_mae: 2.9582\n",
      "bias -0.0045760875\n",
      "si 0.5580593\n",
      "rmse 0.03712954\n",
      "kgeprime [0.5414346]\n",
      "rmse_95 0.05006631\n",
      "rmse_99 0.055180307\n",
      "pearson 0.8095649523044115\n",
      "pearson_95 0.6808621825507785\n",
      "pearson_99 0.6294597718163514\n",
      "rscore 0.6497255924521044\n",
      "rscore_95 -2.1866714871986903\n",
      "rscore_99 -3.0852422847649494\n",
      "nse [0.64972559]\n",
      "nse_95 [-2.18667149]\n",
      "nse_99 [-3.08524228]\n",
      "kge [0.64024256]\n",
      "ext_kge_95 [0.44780763]\n",
      "ext_kge_99 [0.47589815]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([43777 43801 43825], shape=(3,), dtype=int64) Times out: tf.Tensor(43825, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([61986 62010 62034], shape=(3,), dtype=int64) Times out: tf.Tensor(62034, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([162450 162474 162498], shape=(3,), dtype=int64) Times out: tf.Tensor(162498, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([51882 51906 51930], shape=(3,), dtype=int64) Times out: tf.Tensor(51930, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_347\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_348 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_694 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_695 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_347 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_694 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_347 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_695 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.0440 - mse: 19.9966 - mae: 3.4475 - val_loss: 14.4202 - val_mse: 14.3582 - val_mae: 2.9481\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.6791 - mse: 15.6121 - mae: 3.0823 - val_loss: 13.8973 - val_mse: 13.8251 - val_mae: 2.8975\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.2825 - mse: 15.2061 - mae: 3.0472 - val_loss: 13.6379 - val_mse: 13.5570 - val_mae: 2.8707\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.0880 - mse: 15.0038 - mae: 3.0277 - val_loss: 13.3057 - val_mse: 13.2173 - val_mae: 2.8321\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.8401 - mse: 14.7486 - mae: 3.0016 - val_loss: 13.0860 - val_mse: 12.9907 - val_mae: 2.8134\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.6522 - mse: 14.5543 - mae: 2.9838 - val_loss: 12.8325 - val_mse: 12.7313 - val_mae: 2.7826\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.4883 - mse: 14.3847 - mae: 2.9675 - val_loss: 12.6463 - val_mse: 12.5395 - val_mae: 2.7635\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.3408 - mse: 14.2321 - mae: 2.9496 - val_loss: 12.5567 - val_mse: 12.4455 - val_mae: 2.7557\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1766 - mse: 14.0636 - mae: 2.9330 - val_loss: 12.4163 - val_mse: 12.3013 - val_mae: 2.7389\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1271 - mse: 14.0108 - mae: 2.9277 - val_loss: 12.3265 - val_mse: 12.2085 - val_mae: 2.7274\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.0133 - mse: 13.8941 - mae: 2.9166 - val_loss: 12.2545 - val_mse: 12.1340 - val_mae: 2.7220\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.9781 - mse: 13.8566 - mae: 2.9121 - val_loss: 12.2459 - val_mse: 12.1232 - val_mae: 2.7210\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.8879 - mse: 13.7643 - mae: 2.9007 - val_loss: 12.2159 - val_mse: 12.0910 - val_mae: 2.7175\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.8024 - mse: 13.6767 - mae: 2.8936 - val_loss: 12.1495 - val_mse: 12.0228 - val_mae: 2.7115\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7862 - mse: 13.6586 - mae: 2.8884 - val_loss: 12.0023 - val_mse: 11.8736 - val_mae: 2.6943\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7042 - mse: 13.5747 - mae: 2.8833 - val_loss: 12.0480 - val_mse: 11.9175 - val_mae: 2.7007\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6320 - mse: 13.5008 - mae: 2.8736 - val_loss: 12.0198 - val_mse: 11.8877 - val_mae: 2.6978\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5062 - mse: 13.3733 - mae: 2.8626 - val_loss: 11.7936 - val_mse: 11.6597 - val_mae: 2.6737\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5296 - mse: 13.3949 - mae: 2.8617 - val_loss: 11.8169 - val_mse: 11.6813 - val_mae: 2.6748\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4880 - mse: 13.3516 - mae: 2.8582 - val_loss: 11.7816 - val_mse: 11.6443 - val_mae: 2.6689\n",
      "bias 0.000985998\n",
      "si 0.5265958\n",
      "rmse 0.034123704\n",
      "kgeprime [0.76479962]\n",
      "rmse_95 0.050442867\n",
      "rmse_99 0.062897734\n",
      "pearson 0.8286658491756407\n",
      "pearson_95 0.5183836264315228\n",
      "pearson_99 0.5341997382891925\n",
      "rscore 0.6859742391936908\n",
      "rscore_95 -1.7148861618088338\n",
      "rscore_99 -8.837353543384529\n",
      "nse [0.68597424]\n",
      "nse_95 [-1.71488616]\n",
      "nse_99 [-8.83735354]\n",
      "kge [0.73840876]\n",
      "ext_kge_95 [0.42884891]\n",
      "ext_kge_99 [0.14383788]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 23, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -40.75 -40.43 -40.12 ... -34.5 -34.19\n",
      "  * longitude       (longitude) float32 174.7 175.0 175.3 ... 180.9 181.2 181.6\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 0.9892 0.7285 ... 6.018\n",
      "    vgrd10m         (time, latitude, longitude) float32 -4.41 -3.709 ... -3.457\n",
      "    uw2             (time, latitude, longitude) float32 0.9784 0.5307 ... 36.22\n",
      "    vw2             (time, latitude, longitude) float32 19.44 13.76 ... 11.95\n",
      "    wind_magnitude  (time, latitude, longitude) float32 4.519 3.78 ... 6.94\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([68401 68425 68449], shape=(3,), dtype=int64) Times out: tf.Tensor(68449, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([76734 76758 76782], shape=(3,), dtype=int64) Times out: tf.Tensor(76782, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([145959 145983 146007], shape=(3,), dtype=int64) Times out: tf.Tensor(146007, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([5255 5279 5303], shape=(3,), dtype=int64) Times out: tf.Tensor(5303, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_348\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_349 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_696 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_697 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_348 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_696 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_348 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_697 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 21.2793 - mse: 21.2340 - mae: 3.5849 - val_loss: 12.9555 - val_mse: 12.8973 - val_mae: 2.8324\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 16.0859 - mse: 16.0226 - mae: 3.1444 - val_loss: 12.2547 - val_mse: 12.1871 - val_mae: 2.7541\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.5936 - mse: 15.5223 - mae: 3.0911 - val_loss: 12.1980 - val_mse: 12.1232 - val_mae: 2.7558\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.3286 - mse: 15.2506 - mae: 3.0676 - val_loss: 12.1456 - val_mse: 12.0646 - val_mae: 2.7538\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.0921 - mse: 15.0084 - mae: 3.0406 - val_loss: 11.7639 - val_mse: 11.6776 - val_mae: 2.7076\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.9573 - mse: 14.8691 - mae: 3.0281 - val_loss: 11.7486 - val_mse: 11.6584 - val_mae: 2.7083\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.8064 - mse: 14.7147 - mae: 3.0122 - val_loss: 11.7637 - val_mse: 11.6705 - val_mae: 2.7076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.7315 - mse: 14.6373 - mae: 3.0033 - val_loss: 11.8778 - val_mse: 11.7824 - val_mae: 2.7209\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.6690 - mse: 14.5727 - mae: 2.9991 - val_loss: 11.9250 - val_mse: 11.8275 - val_mae: 2.7291\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.5703 - mse: 14.4720 - mae: 2.9890 - val_loss: 11.6127 - val_mse: 11.5136 - val_mae: 2.6902\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.4604 - mse: 14.3605 - mae: 2.9790 - val_loss: 11.5555 - val_mse: 11.4549 - val_mae: 2.6832\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 14.4249 - mse: 14.3235 - mae: 2.9697 - val_loss: 11.5712 - val_mse: 11.4691 - val_mae: 2.6864\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.3401 - mse: 14.2375 - mae: 2.9628 - val_loss: 11.5183 - val_mse: 11.4150 - val_mae: 2.6786\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.3117 - mse: 14.2080 - mae: 2.9618 - val_loss: 11.4607 - val_mse: 11.3563 - val_mae: 2.6683\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.2652 - mse: 14.1604 - mae: 2.9533 - val_loss: 11.3530 - val_mse: 11.2477 - val_mae: 2.6585\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1898 - mse: 14.0838 - mae: 2.9475 - val_loss: 11.3306 - val_mse: 11.2241 - val_mae: 2.6570\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1762 - mse: 14.0691 - mae: 2.9447 - val_loss: 11.3134 - val_mse: 11.2056 - val_mae: 2.6524\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1443 - mse: 14.0359 - mae: 2.9394 - val_loss: 11.4548 - val_mse: 11.3457 - val_mae: 2.6689\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.0992 - mse: 13.9897 - mae: 2.9350 - val_loss: 11.1949 - val_mse: 11.0850 - val_mae: 2.6386\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.0130 - mse: 13.9025 - mae: 2.9279 - val_loss: 11.3949 - val_mse: 11.2838 - val_mae: 2.6593\n",
      "bias -0.0048254714\n",
      "si 0.5340942\n",
      "rmse 0.03359135\n",
      "kgeprime [0.53028831]\n",
      "rmse_95 0.046004474\n",
      "rmse_99 0.061177235\n",
      "pearson 0.8271310941329967\n",
      "pearson_95 0.6793976184851398\n",
      "pearson_99 0.38497842486230094\n",
      "rscore 0.6764798961216132\n",
      "rscore_95 -0.7794447073617026\n",
      "rscore_99 -2.1368917459349013\n",
      "nse [0.6764799]\n",
      "nse_95 [-0.77944471]\n",
      "nse_99 [-2.13689175]\n",
      "kge [0.63678526]\n",
      "ext_kge_95 [0.59175842]\n",
      "ext_kge_99 [0.34479242]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([115219 115243 115267], shape=(3,), dtype=int64) Times out: tf.Tensor(115267, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([82036 82060 82084], shape=(3,), dtype=int64) Times out: tf.Tensor(82084, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([16699 16723 16747], shape=(3,), dtype=int64) Times out: tf.Tensor(16747, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([88137 88161 88185], shape=(3,), dtype=int64) Times out: tf.Tensor(88185, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_349\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_350 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_698 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_699 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_349 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_698 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_349 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_699 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 20.3106 - mse: 20.2724 - mae: 3.5072 - val_loss: 15.7176 - val_mse: 15.6706 - val_mae: 3.0855\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 15.7153 - mse: 15.6628 - mae: 3.1162 - val_loss: 14.6091 - val_mse: 14.5513 - val_mae: 2.9734\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 15.1130 - mse: 15.0511 - mae: 3.0512 - val_loss: 14.4313 - val_mse: 14.3658 - val_mae: 2.9538\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.8819 - mse: 14.8135 - mae: 3.0291 - val_loss: 14.2516 - val_mse: 14.1804 - val_mae: 2.9323\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.6687 - mse: 14.5950 - mae: 3.0077 - val_loss: 14.4239 - val_mse: 14.3481 - val_mae: 2.9520\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.5677 - mse: 14.4895 - mae: 2.9994 - val_loss: 14.0318 - val_mse: 13.9517 - val_mae: 2.9074\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.4209 - mse: 14.3386 - mae: 2.9872 - val_loss: 14.0524 - val_mse: 13.9685 - val_mae: 2.9067\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.3177 - mse: 14.2317 - mae: 2.9745 - val_loss: 14.0214 - val_mse: 13.9337 - val_mae: 2.9056\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.1925 - mse: 14.1030 - mae: 2.9612 - val_loss: 13.9151 - val_mse: 13.8245 - val_mae: 2.8884\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.1646 - mse: 14.0721 - mae: 2.9600 - val_loss: 13.8444 - val_mse: 13.7508 - val_mae: 2.8809\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 14.0651 - mse: 13.9700 - mae: 2.9499 - val_loss: 13.8978 - val_mse: 13.8017 - val_mae: 2.8869\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 14.0185 - mse: 13.9208 - mae: 2.9414 - val_loss: 13.7358 - val_mse: 13.6372 - val_mae: 2.8692\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.9020 - mse: 13.8019 - mae: 2.9347 - val_loss: 13.7161 - val_mse: 13.6151 - val_mae: 2.8641\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.9020 - mse: 13.7996 - mae: 2.9300 - val_loss: 13.6429 - val_mse: 13.5399 - val_mae: 2.8566\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.8321 - mse: 13.7278 - mae: 2.9236 - val_loss: 13.6997 - val_mse: 13.5947 - val_mae: 2.8607\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7521 - mse: 13.6459 - mae: 2.9125 - val_loss: 13.5814 - val_mse: 13.4746 - val_mae: 2.8499\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7083 - mse: 13.6007 - mae: 2.9102 - val_loss: 13.5206 - val_mse: 13.4125 - val_mae: 2.8407\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6871 - mse: 13.5781 - mae: 2.9073 - val_loss: 13.4382 - val_mse: 13.3287 - val_mae: 2.8354\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6341 - mse: 13.5237 - mae: 2.9032 - val_loss: 13.4943 - val_mse: 13.3835 - val_mae: 2.8375\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6396 - mse: 13.5279 - mae: 2.9006 - val_loss: 13.4323 - val_mse: 13.3203 - val_mae: 2.8312\n",
      "bias -0.00010629996\n",
      "si 0.54142445\n",
      "rmse 0.03649703\n",
      "kgeprime [0.71550257]\n",
      "rmse_95 0.055891365\n",
      "rmse_99 0.07629856\n",
      "pearson 0.8242016565474537\n",
      "pearson_95 0.6221978177859921\n",
      "pearson_99 0.7285020538235071\n",
      "rscore 0.6773896423069738\n",
      "rscore_95 -1.1378147629448314\n",
      "rscore_99 -1.103478067254216\n",
      "nse [0.67738964]\n",
      "nse_95 [-1.13781476]\n",
      "nse_99 [-1.10347807]\n",
      "kge [0.71867555]\n",
      "ext_kge_95 [0.52239797]\n",
      "ext_kge_99 [0.58911587]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([1026 1050 1074], shape=(3,), dtype=int64) Times out: tf.Tensor(1074, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([24457 24481 24505], shape=(3,), dtype=int64) Times out: tf.Tensor(24505, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([1063 1087 1111], shape=(3,), dtype=int64) Times out: tf.Tensor(1111, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([26138 26162 26186], shape=(3,), dtype=int64) Times out: tf.Tensor(26186, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_350\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_351 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_700 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_701 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_350 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_700 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_350 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_701 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 18.8116 - mse: 18.7744 - mae: 3.3873 - val_loss: 15.5742 - val_mse: 15.5312 - val_mae: 3.0732\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 15.3901 - mse: 15.3440 - mae: 3.0840 - val_loss: 15.1589 - val_mse: 15.1102 - val_mae: 3.0291\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.9903 - mse: 14.9390 - mae: 3.0461 - val_loss: 14.8160 - val_mse: 14.7624 - val_mae: 2.9867\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.7719 - mse: 14.7162 - mae: 3.0240 - val_loss: 14.4255 - val_mse: 14.3680 - val_mae: 2.9421\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.6399 - mse: 14.5801 - mae: 3.0117 - val_loss: 14.4565 - val_mse: 14.3949 - val_mae: 2.9550\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.4442 - mse: 14.3807 - mae: 2.9920 - val_loss: 14.2409 - val_mse: 14.1760 - val_mae: 2.9299\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.3336 - mse: 14.2668 - mae: 2.9791 - val_loss: 14.1209 - val_mse: 14.0529 - val_mae: 2.9187\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.2984 - mse: 14.2287 - mae: 2.9790 - val_loss: 14.3200 - val_mse: 14.2491 - val_mae: 2.9406\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.1723 - mse: 14.1000 - mae: 2.9651 - val_loss: 14.0040 - val_mse: 13.9306 - val_mae: 2.9110\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.0924 - mse: 14.0177 - mae: 2.9552 - val_loss: 13.9778 - val_mse: 13.9021 - val_mae: 2.9084\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.9880 - mse: 13.9109 - mae: 2.9446 - val_loss: 13.8947 - val_mse: 13.8167 - val_mae: 2.8989\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.8980 - mse: 13.8187 - mae: 2.9336 - val_loss: 13.7236 - val_mse: 13.6436 - val_mae: 2.8817\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.8568 - mse: 13.7756 - mae: 2.9315 - val_loss: 13.6566 - val_mse: 13.5747 - val_mae: 2.8693\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.7887 - mse: 13.7057 - mae: 2.9195 - val_loss: 13.5895 - val_mse: 13.5058 - val_mae: 2.8632\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.8039 - mse: 13.7191 - mae: 2.9218 - val_loss: 13.4212 - val_mse: 13.3358 - val_mae: 2.8433\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.7725 - mse: 13.6861 - mae: 2.9204 - val_loss: 13.5700 - val_mse: 13.4831 - val_mae: 2.8611\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.6480 - mse: 13.5601 - mae: 2.9095 - val_loss: 13.6255 - val_mse: 13.5371 - val_mae: 2.8663\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.5993 - mse: 13.5098 - mae: 2.8992 - val_loss: 13.2747 - val_mse: 13.1845 - val_mae: 2.8250\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.5533 - mse: 13.4621 - mae: 2.8973 - val_loss: 13.3763 - val_mse: 13.2846 - val_mae: 2.8378\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.5364 - mse: 13.4436 - mae: 2.8951 - val_loss: 13.5170 - val_mse: 13.4234 - val_mae: 2.8565\n",
      "bias -0.0017155991\n",
      "si 0.5460616\n",
      "rmse 0.036637995\n",
      "kgeprime [0.61648219]\n",
      "rmse_95 0.07071392\n",
      "rmse_99 0.10627049\n",
      "pearson 0.8267975285174902\n",
      "pearson_95 0.3483282289489176\n",
      "pearson_99 -0.5004039243383093\n",
      "rscore 0.6730208696491682\n",
      "rscore_95 -4.843074399267477\n",
      "rscore_99 -46.08025085571004\n",
      "nse [0.67302087]\n",
      "nse_95 [-4.8430744]\n",
      "nse_99 [-46.08025086]\n",
      "kge [0.6655469]\n",
      "ext_kge_95 [0.12719527]\n",
      "ext_kge_99 [-2.39291124]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([2866 2890 2914], shape=(3,), dtype=int64) Times out: tf.Tensor(2914, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([18201 18225 18249], shape=(3,), dtype=int64) Times out: tf.Tensor(18249, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([31630 31654 31678], shape=(3,), dtype=int64) Times out: tf.Tensor(31678, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([-31  -7  17], shape=(3,), dtype=int64) Times out: tf.Tensor(17, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_351\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_352 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_702 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_703 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_351 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_702 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_351 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_703 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.2647 - mse: 19.2150 - mae: 3.3915 - val_loss: 15.9050 - val_mse: 15.8453 - val_mae: 3.1767\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 7s 2ms/step - loss: 15.3027 - mse: 15.2387 - mae: 3.0455 - val_loss: 15.3066 - val_mse: 15.2392 - val_mae: 3.1149\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.8109 - mse: 14.7399 - mae: 2.9982 - val_loss: 14.9503 - val_mse: 14.8761 - val_mae: 3.0836\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.5695 - mse: 14.4920 - mae: 2.9734 - val_loss: 14.6391 - val_mse: 14.5592 - val_mae: 3.0528\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.3681 - mse: 14.2851 - mae: 2.9532 - val_loss: 14.5211 - val_mse: 14.4362 - val_mae: 3.0409\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.2525 - mse: 14.1651 - mae: 2.9409 - val_loss: 14.2999 - val_mse: 14.2109 - val_mae: 3.0182\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.1500 - mse: 14.0588 - mae: 2.9338 - val_loss: 14.2571 - val_mse: 14.1647 - val_mae: 3.0115\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.0042 - mse: 13.9096 - mae: 2.9193 - val_loss: 14.1423 - val_mse: 14.0465 - val_mae: 3.0013\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.8888 - mse: 13.7912 - mae: 2.9102 - val_loss: 14.0503 - val_mse: 13.9520 - val_mae: 2.9904\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.8382 - mse: 13.7380 - mae: 2.9028 - val_loss: 14.0457 - val_mse: 13.9449 - val_mae: 2.9883\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7280 - mse: 13.6255 - mae: 2.8921 - val_loss: 13.9435 - val_mse: 13.8403 - val_mae: 2.9771\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7013 - mse: 13.5966 - mae: 2.8880 - val_loss: 13.9702 - val_mse: 13.8648 - val_mae: 2.9807\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6301 - mse: 13.5233 - mae: 2.8810 - val_loss: 13.8577 - val_mse: 13.7505 - val_mae: 2.9689\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6065 - mse: 13.4978 - mae: 2.8801 - val_loss: 13.8682 - val_mse: 13.7587 - val_mae: 2.9678\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.5299 - mse: 13.4194 - mae: 2.8720 - val_loss: 13.7753 - val_mse: 13.6641 - val_mae: 2.9580\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4510 - mse: 13.3387 - mae: 2.8604 - val_loss: 13.8087 - val_mse: 13.6960 - val_mae: 2.9611\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4052 - mse: 13.2913 - mae: 2.8576 - val_loss: 13.7178 - val_mse: 13.6034 - val_mae: 2.9513\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.3361 - mse: 13.2206 - mae: 2.8496 - val_loss: 13.7046 - val_mse: 13.5887 - val_mae: 2.9481\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.2698 - mse: 13.1529 - mae: 2.8418 - val_loss: 13.6884 - val_mse: 13.5711 - val_mae: 2.9481\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.3125 - mse: 13.1941 - mae: 2.8437 - val_loss: 13.7791 - val_mse: 13.6602 - val_mae: 2.9571\n",
      "bias 0.0008488307\n",
      "si 0.57432455\n",
      "rmse 0.03695974\n",
      "kgeprime [0.76645789]\n",
      "rmse_95 0.053167537\n",
      "rmse_99 0.0625833\n",
      "pearson 0.8005181970678517\n",
      "pearson_95 0.3669250487387974\n",
      "pearson_99 0.26124079969267155\n",
      "rscore 0.6384367502013641\n",
      "rscore_95 -4.99155642042826\n",
      "rscore_99 -33.12825766961977\n",
      "nse [0.63843675]\n",
      "nse_95 [-4.99155642]\n",
      "nse_99 [-33.12825767]\n",
      "kge [0.74492724]\n",
      "ext_kge_95 [0.19699759]\n",
      "ext_kge_99 [-0.13751868]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([185723 185747 185771], shape=(3,), dtype=int64) Times out: tf.Tensor(185771, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([155676 155700 155724], shape=(3,), dtype=int64) Times out: tf.Tensor(155724, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([138294 138318 138342], shape=(3,), dtype=int64) Times out: tf.Tensor(138342, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([72340 72364 72388], shape=(3,), dtype=int64) Times out: tf.Tensor(72388, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_352\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_353 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_704 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_705 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_352 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_704 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_352 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_705 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.6106 - mse: 18.5640 - mae: 3.3678 - val_loss: 14.8006 - val_mse: 14.7461 - val_mae: 3.0089\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.4034 - mse: 15.3433 - mae: 3.0834 - val_loss: 14.0551 - val_mse: 13.9894 - val_mae: 2.9298\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.8747 - mse: 14.8037 - mae: 3.0260 - val_loss: 13.8308 - val_mse: 13.7551 - val_mae: 2.9122\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.5838 - mse: 14.5038 - mae: 2.9946 - val_loss: 13.6047 - val_mse: 13.5209 - val_mae: 2.8896\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.4506 - mse: 14.3635 - mae: 2.9794 - val_loss: 13.4018 - val_mse: 13.3117 - val_mae: 2.8683\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.3117 - mse: 14.2191 - mae: 2.9687 - val_loss: 13.2632 - val_mse: 13.1684 - val_mae: 2.8557\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.2084 - mse: 14.1116 - mae: 2.9593 - val_loss: 13.2135 - val_mse: 13.1145 - val_mae: 2.8543\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1186 - mse: 14.0180 - mae: 2.9472 - val_loss: 13.1728 - val_mse: 13.0704 - val_mae: 2.8506\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.0207 - mse: 13.9165 - mae: 2.9395 - val_loss: 13.0185 - val_mse: 12.9127 - val_mae: 2.8336\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.9463 - mse: 13.8389 - mae: 2.9314 - val_loss: 13.0705 - val_mse: 12.9616 - val_mae: 2.8410\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.9138 - mse: 13.8033 - mae: 2.9261 - val_loss: 12.9746 - val_mse: 12.8629 - val_mae: 2.8311\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7911 - mse: 13.6778 - mae: 2.9145 - val_loss: 12.8690 - val_mse: 12.7544 - val_mae: 2.8187\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7157 - mse: 13.5996 - mae: 2.9022 - val_loss: 12.9502 - val_mse: 12.8327 - val_mae: 2.8290\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7180 - mse: 13.5992 - mae: 2.9034 - val_loss: 12.6692 - val_mse: 12.5491 - val_mae: 2.7931\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6454 - mse: 13.5242 - mae: 2.8969 - val_loss: 12.6716 - val_mse: 12.5495 - val_mae: 2.7923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6046 - mse: 13.4815 - mae: 2.8911 - val_loss: 12.7868 - val_mse: 12.6627 - val_mae: 2.8077\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5480 - mse: 13.4229 - mae: 2.8851 - val_loss: 12.5051 - val_mse: 12.3790 - val_mae: 2.7743\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4852 - mse: 13.3579 - mae: 2.8796 - val_loss: 12.5209 - val_mse: 12.3926 - val_mae: 2.7723\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4720 - mse: 13.3428 - mae: 2.8763 - val_loss: 12.5183 - val_mse: 12.3885 - val_mae: 2.7770\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4377 - mse: 13.3068 - mae: 2.8734 - val_loss: 12.4447 - val_mse: 12.3130 - val_mae: 2.7644\n",
      "bias 0.00027080026\n",
      "si 0.5476852\n",
      "rmse 0.035089906\n",
      "kgeprime [0.74078965]\n",
      "rmse_95 0.053723402\n",
      "rmse_99 0.07323013\n",
      "pearson 0.816249443171345\n",
      "pearson_95 0.5164512135874668\n",
      "pearson_99 0.8126380496104927\n",
      "rscore 0.666154061783298\n",
      "rscore_95 -1.5110697385037026\n",
      "rscore_99 -3.0212293466173072\n",
      "nse [0.66615406]\n",
      "nse_95 [-1.51106974]\n",
      "nse_99 [-3.02122935]\n",
      "kge [0.73304521]\n",
      "ext_kge_95 [0.45073291]\n",
      "ext_kge_99 [0.61459078]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -40.43 -40.12 -39.81 ... -34.19 -33.88\n",
      "  * longitude       (longitude) float32 172.2 172.5 172.8 ... 178.1 178.4 178.8\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -1.109 -1.25 ... 7.283\n",
      "    vgrd10m         (time, latitude, longitude) float32 -1.97 -1.899 ... -2.948\n",
      "    uw2             (time, latitude, longitude) float32 1.229 1.562 ... 53.04\n",
      "    vw2             (time, latitude, longitude) float32 3.88 3.607 ... 8.692\n",
      "    wind_magnitude  (time, latitude, longitude) float32 2.26 2.274 ... 7.857\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n",
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([20610 20634 20658], shape=(3,), dtype=int64) Times out: tf.Tensor(20658, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([148665 148689 148713], shape=(3,), dtype=int64) Times out: tf.Tensor(148713, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([13558 13582 13606], shape=(3,), dtype=int64) Times out: tf.Tensor(13606, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([52514 52538 52562], shape=(3,), dtype=int64) Times out: tf.Tensor(52562, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_353\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_354 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_706 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_707 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_353 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_706 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_353 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_707 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.7159 - mse: 18.6723 - mae: 3.3300 - val_loss: 11.5539 - val_mse: 11.5015 - val_mae: 2.6477\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.9085 - mse: 14.8506 - mae: 3.0033 - val_loss: 11.2232 - val_mse: 11.1605 - val_mae: 2.6054\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.5390 - mse: 14.4719 - mae: 2.9663 - val_loss: 11.0595 - val_mse: 10.9883 - val_mae: 2.5869\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.2715 - mse: 14.1965 - mae: 2.9376 - val_loss: 11.0075 - val_mse: 10.9287 - val_mae: 2.5797\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.1481 - mse: 14.0661 - mae: 2.9240 - val_loss: 10.9758 - val_mse: 10.8907 - val_mae: 2.5822\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.9457 - mse: 13.8577 - mae: 2.9028 - val_loss: 10.8136 - val_mse: 10.7230 - val_mae: 2.5651\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.8588 - mse: 13.7657 - mae: 2.8904 - val_loss: 10.7466 - val_mse: 10.6513 - val_mae: 2.5548\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6960 - mse: 13.5984 - mae: 2.8749 - val_loss: 10.7683 - val_mse: 10.6690 - val_mae: 2.5547\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6187 - mse: 13.5176 - mae: 2.8665 - val_loss: 10.6462 - val_mse: 10.5436 - val_mae: 2.5406\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5776 - mse: 13.4734 - mae: 2.8612 - val_loss: 10.5757 - val_mse: 10.4702 - val_mae: 2.5299\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4537 - mse: 13.3467 - mae: 2.8476 - val_loss: 10.5497 - val_mse: 10.4414 - val_mae: 2.5260\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.4364 - mse: 13.3270 - mae: 2.8439 - val_loss: 10.6415 - val_mse: 10.5310 - val_mae: 2.5362\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.3647 - mse: 13.2532 - mae: 2.8364 - val_loss: 10.4840 - val_mse: 10.3717 - val_mae: 2.5167\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.2557 - mse: 13.1423 - mae: 2.8282 - val_loss: 10.5166 - val_mse: 10.4024 - val_mae: 2.5244\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.2221 - mse: 13.1069 - mae: 2.8234 - val_loss: 10.4882 - val_mse: 10.3723 - val_mae: 2.5173\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.1951 - mse: 13.0783 - mae: 2.8179 - val_loss: 10.4403 - val_mse: 10.3227 - val_mae: 2.5149\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.1275 - mse: 13.0093 - mae: 2.8150 - val_loss: 10.4416 - val_mse: 10.3227 - val_mae: 2.5157\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.1074 - mse: 12.9877 - mae: 2.8116 - val_loss: 10.3946 - val_mse: 10.2742 - val_mae: 2.5081\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.0463 - mse: 12.9251 - mae: 2.8016 - val_loss: 10.4128 - val_mse: 10.2912 - val_mae: 2.5127\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 12.9808 - mse: 12.8585 - mae: 2.7962 - val_loss: 10.5187 - val_mse: 10.3958 - val_mae: 2.5251\n",
      "bias 0.0028558597\n",
      "si 0.5237085\n",
      "rmse 0.032242496\n",
      "kgeprime [0.76485788]\n",
      "rmse_95 0.05089842\n",
      "rmse_99 0.058986645\n",
      "pearson 0.8317336068895583\n",
      "pearson_95 0.702391546967058\n",
      "pearson_99 0.7513039191836713\n",
      "rscore 0.6871032639797432\n",
      "rscore_95 -1.5007126924474945\n",
      "rscore_99 -5.847659380965035\n",
      "nse [0.68710326]\n",
      "nse_95 [-1.50071269]\n",
      "nse_99 [-5.84765938]\n",
      "kge [0.69450097]\n",
      "ext_kge_95 [0.56378396]\n",
      "ext_kge_99 [0.31552617]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([20213 20237 20261], shape=(3,), dtype=int64) Times out: tf.Tensor(20261, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([110342 110366 110390], shape=(3,), dtype=int64) Times out: tf.Tensor(110390, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([39641 39665 39689], shape=(3,), dtype=int64) Times out: tf.Tensor(39689, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([72033 72057 72081], shape=(3,), dtype=int64) Times out: tf.Tensor(72081, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_354\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_355 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_708 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_709 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_354 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_708 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_354 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_709 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 17.4737 - mse: 17.4357 - mae: 3.2255 - val_loss: 14.4018 - val_mse: 14.3562 - val_mae: 2.9228\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 14.0829 - mse: 14.0330 - mae: 2.9249 - val_loss: 13.9838 - val_mse: 13.9308 - val_mae: 2.8854\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.7075 - mse: 13.6507 - mae: 2.8826 - val_loss: 13.5406 - val_mse: 13.4813 - val_mae: 2.8464\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.5082 - mse: 13.4454 - mae: 2.8621 - val_loss: 13.3366 - val_mse: 13.2717 - val_mae: 2.8282\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.3090 - mse: 13.2411 - mae: 2.8402 - val_loss: 13.2148 - val_mse: 13.1451 - val_mae: 2.8174\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.1757 - mse: 13.1033 - mae: 2.8246 - val_loss: 13.1850 - val_mse: 13.1112 - val_mae: 2.8138\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.0398 - mse: 12.9636 - mae: 2.8123 - val_loss: 13.0372 - val_mse: 12.9598 - val_mae: 2.7984\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.9498 - mse: 12.8703 - mae: 2.8037 - val_loss: 13.1352 - val_mse: 13.0546 - val_mae: 2.8103\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 12.8612 - mse: 12.7786 - mae: 2.7921 - val_loss: 12.8480 - val_mse: 12.7646 - val_mae: 2.7810\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.8081 - mse: 12.7228 - mae: 2.7843 - val_loss: 12.8687 - val_mse: 12.7826 - val_mae: 2.7848\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.6938 - mse: 12.6060 - mae: 2.7718 - val_loss: 12.7835 - val_mse: 12.6949 - val_mae: 2.7773\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.6297 - mse: 12.5395 - mae: 2.7657 - val_loss: 12.7820 - val_mse: 12.6911 - val_mae: 2.7823\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.5891 - mse: 12.4967 - mae: 2.7603 - val_loss: 12.7479 - val_mse: 12.6547 - val_mae: 2.7772\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.5216 - mse: 12.4270 - mae: 2.7541 - val_loss: 12.7041 - val_mse: 12.6089 - val_mae: 2.7730\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.4332 - mse: 12.3366 - mae: 2.7461 - val_loss: 12.7603 - val_mse: 12.6632 - val_mae: 2.7847\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.3957 - mse: 12.2972 - mae: 2.7400 - val_loss: 12.6255 - val_mse: 12.5263 - val_mae: 2.7703\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.3035 - mse: 12.2029 - mae: 2.7312 - val_loss: 12.6321 - val_mse: 12.5308 - val_mae: 2.7728\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.2650 - mse: 12.1624 - mae: 2.7261 - val_loss: 12.5715 - val_mse: 12.4684 - val_mae: 2.7669\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 12.1895 - mse: 12.0850 - mae: 2.7188 - val_loss: 12.7132 - val_mse: 12.6082 - val_mae: 2.7865\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 12.1783 - mse: 12.0719 - mae: 2.7156 - val_loss: 12.5830 - val_mse: 12.4762 - val_mae: 2.7692\n",
      "bias 0.0013264944\n",
      "si 0.52037686\n",
      "rmse 0.035321724\n",
      "kgeprime [0.76334746]\n",
      "rmse_95 0.0564973\n",
      "rmse_99 0.071372695\n",
      "pearson 0.8376087692777018\n",
      "pearson_95 0.6211023102088967\n",
      "pearson_99 0.7577929336474256\n",
      "rscore 0.6986486492299179\n",
      "rscore_95 -0.7077236318679223\n",
      "rscore_99 -1.0640179446313032\n",
      "nse [0.69864865]\n",
      "nse_95 [-0.70772363]\n",
      "nse_99 [-1.06401794]\n",
      "kge [0.72547694]\n",
      "ext_kge_95 [0.53524853]\n",
      "ext_kge_99 [0.6389753]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([62886 62910 62934], shape=(3,), dtype=int64) Times out: tf.Tensor(62934, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([36250 36274 36298], shape=(3,), dtype=int64) Times out: tf.Tensor(36298, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([29989 30013 30037], shape=(3,), dtype=int64) Times out: tf.Tensor(30037, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([73050 73074 73098], shape=(3,), dtype=int64) Times out: tf.Tensor(73098, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_355\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_356 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_710 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_711 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_355 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_710 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_355 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_711 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 19.2194 - mse: 19.1738 - mae: 3.3809 - val_loss: 14.7839 - val_mse: 14.7271 - val_mae: 2.9854\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.8953 - mse: 14.8350 - mae: 3.0034 - val_loss: 14.1194 - val_mse: 14.0557 - val_mae: 2.9105\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.6065 - mse: 14.5412 - mae: 2.9711 - val_loss: 13.9427 - val_mse: 13.8758 - val_mae: 2.8924\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.4323 - mse: 14.3640 - mae: 2.9535 - val_loss: 13.7436 - val_mse: 13.6740 - val_mae: 2.8780\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.3214 - mse: 14.2505 - mae: 2.9407 - val_loss: 13.6311 - val_mse: 13.5589 - val_mae: 2.8612\n",
      "Epoch 6/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.2301 - mse: 14.1570 - mae: 2.9311 - val_loss: 13.7464 - val_mse: 13.6725 - val_mae: 2.8595\n",
      "Epoch 7/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 14.0839 - mse: 14.0089 - mae: 2.9180 - val_loss: 13.5638 - val_mse: 13.4879 - val_mae: 2.8353\n",
      "Epoch 8/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.9911 - mse: 13.9144 - mae: 2.9053 - val_loss: 13.5927 - val_mse: 13.5152 - val_mae: 2.8418\n",
      "Epoch 9/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.9803 - mse: 13.9018 - mae: 2.9026 - val_loss: 13.4223 - val_mse: 13.3430 - val_mae: 2.8441\n",
      "Epoch 10/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 13.8706 - mse: 13.7903 - mae: 2.8943 - val_loss: 13.3376 - val_mse: 13.2566 - val_mae: 2.8331\n",
      "Epoch 11/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.8405 - mse: 13.7585 - mae: 2.8896 - val_loss: 13.5507 - val_mse: 13.4678 - val_mae: 2.8168\n",
      "Epoch 12/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.7722 - mse: 13.6882 - mae: 2.8831 - val_loss: 13.2382 - val_mse: 13.1536 - val_mae: 2.8006\n",
      "Epoch 13/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.6751 - mse: 13.5892 - mae: 2.8744 - val_loss: 13.2591 - val_mse: 13.1725 - val_mae: 2.8018\n",
      "Epoch 14/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.6574 - mse: 13.5698 - mae: 2.8704 - val_loss: 13.3129 - val_mse: 13.2249 - val_mae: 2.8014\n",
      "Epoch 15/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.6050 - mse: 13.5159 - mae: 2.8658 - val_loss: 12.9801 - val_mse: 12.8905 - val_mae: 2.7670\n",
      "Epoch 16/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.4983 - mse: 13.4077 - mae: 2.8538 - val_loss: 13.0238 - val_mse: 12.9326 - val_mae: 2.7898\n",
      "Epoch 17/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.4838 - mse: 13.3917 - mae: 2.8531 - val_loss: 13.0336 - val_mse: 12.9410 - val_mae: 2.7679\n",
      "Epoch 18/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.4050 - mse: 13.3116 - mae: 2.8440 - val_loss: 12.8909 - val_mse: 12.7970 - val_mae: 2.7583\n",
      "Epoch 19/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.3651 - mse: 13.2703 - mae: 2.8375 - val_loss: 12.8858 - val_mse: 12.7905 - val_mae: 2.7507\n",
      "Epoch 20/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 13.2925 - mse: 13.1965 - mae: 2.8279 - val_loss: 12.7240 - val_mse: 12.6279 - val_mae: 2.7579\n",
      "bias -0.0029399565\n",
      "si 0.5454118\n",
      "rmse 0.0355357\n",
      "kgeprime [0.60716053]\n",
      "rmse_95 0.06489226\n",
      "rmse_99 0.109665856\n",
      "pearson 0.8189405695413144\n",
      "pearson_95 0.19137923162544598\n",
      "pearson_99 -0.5851625431778106\n",
      "rscore 0.6667748321501686\n",
      "rscore_95 -3.945424165716118\n",
      "rscore_99 -35.52195206827743\n",
      "nse [0.66677483]\n",
      "nse_95 [-3.94542417]\n",
      "nse_99 [-35.52195207]\n",
      "kge [0.68068009]\n",
      "ext_kge_95 [0.02911203]\n",
      "ext_kge_99 [-2.1072753]\n",
      "Returning fold  3  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 1999-05-08 14:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([16432 16456 16480], shape=(3,), dtype=int64) Times out: tf.Tensor(16480, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([33723 33747 33771], shape=(3,), dtype=int64) Times out: tf.Tensor(33771, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([10000 10024 10048], shape=(3,), dtype=int64) Times out: tf.Tensor(10048, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([9702 9726 9750], shape=(3,), dtype=int64) Times out: tf.Tensor(9750, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2003-10-14 05:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_356\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_357 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_712 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_713 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_356 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_712 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_356 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_713 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 19.4111 - mse: 19.3809 - mae: 3.3959 - val_loss: 16.1008 - val_mse: 16.0653 - val_mae: 3.1867\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 15.3025 - mse: 15.2633 - mae: 3.0318 - val_loss: 15.1811 - val_mse: 15.1389 - val_mae: 3.0884\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 14.6411 - mse: 14.5962 - mae: 2.9606 - val_loss: 15.3761 - val_mse: 15.3287 - val_mae: 3.1081\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 14.3934 - mse: 14.3437 - mae: 2.9333 - val_loss: 14.8857 - val_mse: 14.8340 - val_mae: 3.0582\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 14.1271 - mse: 14.0734 - mae: 2.9063 - val_loss: 14.5977 - val_mse: 14.5425 - val_mae: 3.0239\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 14.0189 - mse: 13.9623 - mae: 2.8935 - val_loss: 14.2548 - val_mse: 14.1971 - val_mae: 2.9887\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.9021 - mse: 13.8431 - mae: 2.8816 - val_loss: 14.4369 - val_mse: 14.3768 - val_mae: 3.0047\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.8510 - mse: 13.7899 - mae: 2.8744 - val_loss: 14.2534 - val_mse: 14.1913 - val_mae: 2.9847\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.7885 - mse: 13.7255 - mae: 2.8689 - val_loss: 13.9861 - val_mse: 13.9226 - val_mae: 2.9578\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.7284 - mse: 13.6637 - mae: 2.8641 - val_loss: 13.9493 - val_mse: 13.8837 - val_mae: 2.9490\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.6448 - mse: 13.5780 - mae: 2.8552 - val_loss: 14.1099 - val_mse: 14.0423 - val_mae: 2.9668\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.6118 - mse: 13.5434 - mae: 2.8500 - val_loss: 13.6859 - val_mse: 13.6169 - val_mae: 2.9187\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.5233 - mse: 13.4533 - mae: 2.8423 - val_loss: 14.2843 - val_mse: 14.2138 - val_mae: 2.9827\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.4833 - mse: 13.4119 - mae: 2.8411 - val_loss: 13.9954 - val_mse: 13.9235 - val_mae: 2.9554\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.4414 - mse: 13.3687 - mae: 2.8359 - val_loss: 13.6427 - val_mse: 13.5695 - val_mae: 2.9170\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.3587 - mse: 13.2847 - mae: 2.8242 - val_loss: 13.7109 - val_mse: 13.6365 - val_mae: 2.9226\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.3279 - mse: 13.2526 - mae: 2.8225 - val_loss: 13.9162 - val_mse: 13.8405 - val_mae: 2.9449\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 13.2357 - mse: 13.1593 - mae: 2.8130 - val_loss: 13.6700 - val_mse: 13.5931 - val_mae: 2.9171\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.2879 - mse: 13.2102 - mae: 2.8184 - val_loss: 13.6206 - val_mse: 13.5425 - val_mae: 2.9172\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 13.2166 - mse: 13.1378 - mae: 2.8111 - val_loss: 14.1343 - val_mse: 14.0551 - val_mae: 2.9699\n",
      "bias -0.009228307\n",
      "si 0.5697883\n",
      "rmse 0.037490144\n",
      "kgeprime [0.32959812]\n",
      "rmse_95 0.04685415\n",
      "rmse_99 0.05718353\n",
      "pearson 0.8007153369478832\n",
      "pearson_95 0.5814230126488326\n",
      "pearson_99 0.6407058894194055\n",
      "rscore 0.6174973987061352\n",
      "rscore_95 -1.4852782252861574\n",
      "rscore_99 -2.5125940377099276\n",
      "nse [0.6174974]\n",
      "nse_95 [-1.48527823]\n",
      "nse_99 [-2.51259404]\n",
      "kge [0.46307295]\n",
      "ext_kge_95 [0.46822958]\n",
      "ext_kge_99 [0.51308293]\n",
      "Returning fold  4  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1999-05-08 14:00:00 2017-02-01 01:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([40945 40969 40993], shape=(3,), dtype=int64) Times out: tf.Tensor(40993, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([105685 105709 105733], shape=(3,), dtype=int64) Times out: tf.Tensor(105733, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([116245 116269 116293], shape=(3,), dtype=int64) Times out: tf.Tensor(116293, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([123729 123753 123777], shape=(3,), dtype=int64) Times out: tf.Tensor(123777, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_357\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_358 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_714 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_715 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_357 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_714 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_357 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_715 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 20.4674 - mse: 20.4212 - mae: 3.4650 - val_loss: 13.5711 - val_mse: 13.5136 - val_mae: 2.8689\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 15.1728 - mse: 15.1118 - mae: 3.0222 - val_loss: 12.9221 - val_mse: 12.8585 - val_mae: 2.7929\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.6914 - mse: 14.6251 - mae: 2.9723 - val_loss: 12.7767 - val_mse: 12.7086 - val_mae: 2.7771\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 14.4805 - mse: 14.4103 - mae: 2.9529 - val_loss: 12.4742 - val_mse: 12.4024 - val_mae: 2.7391\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.2910 - mse: 14.2174 - mae: 2.9315 - val_loss: 12.2826 - val_mse: 12.2074 - val_mae: 2.7208\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 14.2796 - mse: 14.2027 - mae: 2.9314 - val_loss: 12.2712 - val_mse: 12.1929 - val_mae: 2.7219\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 14.1596 - mse: 14.0799 - mae: 2.9163 - val_loss: 12.2290 - val_mse: 12.1481 - val_mae: 2.7154\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 14.0713 - mse: 13.9888 - mae: 2.9128 - val_loss: 12.0889 - val_mse: 12.0052 - val_mae: 2.7043\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 14.0139 - mse: 13.9289 - mae: 2.9043 - val_loss: 12.0118 - val_mse: 11.9258 - val_mae: 2.6955\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.9651 - mse: 13.8780 - mae: 2.9007 - val_loss: 12.0004 - val_mse: 11.9125 - val_mae: 2.6957\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 13.9516 - mse: 13.8625 - mae: 2.8973 - val_loss: 12.0087 - val_mse: 11.9189 - val_mae: 2.6957\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.8708 - mse: 13.7800 - mae: 2.8871 - val_loss: 11.9262 - val_mse: 11.8347 - val_mae: 2.6896\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.8197 - mse: 13.7274 - mae: 2.8850 - val_loss: 11.9352 - val_mse: 11.8423 - val_mae: 2.6894\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 13.8277 - mse: 13.7338 - mae: 2.8854 - val_loss: 11.8149 - val_mse: 11.7205 - val_mae: 2.6778\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7475 - mse: 13.6521 - mae: 2.8766 - val_loss: 11.8337 - val_mse: 11.7377 - val_mae: 2.6759\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.7227 - mse: 13.6260 - mae: 2.8739 - val_loss: 11.7597 - val_mse: 11.6623 - val_mae: 2.6672\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.6426 - mse: 13.5445 - mae: 2.8646 - val_loss: 11.7585 - val_mse: 11.6597 - val_mae: 2.6690\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 13.6412 - mse: 13.5417 - mae: 2.8657 - val_loss: 11.9164 - val_mse: 11.8164 - val_mae: 2.6857\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 13.6092 - mse: 13.5083 - mae: 2.8610 - val_loss: 11.6003 - val_mse: 11.4986 - val_mae: 2.6511\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 13.5940 - mse: 13.4917 - mae: 2.8564 - val_loss: 11.6370 - val_mse: 11.5340 - val_mae: 2.6577\n",
      "bias -0.0021092624\n",
      "si 0.5363052\n",
      "rmse 0.033961788\n",
      "kgeprime [0.63169893]\n",
      "rmse_95 0.057496704\n",
      "rmse_99 0.07496148\n",
      "pearson 0.824807608263983\n",
      "pearson_95 0.5245812009636195\n",
      "pearson_99 0.3643965336269899\n",
      "rscore 0.674970520439704\n",
      "rscore_95 -2.340593619930125\n",
      "rscore_99 -4.194789605695198\n",
      "nse [0.67497052]\n",
      "nse_95 [-2.34059362]\n",
      "nse_99 [-4.19478961]\n",
      "kge [0.68721814]\n",
      "ext_kge_95 [0.38289842]\n",
      "ext_kge_99 [0.27752496]\n",
      "Loading U\n",
      "Done\n",
      "Loading V\n",
      "Done\n",
      "Calculating relative winds\n",
      "\n",
      " calculating winds with: \n",
      "\n",
      " <xarray.Dataset>\n",
      "Dimensions:         (latitude: 22, longitude: 22, time: 195745)\n",
      "Coordinates:\n",
      "  * latitude        (latitude) float32 -40.43 -40.12 -39.81 ... -34.19 -33.88\n",
      "  * longitude       (longitude) float32 171.2 171.6 171.9 ... 177.2 177.5 177.8\n",
      "  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01\n",
      "Data variables:\n",
      "    ugrd10m         (time, latitude, longitude) float32 -2.06 -1.348 ... 7.381\n",
      "    vgrd10m         (time, latitude, longitude) float32 -2.559 -2.299 ... -3.26\n",
      "    uw2             (time, latitude, longitude) float32 4.242 1.817 ... 54.48\n",
      "    vw2             (time, latitude, longitude) float32 6.551 5.284 ... 10.63\n",
      "    wind_magnitude  (time, latitude, longitude) float32 3.285 2.665 ... 8.069\n",
      "Attributes:\n",
      "    short_name:  ugrd10m\n",
      "    long_name:   U-Component of Wind\n",
      "    level:       10 m above ground\n",
      "    units:       m/s \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Clearing U,V\n",
      "Done\n",
      "Loading MSLP\n",
      "Done\n",
      "\n",
      " adding the wind to the predictor... \n",
      "\n",
      "\n",
      " calculating the gradient of the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " pressure/gradient predictor both with shape: \n",
      " (195745, 10, 10) \n",
      "\n",
      "Returning fold  0  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2012-08-26 10:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([151797 151821 151845], shape=(3,), dtype=int64) Times out: tf.Tensor(151845, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([9325 9349 9373], shape=(3,), dtype=int64) Times out: tf.Tensor(9373, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([39357 39381 39405], shape=(3,), dtype=int64) Times out: tf.Tensor(39405, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([91866 91890 91914], shape=(3,), dtype=int64) Times out: tf.Tensor(91914, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_358\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_359 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_716 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_717 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_358 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_716 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_358 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_717 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 28.3108 - mse: 28.2699 - mae: 4.0558 - val_loss: 18.4017 - val_mse: 18.3545 - val_mae: 3.3194\n",
      "Epoch 2/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 22.0442 - mse: 21.9943 - mae: 3.6150 - val_loss: 17.4699 - val_mse: 17.4173 - val_mae: 3.2407\n",
      "Epoch 3/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 21.0570 - mse: 21.0015 - mae: 3.5333 - val_loss: 16.9425 - val_mse: 16.8839 - val_mae: 3.2006\n",
      "Epoch 4/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.5984 - mse: 20.5371 - mae: 3.4913 - val_loss: 16.6277 - val_mse: 16.5638 - val_mae: 3.1667\n",
      "Epoch 5/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.3549 - mse: 20.2888 - mae: 3.4720 - val_loss: 16.6004 - val_mse: 16.5323 - val_mae: 3.1662\n",
      "Epoch 6/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 20.0825 - mse: 20.0123 - mae: 3.4490 - val_loss: 16.1663 - val_mse: 16.0939 - val_mae: 3.1287\n",
      "Epoch 7/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.8285 - mse: 19.7541 - mae: 3.4258 - val_loss: 15.8313 - val_mse: 15.7550 - val_mae: 3.0926\n",
      "Epoch 8/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.5871 - mse: 19.5090 - mae: 3.4028 - val_loss: 15.5086 - val_mse: 15.4288 - val_mae: 3.0673\n",
      "Epoch 9/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.3857 - mse: 19.3041 - mae: 3.3809 - val_loss: 15.2841 - val_mse: 15.2008 - val_mae: 3.0513\n",
      "Epoch 10/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.2474 - mse: 19.1627 - mae: 3.3720 - val_loss: 15.2562 - val_mse: 15.1700 - val_mae: 3.0428\n",
      "Epoch 11/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 19.0785 - mse: 18.9909 - mae: 3.3585 - val_loss: 15.1860 - val_mse: 15.0966 - val_mae: 3.0431\n",
      "Epoch 12/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.9508 - mse: 18.8602 - mae: 3.3492 - val_loss: 14.7715 - val_mse: 14.6794 - val_mae: 2.9977\n",
      "Epoch 13/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8304 - mse: 18.7371 - mae: 3.3421 - val_loss: 14.9002 - val_mse: 14.8055 - val_mae: 3.0138\n",
      "Epoch 14/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.8170 - mse: 18.7211 - mae: 3.3379 - val_loss: 14.4617 - val_mse: 14.3643 - val_mae: 2.9644\n",
      "Epoch 15/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.6963 - mse: 18.5978 - mae: 3.3261 - val_loss: 14.5694 - val_mse: 14.4697 - val_mae: 2.9792\n",
      "Epoch 16/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.6105 - mse: 18.5095 - mae: 3.3179 - val_loss: 14.4620 - val_mse: 14.3596 - val_mae: 2.9679\n",
      "Epoch 17/20\n",
      "4857/4857 [==============================] - 7s 2ms/step - loss: 18.5811 - mse: 18.4776 - mae: 3.3132 - val_loss: 14.6575 - val_mse: 14.5527 - val_mae: 2.9901\n",
      "Epoch 18/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.5169 - mse: 18.4109 - mae: 3.3104 - val_loss: 14.1613 - val_mse: 14.0541 - val_mae: 2.9348\n",
      "Epoch 19/20\n",
      "4857/4857 [==============================] - 8s 2ms/step - loss: 18.4681 - mse: 18.3597 - mae: 3.3056 - val_loss: 14.2926 - val_mse: 14.1829 - val_mae: 2.9498\n",
      "Epoch 20/20\n",
      "4857/4857 [==============================] - 9s 2ms/step - loss: 18.3393 - mse: 18.2286 - mae: 3.2923 - val_loss: 14.4168 - val_mse: 14.3047 - val_mae: 2.9668\n",
      "bias -0.0006753856\n",
      "si 0.45109713\n",
      "rmse 0.037821546\n",
      "kgeprime [0.7394549]\n",
      "rmse_95 0.062452234\n",
      "rmse_99 0.09272007\n",
      "pearson 0.8841359275225792\n",
      "pearson_95 0.6434411098209343\n",
      "pearson_99 0.7357660145014733\n",
      "rscore 0.7721682265910825\n",
      "rscore_95 -0.744472479720617\n",
      "rscore_99 -2.6231087702753095\n",
      "nse [0.77216823]\n",
      "nse_95 [-0.74447248]\n",
      "nse_99 [-2.62310877]\n",
      "kge [0.75618324]\n",
      "ext_kge_95 [0.56469324]\n",
      "ext_kge_99 [0.58888438]\n",
      "Returning fold  1  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2008-03-20 19:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([76753 76777 76801], shape=(3,), dtype=int64) Times out: tf.Tensor(76801, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([20383 20407 20431], shape=(3,), dtype=int64) Times out: tf.Tensor(20431, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([116085 116109 116133], shape=(3,), dtype=int64) Times out: tf.Tensor(116133, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([81767 81791 81815], shape=(3,), dtype=int64) Times out: tf.Tensor(81815, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2012-08-26 10:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_359\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_360 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_718 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_719 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_359 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_718 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_359 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_719 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4855/4855 [==============================] - 8s 2ms/step - loss: 27.1754 - mse: 27.1289 - mae: 3.9644 - val_loss: 21.0327 - val_mse: 20.9737 - val_mae: 3.5344\n",
      "Epoch 2/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 20.1431 - mse: 20.0791 - mae: 3.4571 - val_loss: 19.9517 - val_mse: 19.8825 - val_mae: 3.4477\n",
      "Epoch 3/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.4365 - mse: 19.3642 - mae: 3.3939 - val_loss: 19.2626 - val_mse: 19.1871 - val_mae: 3.3877\n",
      "Epoch 4/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 19.2022 - mse: 19.1256 - mae: 3.3725 - val_loss: 19.4549 - val_mse: 19.3762 - val_mae: 3.4018\n",
      "Epoch 5/20\n",
      "4855/4855 [==============================] - 8s 2ms/step - loss: 18.9887 - mse: 18.9096 - mae: 3.3530 - val_loss: 19.2392 - val_mse: 19.1583 - val_mae: 3.3830\n",
      "Epoch 6/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.9824 - mse: 18.9013 - mae: 3.3490 - val_loss: 19.1855 - val_mse: 19.1026 - val_mae: 3.3787\n",
      "Epoch 7/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.7669 - mse: 18.6838 - mae: 3.3336 - val_loss: 19.1006 - val_mse: 19.0158 - val_mae: 3.3713\n",
      "Epoch 8/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.6496 - mse: 18.5642 - mae: 3.3229 - val_loss: 18.5235 - val_mse: 18.4364 - val_mae: 3.3199\n",
      "Epoch 9/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.5106 - mse: 18.4229 - mae: 3.3096 - val_loss: 18.2081 - val_mse: 18.1182 - val_mae: 3.2895\n",
      "Epoch 10/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.3032 - mse: 18.2123 - mae: 3.2926 - val_loss: 18.2087 - val_mse: 18.1156 - val_mae: 3.2876\n",
      "Epoch 11/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 18.1139 - mse: 18.0200 - mae: 3.2803 - val_loss: 17.7451 - val_mse: 17.6487 - val_mae: 3.2467\n",
      "Epoch 12/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.9660 - mse: 17.8689 - mae: 3.2656 - val_loss: 17.5994 - val_mse: 17.4999 - val_mae: 3.2328\n",
      "Epoch 13/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.8338 - mse: 17.7336 - mae: 3.2565 - val_loss: 17.7126 - val_mse: 17.6100 - val_mae: 3.2371\n",
      "Epoch 14/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.6763 - mse: 17.5728 - mae: 3.2420 - val_loss: 17.2517 - val_mse: 17.1456 - val_mae: 3.1990\n",
      "Epoch 15/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.5758 - mse: 17.4689 - mae: 3.2315 - val_loss: 17.6460 - val_mse: 17.5367 - val_mae: 3.2315\n",
      "Epoch 16/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.5739 - mse: 17.4640 - mae: 3.2307 - val_loss: 17.3808 - val_mse: 17.2685 - val_mae: 3.2043\n",
      "Epoch 17/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.4548 - mse: 17.3420 - mae: 3.2165 - val_loss: 17.2719 - val_mse: 17.1567 - val_mae: 3.1977\n",
      "Epoch 18/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.4359 - mse: 17.3201 - mae: 3.2146 - val_loss: 17.0849 - val_mse: 16.9669 - val_mae: 3.1819\n",
      "Epoch 19/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.3007 - mse: 17.1818 - mae: 3.2018 - val_loss: 17.1535 - val_mse: 17.0322 - val_mae: 3.1882\n",
      "Epoch 20/20\n",
      "4855/4855 [==============================] - 7s 2ms/step - loss: 17.2697 - mse: 17.1479 - mae: 3.2022 - val_loss: 17.0280 - val_mse: 16.9040 - val_mae: 3.1751\n",
      "bias 0.001853318\n",
      "si 0.45618463\n",
      "rmse 0.041114435\n",
      "kgeprime [0.82668201]\n",
      "rmse_95 0.062087815\n",
      "rmse_99 0.053222835\n",
      "pearson 0.878177936269764\n",
      "pearson_95 0.6945135109986368\n",
      "pearson_99 0.3930548780657428\n",
      "rscore 0.7690101767386174\n",
      "rscore_95 -2.227794587699268\n",
      "rscore_99 -8.431442575503734\n",
      "nse [0.76901018]\n",
      "nse_95 [-2.22779459]\n",
      "nse_99 [-8.43144258]\n",
      "kge [0.78587092]\n",
      "ext_kge_95 [0.20731806]\n",
      "ext_kge_99 [-0.80240425]\n",
      "Returning fold  2  of  5  e.g. 80.0 percent training data\n",
      "\n",
      "1994-12-01 00:00:00 2003-10-14 05:00:00\n",
      "<class 'numpy.ndarray'>\n",
      "All integer correspond to number of hours with respect to reference date\n",
      "Times in: tf.Tensor([1336 1360 1384], shape=(3,), dtype=int64) Times out: tf.Tensor(1384, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([64985 65009 65033], shape=(3,), dtype=int64) Times out: tf.Tensor(65033, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([71238 71262 71286], shape=(3,), dtype=int64) Times out: tf.Tensor(71286, shape=(), dtype=int64)\n",
      "Times in: tf.Tensor([35992 36016 36040], shape=(3,), dtype=int64) Times out: tf.Tensor(36040, shape=(), dtype=int64)\n",
      "\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "2008-03-20 19:00:00 2017-02-01 01:00:00\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "<class 'xarray.core.dataarray.DataArray'>\n",
      "Model: \"model_360\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_361 (InputLayer)       [(None, 3, 10, 10, 3)]    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_720 (TimeDi (None, 3, 10, 10, 24)     672       \n",
      "_________________________________________________________________\n",
      "time_distributed_721 (TimeDi (None, 3, 5, 5, 24)       0         \n",
      "_________________________________________________________________\n",
      "flatten_360 (Flatten)        (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_720 (Dense)            (None, 20)                36020     \n",
      "_________________________________________________________________\n",
      "dropout_360 (Dropout)        (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_721 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 36,713\n",
      "Trainable params: 36,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4856/4856 [==============================] - 8s 2ms/step - loss: 32.1387 - mse: 32.0979 - mae: 4.3075 - val_loss: 20.7360 - val_mse: 20.6841 - val_mae: 3.3998\n",
      "Epoch 2/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 21.3423 - mse: 21.2889 - mae: 3.5746 - val_loss: 19.4074 - val_mse: 19.3521 - val_mae: 3.2817\n",
      "Epoch 3/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.6696 - mse: 20.6135 - mae: 3.5192 - val_loss: 19.2919 - val_mse: 19.2338 - val_mae: 3.2791\n",
      "Epoch 4/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.5424 - mse: 20.4837 - mae: 3.5072 - val_loss: 19.0642 - val_mse: 19.0036 - val_mae: 3.2531\n",
      "Epoch 5/20\n",
      "4856/4856 [==============================] - 7s 2ms/step - loss: 20.2264 - mse: 20.1653 - mae: 3.4823 - val_loss: 19.1669 - val_mse: 19.1043 - val_mae: 3.2663\n",
      "Epoch 6/20\n",
      "3021/4856 [=================>............] - ETA: 2s - loss: 19.9567 - mse: 19.8942 - mae: 3.4661"
     ]
    }
   ],
   "source": [
    "new_results = {}\n",
    "#for site_id in [ 224 ]:\n",
    "for site_id in np.unique( # closest Moana v2 Hindcast to tidal gauges\n",
    "                [ 689,328,393,1327,393,480,999,116,224,1124,949,708, # UHSLC\n",
    "                  1296,1124,780,613,488,1442,1217,578,200,1177,1025,689,949,224,1146, # LINZ\n",
    "                  1174,1260,1217,744,1064,1214,803,999 # OTHER (ports...)\n",
    "                ]\n",
    "            ):\n",
    "    \n",
    "    \n",
    "    location = (float(ss_dset.sel(site=site_id).lon.values),\n",
    "                    float(ss_dset.sel(site=site_id).lat.values))\n",
    "    \n",
    "    \n",
    "    all_predictors = get_best_predictor_for_site(location)\n",
    "    \n",
    "    new_results[site_id]={}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for fold in range(5):\n",
    "        \n",
    "    #try:\n",
    "    #if True:\n",
    "\n",
    "        dset_train, dset_val = get_training_and_validation_sets(predictors=all_predictors,\n",
    "                                                        predictand=predictand.sel(site=site_id),\n",
    "                                                        input_sequence_length=input_sequence_length,\n",
    "                                                        input_sequence_frequency=input_sequence_frequency,\n",
    "                                                        lead_time=lead_time,\n",
    "                                                        target_sequence_frequency=target_sequence_frequency,\n",
    "                                                        batch_size=32,\n",
    "                                                        n_folds=5,\n",
    "                                                        fold=fold,\n",
    "                                                        print_test=True\n",
    "                                                               )\n",
    "\n",
    "\n",
    "        shape_in=(int(input_sequence_length/input_sequence_frequency),) + dset_train.element_spec[0].shape[2:]\n",
    "        model_cnn = build_model_cnn(shape_in=shape_in,\n",
    "                                    nbout=dset_train.element_spec[1].shape[1])\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "        model_cnn.compile(\n",
    "                optimizer,\n",
    "                loss='mse', \n",
    "                metrics=['mse', 'mae'],\n",
    "        )\n",
    "\n",
    "        history = model_cnn.fit(dset_train,\n",
    "                                validation_data=dset_val,\n",
    "                                epochs=20)\n",
    "\n",
    "        prediction_val = model_cnn.predict(dset_val)\n",
    "\n",
    "        new_results[site_id][fold] = calculate_stats(np.concatenate([y.numpy()[:,0] for x, y in dset_val], axis=0)/100, prediction_val[:,0]/100)\n",
    "        new_results[site_id][fold]['history'] = history\n",
    "        new_results[site_id][fold]['prediction'] = prediction_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e88aa99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 [0.847 0.867 0.866 0.854 0.871] 0.8611770913274729\n",
      "200 [0.853 0.883 0.861 0.851 0.869] 0.8632056424892649\n",
      "224 [0.817 0.845 0.841 0.814 0.852] 0.8337756483409251\n",
      "328 [0.814 0.839 0.82  0.797 0.836] 0.8211992680271774\n",
      "393 [0.862 0.888 0.867 0.846 0.878] 0.8682299182188972\n",
      "480 [0.814 0.832 0.808 0.775 0.813] 0.8085695716082986\n",
      "488 [0.813 0.83  0.81  0.781 0.818] 0.8101712131604506\n",
      "578 [0.807 0.83  0.825 0.779 0.815] 0.8110911228516129\n",
      "613 [0.841 0.861 0.856 0.821 0.846] 0.8448179255251743\n",
      "689 [0.84  0.856 0.847 0.818 0.848] 0.8415008402036503\n",
      "708 [0.761 0.779 0.793 0.743 0.768] 0.7687048446076666\n",
      "744 [0.758 0.781 0.79  0.741 0.764] 0.766885375028296\n",
      "780 [0.833 0.848 0.831 0.801 0.838] 0.8303676838023997\n",
      "803 [0.757 0.761 0.781 0.744 0.751] 0.7587142767975468\n",
      "949 [0.783 0.802 0.786 0.761 0.779] 0.7820531250784312\n",
      "999 [0.839 0.852 0.835 0.824 0.842] 0.8385857036833592\n",
      "1025 [0.753 0.763 0.736 0.727 0.727] 0.7413224775402689\n",
      "1064 [0.844 0.853 0.835 0.822 0.844] 0.839551039120131\n",
      "1124 [0.705 0.722 0.725 0.694 0.693] 0.7078216268584046\n",
      "1146 [0.72  0.734 0.721 0.706 0.688] 0.7135973888399679\n",
      "1174 [0.734 0.748 0.746 0.705 0.728] 0.732272623606829\n",
      "1177 [0.839 0.844 0.824 0.814 0.828] 0.8297387165090664\n",
      "1214 [0.697 0.705 0.703 0.667 0.681] 0.6904741648410955\n",
      "1217 [0.643 0.66  0.682 0.625 0.655] 0.6531601329722718\n",
      "1260 [0.829 0.833 0.814 0.807 0.818] 0.8202487534441525\n",
      "1296 [0.69  0.704 0.711 0.673 0.673] 0.6900572184850307\n",
      "1327 [0.577 0.598 0.607 0.566 0.587] 0.5867976023675936\n",
      "1442 [0.629 0.641 0.65  0.592 0.64 ] 0.6305863410713477\n"
     ]
    }
   ],
   "source": [
    "metric = 'pearson'\n",
    "site_id = 116\n",
    "#for fold, res in new_results[site_id].items():\n",
    "for site_id in new_results.keys():\n",
    "        print(site_id, np.round([r[metric] for r in new_results[site_id].values()], 3), np.mean([r[metric] for r in new_results[site_id].values()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a662c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c3b04eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1152 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "%matplotlib inline\n",
    "\n",
    "linear_results = xr.open_dataset('/home/metocean/geocean-nz-ss/data/statistics/experiments/experiment_linear_final_20211113.nc')\n",
    "best_linear_results = linear_results.sel(grad=True, winds=True, tlapse=3, region='local_2.5_2.5', tresample='1D')\n",
    "best_linear_results\n",
    "\n",
    "sites = [s for s in best_linear_results.site.values if s in new_results]\n",
    "metric = 'ext_kge_99'\n",
    "metric = 'pearson'\n",
    "metrics = ['pearson', 'si', 'rel_rmse', 'kgeprime']\n",
    "\n",
    "lons = ss_dset.sel(site=sites).lon.values\n",
    "lats = ss_dset.sel(site=sites).lat.values\n",
    "\n",
    "fig, axes = plt.subplots(ncols=4, nrows=len(metrics), figsize=(14,4*len(metrics)))\n",
    "\n",
    "for im, metric in enumerate(metrics):\n",
    "    \n",
    "    vals_linear = best_linear_results.sel(site=sites)[metric].values\n",
    "    vals_nn = [np.mean([r[metric] for r in new_results[s].values()]) for s in sites]\n",
    "    vals_nn_max = [np.max([r[metric] for r in new_results[s].values()]) for s in sites]\n",
    "    vals_nn_min = [np.min([r[metric] for r in new_results[s].values()]) for s in sites]\n",
    "    \n",
    "    vmin = np.min([np.min(vals_linear), np.min(vals_nn), np.min(vals_nn_max), np.min(vals_nn_min)])\n",
    "    vmax = np.max([np.max(vals_linear), np.max(vals_nn), np.max(vals_nn_max), np.max(vals_nn_min)])\n",
    "\n",
    "    p=axes[im,0].scatter(lons, lats,\n",
    "                      c=vals_linear, vmin=vmin, vmax=vmax)\n",
    "    divider = make_axes_locatable(axes[im,0])\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(p, cax=cax, orientation='vertical')\n",
    "    axes[im,0].set_yticklabels([])\n",
    "    axes[im,0].set_xticklabels([])\n",
    "    axes[im,0].set_title(\"Linear model: \"+metric)\n",
    "\n",
    "    p=axes[im,1].scatter(lons, lats,\n",
    "                      c=vals_nn, vmin=vmin, vmax=vmax)\n",
    "    divider = make_axes_locatable(axes[im,1])\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(p, cax=cax, orientation='vertical')\n",
    "    axes[im,1].set_yticklabels([])\n",
    "    axes[im,1].set_xticklabels([])\n",
    "    axes[im,1].set_title(\"CNN model: mean \"+metric)\n",
    "\n",
    "    p=axes[im,2].scatter(lons, lats,\n",
    "                      c=vals_nn_max, vmin=vmin, vmax=vmax)\n",
    "    divider = make_axes_locatable(axes[im,2])\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(p, cax=cax, orientation='vertical')\n",
    "    axes[im,2].set_yticklabels([])\n",
    "    axes[im,2].set_xticklabels([])\n",
    "    axes[im,2].set_title(\"CNN model max: \"+metric)\n",
    "\n",
    "    p=axes[im,3].scatter(lons, lats,\n",
    "                      c=vals_nn_min, vmin=vmin, vmax=vmax)\n",
    "    divider = make_axes_locatable(axes[im,3])\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    fig.colorbar(p, cax=cax, orientation='vertical')\n",
    "    axes[im,3].set_yticklabels([])\n",
    "    axes[im,3].set_xticklabels([])\n",
    "    axes[im,3].set_title(\"CNN model min: \"+metric)\n",
    "\n",
    "#for v1,v2,v3 in zip(vals_nn, vals_nn_max, vals_nn_min):\n",
    "#    print(v1,v2,v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "56252d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f055a2e2a90>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig, axes = plt.subplots(figure=(16,4))\n",
    "#axes.plot(np.concatenate([y.numpy()[:,0] for x, y in dset_val], axis=0))\n",
    "#axes.plot(prediction_val[:,0])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig,axes = plt.subplots(figsize=(16,4))\n",
    "plt.plot(np.concatenate([y.numpy()[:,0] for x, y in dset_val], axis=0)[20000:40000], label='Truth')\n",
    "plt.plot(prediction_val[20000:40000,0], label='CNN model')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ssnz]",
   "language": "python",
   "name": "conda-env-.conda-ssnz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
