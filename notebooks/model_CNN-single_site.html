
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>&lt;no title&gt; &#8212; Storm Surge NZ - Summary</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="../_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/thebelab-helper.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Storm Surge NZ - Summary</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   geoocean-nz-ss
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapters
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="repo_workflow.html">
   1. Repository workflow + THEORY
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="useful_theory.html">
   2. Useful theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_visualization.html">
   3. Data visualization and validation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="analysis_pca.html">
   4. PC analysis
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models_linear.html">
   5. MultiLinear regression models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models_knn.html">
   6. KNN regression models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models_xgboost.html">
   7. XGBoost regression models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="experiments.html">
   8. RUN experiments
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/model_CNN-single_site.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/javitausia/geocean-nz-ss"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/javitausia/geocean-nz-ss/issues/new?title=Issue%20on%20page%20%2Fnotebooks/model_CNN-single_site.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/javitausia/geocean-nz-ss/master?urlpath=tree/notebooks/model_CNN-single_site.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/javitausia/geocean-nz-ss/blob/master/notebooks/model_CNN-single_site.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="simple visible nav section-nav flex-column">
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1><no title></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="simple visible nav section-nav flex-column">
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># basics</span>
<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">sys</span>

<span class="c1"># arrays</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">xarray</span> <span class="k">as</span> <span class="nn">xr</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">datetime</span><span class="p">,</span>
    <span class="n">timedelta</span>
<span class="p">)</span>

<span class="c1"># plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">cartopy.crs</span> <span class="k">as</span> <span class="nn">ccrs</span>

<span class="c1"># append sscode to path</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;/home/metocean/geocean-nz-ss&#39;</span><span class="p">)</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;/data&#39;</span> <span class="c1">#&#39;/data/storm_surge_data/&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;SSURGE_DATA_PATH&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_path</span>

<span class="c1"># custom</span>
<span class="kn">from</span> <span class="nn">sscode.config</span> <span class="kn">import</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">default_region_reduced</span><span class="p">,</span> <span class="n">default_evaluation_metrics</span><span class="p">,</span> <span class="n">default_region</span>
<span class="kn">from</span> <span class="nn">sscode.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">calculate_relative_winds</span><span class="p">,</span>
    <span class="n">spatial_gradient</span>
<span class="p">)</span>

<span class="c1"># warnings</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="c1"># this is to allow plots to be centered</span>
<span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">&lt;style&gt;</span>
<span class="s2">.output_png {</span>
<span class="s2">    display: table-cell;</span>
<span class="s2">    text-align: center;</span>
<span class="s2">    vertical-align: middle;</span>
<span class="s2">}</span>
<span class="s2">&lt;/style&gt;</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DATA PATH /data
</pre></div>
</div>
<div class="output text_html">
<style>
.output_png {
    display: table-cell;
    text-align: center;
    vertical-align: middle;
}
</style>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset attrs</span>
<span class="n">datasets_attrs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;era5&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="s1">&#39;ERA 5 reanalysis&#39;</span><span class="p">,</span><span class="s1">&#39;u10&#39;</span><span class="p">,</span><span class="s1">&#39;v10&#39;</span><span class="p">),</span>
    <span class="c1">#&#39;cfsr&#39;: (&#39;lon&#39;,&#39;lat&#39;,None,&#39;CFSR reanalysis&#39;,&#39;U_GRD_L103&#39;,&#39;V_GRD_L103&#39;),</span>
     <span class="s1">&#39;cfsr&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="s1">&#39;CFSR reanalysis&#39;</span><span class="p">,</span><span class="s1">&#39;ugrd10m&#39;</span><span class="p">,</span><span class="s1">&#39;vgrd10m&#39;</span><span class="p">),</span>
    <span class="s1">&#39;dac&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span><span class="s1">&#39;DAC global reanalysis&#39;</span><span class="p">),</span>
    <span class="s1">&#39;moana&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;lon&#39;</span><span class="p">,</span><span class="s1">&#39;lat&#39;</span><span class="p">,</span><span class="s1">&#39;site&#39;</span><span class="p">,</span><span class="s1">&#39;Moana v2 hindcast&#39;</span><span class="p">),</span>
    <span class="s1">&#39;codec&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;codec_coords_lon&#39;</span><span class="p">,</span><span class="s1">&#39;codec_coords_lat&#39;</span><span class="p">,</span><span class="s1">&#39;name&#39;</span><span class="p">,</span><span class="s1">&#39;CoDEC reanalysis&#39;</span><span class="p">),</span>
    <span class="s1">&#39;uhslc&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="s1">&#39;name&#39;</span><span class="p">,</span><span class="s1">&#39;UHSLC tgs&#39;</span><span class="p">),</span>
    <span class="s1">&#39;linz&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="s1">&#39;name&#39;</span><span class="p">,</span><span class="s1">&#39;LINZ tgs&#39;</span><span class="p">),</span>
    <span class="s1">&#39;other&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="s1">&#39;name&#39;</span><span class="p">,</span><span class="s1">&#39;OTHER tgs&#39;</span><span class="p">),</span>
    <span class="s1">&#39;privtgs&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="s1">&#39;name&#39;</span><span class="p">,</span><span class="s1">&#39;Private tgs&#39;</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ss_dset</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">open_zarr</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;storm_surge_data/moana_hindcast_v2/moana_coast.zarr/&#39;</span><span class="p">))</span>
<span class="n">ss_dset</span> <span class="o">=</span> <span class="n">ss_dset</span><span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">ss_dset</span><span class="o">.</span><span class="n">lat</span><span class="o">&gt;-</span><span class="mi">50</span><span class="p">,</span> <span class="n">ss_dset</span><span class="o">.</span><span class="n">lon</span> <span class="o">&lt;</span> <span class="mi">180</span><span class="p">))</span>
<span class="n">predictand</span> <span class="o">=</span> <span class="n">ss_dset</span><span class="o">.</span><span class="n">ss</span>\
                    <span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">datetime</span><span class="p">(</span><span class="mi">1994</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2017</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>\
                    <span class="o">.</span><span class="n">drop_duplicates</span><span class="p">(</span><span class="s1">&#39;time&#39;</span><span class="p">)</span>\
                    <span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>\
                    <span class="o">.</span><span class="n">interpolate_na</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;time&#39;</span><span class="p">)</span>\
                    <span class="o">.</span><span class="n">transpose</span><span class="p">()</span>\
                    <span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_best_predictor_for_site</span><span class="p">(</span><span class="n">location</span><span class="p">,</span>
                                <span class="n">dx</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span>
                                <span class="n">region</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">normalised</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    
    <span class="kn">from</span> <span class="nn">sscode.utils</span> <span class="kn">import</span> <span class="n">spatial_gradient</span>
    
    <span class="n">pres_vars</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;SLP&#39;</span><span class="p">,</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">)</span>
    <span class="n">wind_vars</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;wind_proj_mask&#39;</span><span class="p">,</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="s1">&#39;U_GRD_L103&#39;</span><span class="p">,</span><span class="s1">&#39;V_GRD_L103&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">region</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">region</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="n">location</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">dx</span><span class="p">,</span>
                  <span class="n">location</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">dx</span><span class="p">,</span>
                  <span class="n">location</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">dx</span><span class="p">,</span>
                  <span class="n">location</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">dx</span>
                <span class="p">)</span>
    
    <span class="n">region_large</span> <span class="o">=</span> <span class="p">(</span>
              <span class="n">region</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">region</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">region</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">region</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span>
            <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading U&quot;</span><span class="p">)</span>
    <span class="n">uw</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">open_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;cfsr&#39;</span><span class="p">,</span>
                                      <span class="s1">&#39;wnd10m/cfsr_wnd_1979_2021.nc&#39;</span><span class="p">))[</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">4</span><span class="p">]]</span>\
                    <span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">datetime</span><span class="p">(</span><span class="mi">1994</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2017</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>\
                    <span class="o">.</span><span class="n">sel</span><span class="p">({</span>
                      <span class="n">wind_vars</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="nb">slice</span><span class="p">(</span><span class="n">region_large</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">region_large</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                      <span class="n">wind_vars</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="nb">slice</span><span class="p">(</span><span class="n">region_large</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">region_large</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                    <span class="p">})</span>\
                    <span class="o">.</span><span class="n">sortby</span><span class="p">(</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>\
                    <span class="o">.</span><span class="n">sortby</span><span class="p">(</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading V&quot;</span><span class="p">)</span>
    <span class="n">vw</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">open_dataset</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;cfsr&#39;</span><span class="p">,</span>
                                                  <span class="s1">&#39;wnd10m/cfsr_wnd_1979_2021.nc&#39;</span><span class="p">))[</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">5</span><span class="p">]]</span>\
                    <span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">datetime</span><span class="p">(</span><span class="mi">1994</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2017</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>\
                    <span class="o">.</span><span class="n">sel</span><span class="p">({</span>
                      <span class="n">wind_vars</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="nb">slice</span><span class="p">(</span><span class="n">region_large</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">region_large</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                      <span class="n">wind_vars</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="nb">slice</span><span class="p">(</span><span class="n">region_large</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">region_large</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                    <span class="p">})</span>\
                    <span class="o">.</span><span class="n">sortby</span><span class="p">(</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>\
                    <span class="o">.</span><span class="n">sortby</span><span class="p">(</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Calculating relative winds&quot;</span><span class="p">)</span>
    <span class="n">wind</span> <span class="o">=</span> <span class="n">calculate_relative_winds</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="n">location</span><span class="p">,</span> <span class="c1"># load_winds[1],</span>
                                    <span class="n">lat_name</span><span class="o">=</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">lon_name</span><span class="o">=</span><span class="n">datasets_attrs</span><span class="p">[</span><span class="s1">&#39;cfsr&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="n">uw</span><span class="o">=</span><span class="n">uw</span><span class="p">,</span><span class="n">vw</span><span class="o">=</span><span class="n">vw</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Clearing U,V&quot;</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">uw</span>
    <span class="k">del</span> <span class="n">vw</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done&quot;</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading MSLP&quot;</span><span class="p">)</span>
    <span class="n">pres</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">open_dataarray</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;cfsr&#39;</span><span class="p">,</span>
                                          <span class="s1">&#39;CFSR_MSLP_1H_1990_2021.nc&#39;</span><span class="p">))</span>\
             <span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">datetime</span><span class="p">(</span><span class="mi">1994</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">datetime</span><span class="p">(</span><span class="mi">2017</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>\
             <span class="o">.</span><span class="n">sel</span><span class="p">({</span>
                    <span class="n">pres_vars</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="nb">slice</span><span class="p">(</span><span class="n">region</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">region</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                    <span class="n">pres_vars</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="nb">slice</span><span class="p">(</span><span class="n">region</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">region</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
                    <span class="p">})</span>\
             <span class="o">.</span><span class="n">sortby</span><span class="p">(</span><span class="s1">&#39;longitude&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>\
             <span class="o">.</span><span class="n">sortby</span><span class="p">(</span><span class="s1">&#39;latitude&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Done&quot;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">pres_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;wind_proj&#39;</span> <span class="ow">or</span> <span class="n">pres_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;wind_proj_mask&#39;</span><span class="p">:</span> <span class="c1"># when just winds are loaded                                                                               </span>
        <span class="n">pres</span> <span class="o">=</span> <span class="n">pres</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pres</span> <span class="o">=</span> <span class="n">pres</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;time&#39;</span><span class="p">,</span><span class="n">how</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>
    
    <span class="n">wind</span> <span class="o">=</span> <span class="n">wind</span><span class="p">[</span><span class="n">wind_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>\
                             <span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="n">wind_vars</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="n">pres</span><span class="p">[</span><span class="n">pres_vars</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                                             <span class="n">wind_vars</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="n">pres</span><span class="p">[</span><span class="n">pres_vars</span><span class="p">[</span><span class="mi">2</span><span class="p">]]})</span>\
                             <span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">pres</span><span class="o">.</span><span class="n">time</span><span class="p">)</span> <span class="c1"># interp to pressure coords                            </span>
    
    <span class="c1"># calculate the gradient                                                            </span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> calculating the gradient of the sea-level-pressure fields... </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">pres</span> <span class="o">=</span> <span class="n">spatial_gradient</span><span class="p">(</span><span class="n">pres</span><span class="p">,</span><span class="n">pres_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># from utils.py                      </span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> pressure/gradient predictor both with shape: </span><span class="se">\n</span><span class="s1"> </span><span class="si">{}</span><span class="s1"> </span><span class="se">\n</span><span class="s1">&#39;</span>\
            <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pres</span><span class="p">[</span><span class="n">pres_vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        
    <span class="c1"># Normalising</span>
    <span class="k">if</span> <span class="n">normalised</span><span class="p">:</span>
        <span class="n">all_predictors</span> <span class="o">=</span>\
           <span class="n">xr</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
                      <span class="p">(</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP</span><span class="o">.</span><span class="n">min</span><span class="p">()),</span>
                      <span class="p">(</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP_gradient</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP_gradient</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP_gradient</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">pres</span><span class="o">.</span><span class="n">SLP</span><span class="o">.</span><span class="n">min</span><span class="p">()),</span>
                      <span class="p">(</span><span class="n">wind</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">-</span><span class="n">wind</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">wind</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">-</span><span class="n">wind</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
                     <span class="p">],</span>
                     <span class="s2">&quot;channel&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">all_predictors</span> <span class="o">=</span>\
           <span class="n">xr</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
                      <span class="n">pres</span><span class="o">.</span><span class="n">SLP</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                      <span class="n">pres</span><span class="o">.</span><span class="n">SLP_gradient</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                      <span class="n">wind</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                     <span class="p">],</span>
                     <span class="s2">&quot;channel&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">all_predictors</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow.compat.v2</span> <span class="k">as</span> <span class="nn">tfv2</span>
<span class="c1"># pylint: disable=g-classes-have-attributes</span>

<span class="c1"># These functions are adapted from</span>
<span class="c1"># https://github.com/keras-team/keras/blob/06ba37b8662dea768b3bc8201942f1eb877708e8/keras/preprocessing/timeseries.py</span>
<span class="c1"># The main addition is that they targets have been modified to contain both the input grid and the ss output</span>
<span class="c1"># That way it is possible to use the dataset to train both heads of the network</span>
    
<span class="k">def</span> <span class="nf">sequences_from_indices</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">indices_ds</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span><span class="p">):</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(</span><span class="n">array</span><span class="p">[</span><span class="n">start_index</span> <span class="p">:</span> <span class="n">end_index</span><span class="p">])</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">((</span><span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">(),</span> <span class="n">indices_ds</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
      <span class="k">lambda</span> <span class="n">steps</span><span class="p">,</span> <span class="n">inds</span><span class="p">:</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="n">inds</span><span class="p">),</span>  <span class="c1"># pylint: disable=unnecessary-lambda</span>
      <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">dataset</span>

<span class="k">def</span> <span class="nf">timeseries_dataset_from_array_seb</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span>
    <span class="n">targets</span><span class="p">,</span>
    <span class="n">targets_2</span><span class="p">,</span>
    <span class="n">sequence_length</span><span class="p">,</span>
    <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">sequence_stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">start_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">end_index</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Creates a dataset of sliding windows over a timeseries provided as array.</span>
<span class="sd">  This function takes in a sequence of data-points gathered at</span>
<span class="sd">  equal intervals, along with time series parameters such as</span>
<span class="sd">  length of the sequences/windows, spacing between two sequence/windows, etc.,</span>
<span class="sd">  to produce batches of timeseries inputs and targets.</span>
<span class="sd">  Args:</span>
<span class="sd">    data: Numpy array or eager tensor</span>
<span class="sd">      containing consecutive data points (timesteps).</span>
<span class="sd">      Axis 0 is expected to be the time dimension.</span>
<span class="sd">    targets: Targets corresponding to timesteps in `data`.</span>
<span class="sd">      `targets[i]` should be the target</span>
<span class="sd">      corresponding to the window that starts at index `i`</span>
<span class="sd">      (see example 2 below).</span>
<span class="sd">      Pass None if you don&#39;t have target data (in this case the dataset will</span>
<span class="sd">      only yield the input data).</span>
<span class="sd">    sequence_length: Length of the output sequences (in number of timesteps).</span>
<span class="sd">    sequence_stride: Period between successive output sequences.</span>
<span class="sd">      For stride `s`, output samples would</span>
<span class="sd">      start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.</span>
<span class="sd">    sampling_rate: Period between successive individual timesteps</span>
<span class="sd">      within sequences. For rate `r`, timesteps</span>
<span class="sd">      `data[i], data[i + r], ... data[i + sequence_length]`</span>
<span class="sd">      are used for create a sample sequence.</span>
<span class="sd">    batch_size: Number of timeseries samples in each batch</span>
<span class="sd">      (except maybe the last one).</span>
<span class="sd">    shuffle: Whether to shuffle output samples,</span>
<span class="sd">      or instead draw them in chronological order.</span>
<span class="sd">    seed: Optional int; random seed for shuffling.</span>
<span class="sd">    start_index: Optional int; data points earlier (exclusive)</span>
<span class="sd">      than `start_index` will not be used</span>
<span class="sd">      in the output sequences. This is useful to reserve part of the</span>
<span class="sd">      data for test or validation.</span>
<span class="sd">    end_index: Optional int; data points later (exclusive) than `end_index`</span>
<span class="sd">      will not be used in the output sequences.</span>
<span class="sd">      This is useful to reserve part of the data for test or validation.</span>
<span class="sd">  Returns:</span>
<span class="sd">    A tfv2.data.Dataset instance. If `targets` was passed, the dataset yields</span>
<span class="sd">    tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields</span>
<span class="sd">    only `batch_of_sequences`.</span>
<span class="sd">  Example 1:</span>
<span class="sd">  Consider indices `[0, 1, ... 99]`.</span>
<span class="sd">  With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,</span>
<span class="sd">  `shuffle=False`, the dataset will yield batches of sequences</span>
<span class="sd">  composed of the following indices:</span>
<span class="sd">  ```</span>
<span class="sd">  First sequence:  [0  2  4  6  8 10 12 14 16 18]</span>
<span class="sd">  Second sequence: [3  5  7  9 11 13 15 17 19 21]</span>
<span class="sd">  Third sequence:  [6  8 10 12 14 16 18 20 22 24]</span>
<span class="sd">  ...</span>
<span class="sd">  Last sequence:   [78 80 82 84 86 88 90 92 94 96]</span>
<span class="sd">  ```</span>
<span class="sd">  In this case the last 3 data points are discarded since no full sequence</span>
<span class="sd">  can be generated to include them (the next sequence would have started</span>
<span class="sd">  at index 81, and thus its last step would have gone over 99).</span>
<span class="sd">  Example 2: Temporal regression.</span>
<span class="sd">  Consider an array `data` of scalar values, of shape `(steps,)`.</span>
<span class="sd">  To generate a dataset that uses the past 10</span>
<span class="sd">  timesteps to predict the next timestep, you would use:</span>
<span class="sd">  ```python</span>
<span class="sd">  input_data = data[:-10]</span>
<span class="sd">  targets = data[10:]</span>
<span class="sd">  dataset = tfv2.keras.preprocessing.timeseries_dataset_from_array(</span>
<span class="sd">      input_data, targets, sequence_length=10)</span>
<span class="sd">  for batch in dataset:</span>
<span class="sd">    inputs, targets = batch</span>
<span class="sd">    assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]</span>
<span class="sd">    assert np.array_equal(targets[0], data[10])  # Corresponding target: step 10</span>
<span class="sd">    break</span>
<span class="sd">  ```</span>
<span class="sd">  Example 3: Temporal regression for many-to-many architectures.</span>
<span class="sd">  Consider two arrays of scalar values `X` and `Y`,</span>
<span class="sd">  both of shape `(100,)`. The resulting dataset should consist samples with</span>
<span class="sd">  20 timestamps each. The samples should not overlap.</span>
<span class="sd">  To generate a dataset that uses the current timestamp</span>
<span class="sd">  to predict the corresponding target timestep, you would use:</span>
<span class="sd">  ```python</span>
<span class="sd">  X = np.arange(100)</span>
<span class="sd">  Y = X*2</span>
<span class="sd">  sample_length = 20</span>
<span class="sd">  input_dataset = tfv2.keras.preprocessing.timeseries_dataset_from_array(</span>
<span class="sd">    X, None, sequence_length=sample_length, sequence_stride=sample_length)</span>
<span class="sd">  target_dataset = tfv2.keras.preprocessing.timeseries_dataset_from_array(</span>
<span class="sd">    Y, None, sequence_length=sample_length, sequence_stride=sample_length)</span>
<span class="sd">  for batch in zip(input_dataset, target_dataset):</span>
<span class="sd">    inputs, targets = batch</span>
<span class="sd">    assert np.array_equal(inputs[0], X[:sample_length])</span>
<span class="sd">    # second sample equals output timestamps 20-40</span>
<span class="sd">    assert np.array_equal(targets[1], Y[sample_length:2*sample_length])</span>
<span class="sd">    break</span>
<span class="sd">  ```</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">start_index</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">start_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`start_index` must be 0 or greater. Received: &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;start_index=</span><span class="si">{</span><span class="n">start_index</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">start_index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`start_index` must be lower than the length of the &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;data. Received: start_index=</span><span class="si">{</span><span class="n">start_index</span><span class="si">}</span><span class="s1">, for data &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">end_index</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">start_index</span> <span class="ow">and</span> <span class="n">end_index</span> <span class="o">&lt;=</span> <span class="n">start_index</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`end_index` must be higher than `start_index`. &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;Received: start_index=</span><span class="si">{</span><span class="n">start_index</span><span class="si">}</span><span class="s1">, and &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;end_index=</span><span class="si">{</span><span class="n">end_index</span><span class="si">}</span><span class="s1"> &#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end_index</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`end_index` must be lower than the length of the &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;data. Received: end_index=</span><span class="si">{</span><span class="n">end_index</span><span class="si">}</span><span class="s1">, for data of &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end_index</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;`end_index` must be higher than 0. &#39;</span>
                       <span class="sa">f</span><span class="s1">&#39;Received: end_index=</span><span class="si">{</span><span class="n">end_index</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="c1"># Validate strides</span>
  <span class="k">if</span> <span class="n">sampling_rate</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`sampling_rate` must be higher than 0. Received: &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;sampling_rate=</span><span class="si">{</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">sampling_rate</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`sampling_rate` must be lower than the length of the &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;data. Received: sampling_rate=</span><span class="si">{</span><span class="n">sampling_rate</span><span class="si">}</span><span class="s1">, for data &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">sequence_stride</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`sequence_stride` must be higher than 0. Received: &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;sequence_stride=</span><span class="si">{</span><span class="n">sequence_stride</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">sequence_stride</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;`sequence_stride` must be lower than the length of the &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;data. Received: sequence_stride=</span><span class="si">{</span><span class="n">sequence_stride</span><span class="si">}</span><span class="s1">, for &#39;</span>
                     <span class="sa">f</span><span class="s1">&#39;data of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">start_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">if</span> <span class="n">end_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

  <span class="c1"># Determine the lowest dtype to store start positions (to lower memory usage).</span>
  <span class="n">num_seqs</span> <span class="o">=</span> <span class="n">end_index</span> <span class="o">-</span> <span class="n">start_index</span> <span class="o">-</span> <span class="p">(</span><span class="n">sequence_length</span> <span class="o">*</span> <span class="n">sampling_rate</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">num_seqs</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_seqs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">num_seqs</span> <span class="o">&lt;</span> <span class="mi">2147483647</span><span class="p">:</span>
    <span class="n">index_dtype</span> <span class="o">=</span> <span class="s1">&#39;int32&#39;</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">index_dtype</span> <span class="o">=</span> <span class="s1">&#39;int64&#39;</span>

  <span class="c1"># Generate start positions</span>
  <span class="n">start_positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_seqs</span><span class="p">,</span> <span class="n">sequence_stride</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">index_dtype</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mf">1e6</span><span class="p">)</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">start_positions</span><span class="p">)</span>

  <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">index_dtype</span><span class="p">)</span>
  <span class="n">sampling_rate</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">sampling_rate</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">index_dtype</span><span class="p">)</span>

  <span class="n">positions_ds</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensors</span><span class="p">(</span><span class="n">start_positions</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span>

  <span class="c1"># For each initial window position, generates indices of the window elements</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span>
      <span class="p">(</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="p">)),</span> <span class="n">positions_ds</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">positions</span><span class="p">:</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">range</span><span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
              <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
              <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">sequence_length</span> <span class="o">*</span> <span class="n">sampling_rate</span><span class="p">,</span>
              <span class="n">sampling_rate</span><span class="p">),</span>
          <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

  <span class="n">dataset</span> <span class="o">=</span> <span class="n">sequences_from_indices</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset</span><span class="p">]</span>
  <span class="k">if</span> <span class="n">targets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span>
        <span class="p">(</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="p">)),</span> <span class="n">positions_ds</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">positions</span><span class="p">:</span> <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
    <span class="n">target_ds</span> <span class="o">=</span> <span class="n">sequences_from_indices</span><span class="p">(</span>
        <span class="n">targets</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">targets_2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target_ds_2</span> <span class="o">=</span> <span class="n">sequences_from_indices</span><span class="p">(</span>
            <span class="n">targets_2</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span><span class="p">)</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">target_ds</span><span class="p">,</span> <span class="n">target_ds_2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
    <span class="c1">#outputs.append((target_ds, dataset))</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target_ds</span><span class="p">)</span>
    <span class="c1">##dataset = tfv2.data.Dataset.zip((dataset, (target_ds, dataset)))</span>
    
    <span class="c1">#outputs.append((target_ds, dataset))</span>
    <span class="c1">#outputs.append(target_ds)</span>

  <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span>
        <span class="p">(</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="p">)),</span> <span class="n">positions_ds</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">positions</span><span class="p">:</span> <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
    <span class="n">target_weights_dset</span> <span class="o">=</span> <span class="n">sequences_from_indices</span><span class="p">(</span>
            <span class="n">weights</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span><span class="p">)</span>
    
    <span class="n">indices</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span>
      <span class="p">(</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">start_positions</span><span class="p">)),</span> <span class="n">positions_ds</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
          <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">positions</span><span class="p">:</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">range</span><span class="p">(</span>  <span class="c1"># pylint: disable=g-long-lambda</span>
              <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
              <span class="n">positions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">sequence_length</span> <span class="o">*</span> <span class="n">sampling_rate</span><span class="p">,</span>
              <span class="n">sampling_rate</span><span class="p">),</span>
          <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

    <span class="n">ae_weights_dset</span> <span class="o">=</span> <span class="n">sequences_from_indices</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">indices</span><span class="p">,</span> <span class="n">start_index</span><span class="p">,</span> <span class="n">end_index</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">target_weights_dset</span><span class="p">,</span> <span class="n">ae_weights_dset</span><span class="p">))</span>
    <span class="c1">#dataset = tfv2.data.Dataset.zip((dataset, (target_ds, dataset)))</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">zip</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
    <span class="c1"># Shuffle locally at each iteration</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tfv2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sscode.validation</span> <span class="kn">import</span> <span class="n">generate_stats</span>
<span class="kn">from</span> <span class="nn">sscode.config</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">default_evaluation_metrics</span><span class="p">,</span>
    <span class="n">default_ext_quantile</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_stats</span><span class="p">(</span><span class="n">ts1</span><span class="p">,</span> <span class="n">ts2</span><span class="p">):</span>
    <span class="n">title</span><span class="p">,</span> <span class="n">stats</span> <span class="o">=</span> <span class="n">generate_stats</span><span class="p">(</span><span class="n">ts1</span><span class="p">,</span>
                              <span class="n">ts2</span><span class="p">,</span>
                              <span class="n">metrics</span><span class="o">=</span><span class="n">default_evaluation_metrics</span><span class="p">,</span>
                              <span class="n">ext_quantile</span><span class="o">=</span><span class="n">default_ext_quantile</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;si&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;rmse&#39;</span><span class="p">,</span> <span class="s1">&#39;kgeprime&#39;</span><span class="p">,</span> <span class="s1">&#39;rmse_95&#39;</span><span class="p">,</span> <span class="s1">&#39;rmse_99&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;pearson&#39;</span><span class="p">,</span> <span class="s1">&#39;pearson_95&#39;</span><span class="p">,</span> <span class="s1">&#39;pearson_99&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;rscore&#39;</span><span class="p">,</span> <span class="s1">&#39;rscore_95&#39;</span><span class="p">,</span> <span class="s1">&#39;rscore_99&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;nse&#39;</span><span class="p">,</span> <span class="s1">&#39;nse_95&#39;</span><span class="p">,</span> <span class="s1">&#39;nse_99&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;kge&#39;</span><span class="p">,</span> <span class="s1">&#39;ext_kge_95&#39;</span><span class="p">,</span> <span class="s1">&#39;ext_kge_99&#39;</span><span class="p">]:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">stats</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">stats</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Conv2D</span><span class="p">,</span>
    <span class="n">Conv3D</span><span class="p">,</span>
    <span class="n">BatchNormalization</span><span class="p">,</span>
    <span class="n">MaxPool2D</span><span class="p">,</span>
    <span class="n">MaxPool3D</span><span class="p">,</span>
    <span class="n">ConvLSTM2D</span><span class="p">,</span>
    <span class="n">GlobalMaxPool2D</span><span class="p">,</span>
    <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span>
    <span class="n">TimeDistributed</span><span class="p">,</span>
    <span class="n">GRU</span><span class="p">,</span>
    <span class="n">Dense</span><span class="p">,</span>
    <span class="n">Dropout</span><span class="p">,</span>
    <span class="n">Conv1D</span><span class="p">,</span>
    <span class="n">LSTM</span><span class="p">,</span>
    <span class="n">Conv2DTranspose</span><span class="p">,</span>
    <span class="n">Reshape</span><span class="p">,</span>
    <span class="n">Cropping2D</span><span class="p">,</span>
    <span class="n">Cropping1D</span><span class="p">,</span>
    <span class="n">Activation</span><span class="p">,</span>
    <span class="n">Lambda</span><span class="p">,</span>
    <span class="n">Concatenate</span><span class="p">,</span>
    <span class="n">TimeDistributed</span><span class="p">,</span>
    <span class="n">Flatten</span><span class="p">,</span>
    <span class="n">Reshape</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This one works!!!</span>
<span class="k">def</span> <span class="nf">build_model_cnn</span><span class="p">(</span><span class="n">shape_in</span><span class="p">,</span>
                    <span class="n">nbout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">shape_in</span><span class="p">)</span>
    
    <span class="n">conv_1</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">cnn_outputs</span> <span class="o">=</span> <span class="n">TimeDistributed</span><span class="p">(</span><span class="n">MaxPool2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))(</span><span class="n">conv_1</span><span class="p">)</span>     
    
    <span class="n">flat</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">cnn_outputs</span><span class="p">)</span>

    <span class="n">dense_1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span>
                    <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span>
                    <span class="p">)(</span><span class="n">flat</span><span class="p">)</span>
    
    <span class="n">dropout_1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">dense_1</span><span class="p">)</span>
    
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">nbout</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">dropout_1</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">to_use</span><span class="o">=</span><span class="mi">1</span>

<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">set_visible_devices</span><span class="p">(</span><span class="n">gpus</span><span class="p">[</span><span class="n">to_use</span><span class="p">],</span> <span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
    <span class="n">logical_gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_logical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gpus</span><span class="p">),</span> <span class="s2">&quot;Physical GPUs,&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logical_gpus</span><span class="p">),</span> <span class="s2">&quot;Logical GPU&quot;</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="c1"># Visible devices must be set before GPUs have been initialized</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="c1"># Invalid device or cannot modify virtual devices once initialized.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Failed to select GPU&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 Physical GPUs, 1 Logical GPU
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_training_and_validation_sets</span><span class="p">(</span><span class="n">predictors</span><span class="p">,</span>
                                     <span class="n">predictand</span><span class="p">,</span>
                                     <span class="n">input_sequence_length</span><span class="p">,</span>
                                     <span class="n">input_sequence_frequency</span><span class="p">,</span>
                                     <span class="n">lead_time</span><span class="p">,</span>
                                     <span class="n">target_sequence_frequency</span><span class="p">,</span>
                                     <span class="n">fold</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                     <span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                                     <span class="n">print_test</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="k">if</span> <span class="p">(</span> <span class="ow">not</span> <span class="p">(</span><span class="n">predictand</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">predictand</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span>\
        <span class="p">(</span><span class="n">predictand</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">predictand</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Time spacing is not constant in predictand dataset&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="k">if</span> <span class="p">(</span> <span class="ow">not</span> <span class="p">(</span><span class="n">predictors</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">predictors</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span>\
        <span class="p">(</span><span class="n">predictors</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">predictors</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Time spacing is not constant in predictor dataset&quot;</span><span class="p">)</span>
            <span class="k">raise</span>
    
    <span class="c1"># Find start, end and extent of dataset</span>
    <span class="p">[</span><span class="n">tstart_predictor</span><span class="p">,</span> <span class="n">tend_predictor</span><span class="p">]</span> <span class="o">=</span> <span class="n">predictors</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;datetime64[s]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="p">[</span><span class="n">tstart_predictand</span><span class="p">,</span> <span class="n">tend_predictand</span><span class="p">]</span> <span class="o">=</span> <span class="n">predictand</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;datetime64[s]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">tstart</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tstart_predictand</span><span class="p">,</span> <span class="n">tstart_predictor</span><span class="p">)</span>
    <span class="n">tend</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">tend_predictand</span><span class="p">,</span> <span class="n">tend_predictor</span><span class="p">)</span>
    <span class="n">time_extent</span> <span class="o">=</span> <span class="n">tend</span> <span class="o">-</span> <span class="n">tstart</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Returning fold &quot;</span><span class="p">,</span><span class="n">fold</span><span class="p">,</span><span class="s2">&quot; of &quot;</span><span class="p">,</span> <span class="n">n_folds</span><span class="p">,</span> <span class="s2">&quot; e.g. </span><span class="si">%2.1f</span><span class="s2"> percent training data&quot;</span><span class="o">%</span><span class="p">((</span><span class="n">n_folds</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n_folds</span><span class="o">*</span><span class="mf">100.</span><span class="p">))</span>

    <span class="c1"># Finding time bounds of training data segments</span>
    <span class="n">tstart_1</span> <span class="o">=</span> <span class="n">tstart</span>
    <span class="n">tend_1</span> <span class="o">=</span> <span class="p">(</span><span class="n">tstart</span> <span class="o">+</span> <span class="n">time_extent</span><span class="o">*</span><span class="p">((</span><span class="n">n_folds</span><span class="o">-</span><span class="mi">1</span><span class="o">-</span><span class="n">fold</span><span class="p">)</span><span class="o">/</span><span class="n">n_folds</span><span class="p">))</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">second</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">microsecond</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">minute</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">tstart_2</span> <span class="o">=</span> <span class="p">(</span><span class="n">tstart</span> <span class="o">+</span> <span class="n">time_extent</span><span class="o">*</span><span class="p">((</span><span class="n">n_folds</span><span class="o">-</span><span class="n">fold</span><span class="p">)</span><span class="o">/</span><span class="n">n_folds</span><span class="p">))</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">second</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">microsecond</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">minute</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">tend_2</span> <span class="o">=</span> <span class="n">tend</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    
    <span class="n">train_dset</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tstart_train</span><span class="p">,</span> <span class="n">tend_train</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">tstart_1</span><span class="p">,</span> <span class="n">tstart_2</span><span class="p">],[</span><span class="n">tend_1</span><span class="p">,</span> <span class="n">tend_2</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">tstart_train</span> <span class="o">!=</span> <span class="n">tend_train</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">tstart_train</span><span class="p">,</span> <span class="n">tend_train</span><span class="p">)</span>

            <span class="n">predictor_train</span> <span class="o">=</span>\
                <span class="n">predictors</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">tstart_train</span><span class="p">,</span>
                                          <span class="n">tend_train</span><span class="o">-</span><span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="p">(</span><span class="n">lead_time</span><span class="o">-</span><span class="n">target_sequence_frequency</span><span class="p">))))</span>\
                          <span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">input_sequence_frequency</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>\
                          <span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="p">(</span><span class="n">input_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>\
                          <span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">input_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>\
                          <span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="n">ss_train</span> <span class="o">=</span> <span class="n">predictand</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">tstart_train</span><span class="o">+</span><span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="n">input_sequence_length</span><span class="o">+</span><span class="n">lead_time</span><span class="o">-</span><span class="n">target_sequence_frequency</span><span class="p">),</span>
                                      <span class="n">tend_train</span><span class="p">))</span>\
                                 <span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>\
                                 <span class="o">.</span><span class="n">interpolate_na</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;time&#39;</span><span class="p">)</span>\
                                 <span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">target_sequence_frequency</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>\
                                 <span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>\
                                 <span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>\
                                 <span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>

            <span class="k">if</span> <span class="n">print_test</span><span class="p">:</span> <span class="c1"># To check indices are fine</span>
                <span class="n">test_dset</span> <span class="o">=</span> <span class="n">timeseries_dataset_from_array_seb</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">predictor_train</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span><span class="o">-</span><span class="mi">786412800000000000</span><span class="p">)</span><span class="o">//</span><span class="mi">3600000000000</span><span class="p">,</span>
                    <span class="p">(</span><span class="n">ss_train</span><span class="o">.</span><span class="n">time</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span><span class="o">-</span><span class="mi">786412800000000000</span><span class="p">)</span><span class="o">//</span><span class="mi">3600000000000</span><span class="p">,</span>
                    <span class="n">targets_2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">sequence_length</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">input_sequence_length</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),</span>
                    <span class="n">sequence_stride</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),</span>
                    <span class="n">sampling_rate</span><span class="o">=</span><span class="n">input_sequence_frequency</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end_index</span><span class="o">=</span><span class="kc">None</span>
                    <span class="p">)</span>
        
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All integer correspond to number of hours with respect to reference date&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dset</span><span class="p">:</span>
                    <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span>
                    <span class="k">for</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Times in:&quot;</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="s2">&quot;Times out:&quot;</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
                    <span class="n">print_test</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">break</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

            <span class="n">train_dset</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">timeseries_dataset_from_array_seb</span><span class="p">(</span>
                    <span class="n">predictor_train</span><span class="p">,</span>
                    <span class="n">ss_train</span><span class="p">,</span>
                    <span class="n">targets_2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">input_sequence_length</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),</span>
                <span class="n">sequence_stride</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),</span>
                <span class="n">sampling_rate</span><span class="o">=</span><span class="n">input_sequence_frequency</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end_index</span><span class="o">=</span><span class="kc">None</span>
                <span class="p">))</span>
            
    <span class="c1"># If multiple segments concatenate them into a single dataset</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">train_dset</span> <span class="o">=</span> <span class="n">train_dset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
        <span class="n">train_dset</span> <span class="o">=</span> <span class="n">train_dset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">train_dset</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


    <span class="c1"># Validation data</span>
    <span class="n">predictor_val</span> <span class="o">=</span> <span class="n">predictors</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">tend_1</span><span class="p">,</span>
                                              <span class="n">tstart_2</span><span class="o">-</span><span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="p">(</span><span class="n">lead_time</span><span class="o">-</span><span class="n">target_sequence_frequency</span><span class="p">))))</span>\
                              <span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">input_sequence_frequency</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>\
                              <span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="p">(</span><span class="n">input_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>\
                              <span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">input_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>\
                              <span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">ss_val</span> <span class="o">=</span> <span class="n">predictand</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="n">tend_1</span><span class="o">+</span><span class="n">timedelta</span><span class="p">(</span><span class="n">hours</span><span class="o">=</span><span class="n">input_sequence_length</span><span class="o">+</span><span class="n">lead_time</span><span class="o">-</span><span class="n">target_sequence_frequency</span><span class="p">),</span>
                                       <span class="n">tstart_2</span><span class="p">))</span>\
                       <span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>\
                       <span class="o">.</span><span class="n">interpolate_na</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="s1">&#39;time&#39;</span><span class="p">)</span>\
                       <span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">target_sequence_frequency</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>\
                       <span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="n">time</span><span class="o">=-</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>\
                       <span class="o">.</span><span class="n">isel</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>\
                       <span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="s2">&quot;channel&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>


    <span class="n">val_dset</span> <span class="o">=</span> <span class="n">timeseries_dataset_from_array_seb</span><span class="p">(</span>
        <span class="n">predictor_val</span><span class="p">,</span>
        <span class="n">ss_val</span><span class="p">,</span>
        <span class="n">targets_2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sequence_length</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">input_sequence_length</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),</span>
        <span class="n">sequence_stride</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">target_sequence_frequency</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="n">input_sequence_frequency</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">start_index</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end_index</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">train_dset</span><span class="p">,</span> <span class="n">val_dset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1">#for site_id in [ 224 ]:</span>
<span class="k">for</span> <span class="n">site_id</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span> <span class="c1"># closest Moana v2 Hindcast to tidal gauges</span>
                <span class="p">[</span> <span class="mi">689</span><span class="p">,</span><span class="mi">328</span><span class="p">,</span><span class="mi">393</span><span class="p">,</span><span class="mi">1327</span><span class="p">,</span><span class="mi">393</span><span class="p">,</span><span class="mi">480</span><span class="p">,</span><span class="mi">999</span><span class="p">,</span><span class="mi">116</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">1124</span><span class="p">,</span><span class="mi">949</span><span class="p">,</span><span class="mi">708</span><span class="p">,</span> <span class="c1"># UHSLC</span>
                  <span class="mi">1296</span><span class="p">,</span><span class="mi">1124</span><span class="p">,</span><span class="mi">780</span><span class="p">,</span><span class="mi">613</span><span class="p">,</span><span class="mi">488</span><span class="p">,</span><span class="mi">1442</span><span class="p">,</span><span class="mi">1217</span><span class="p">,</span><span class="mi">578</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">1177</span><span class="p">,</span><span class="mi">1025</span><span class="p">,</span><span class="mi">689</span><span class="p">,</span><span class="mi">949</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">1146</span><span class="p">,</span> <span class="c1"># LINZ</span>
                  <span class="mi">1174</span><span class="p">,</span><span class="mi">1260</span><span class="p">,</span><span class="mi">1217</span><span class="p">,</span><span class="mi">744</span><span class="p">,</span><span class="mi">1064</span><span class="p">,</span><span class="mi">1214</span><span class="p">,</span><span class="mi">803</span><span class="p">,</span><span class="mi">999</span> <span class="c1"># OTHER (ports...)</span>
                <span class="p">]</span>
            <span class="p">):</span>
    
    
    <span class="n">location</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">ss_dset</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">site_id</span><span class="p">)</span><span class="o">.</span><span class="n">lon</span><span class="o">.</span><span class="n">values</span><span class="p">),</span>
                    <span class="nb">float</span><span class="p">(</span><span class="n">ss_dset</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">site_id</span><span class="p">)</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
    
    
    <span class="n">all_predictors</span> <span class="o">=</span> <span class="n">get_best_predictor_for_site</span><span class="p">(</span><span class="n">location</span><span class="p">)</span>
    
    <span class="n">new_results</span><span class="p">[</span><span class="n">site_id</span><span class="p">]</span><span class="o">=</span><span class="p">{}</span>
    
    
    
    <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        
    <span class="c1">#try:</span>
    <span class="c1">#if True:</span>

        <span class="n">dset_train</span><span class="p">,</span> <span class="n">dset_val</span> <span class="o">=</span> <span class="n">get_training_and_validation_sets</span><span class="p">(</span><span class="n">predictors</span><span class="o">=</span><span class="n">all_predictors</span><span class="p">,</span>
                                                        <span class="n">predictand</span><span class="o">=</span><span class="n">predictand</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">site_id</span><span class="p">),</span>
                                                        <span class="n">input_sequence_length</span><span class="o">=</span><span class="n">input_sequence_length</span><span class="p">,</span>
                                                        <span class="n">input_sequence_frequency</span><span class="o">=</span><span class="n">input_sequence_frequency</span><span class="p">,</span>
                                                        <span class="n">lead_time</span><span class="o">=</span><span class="n">lead_time</span><span class="p">,</span>
                                                        <span class="n">target_sequence_frequency</span><span class="o">=</span><span class="n">target_sequence_frequency</span><span class="p">,</span>
                                                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                                                        <span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                                        <span class="n">fold</span><span class="o">=</span><span class="n">fold</span><span class="p">,</span>
                                                        <span class="n">print_test</span><span class="o">=</span><span class="kc">True</span>
                                                               <span class="p">)</span>


        <span class="n">shape_in</span><span class="o">=</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">input_sequence_length</span><span class="o">/</span><span class="n">input_sequence_frequency</span><span class="p">),)</span> <span class="o">+</span> <span class="n">dset_train</span><span class="o">.</span><span class="n">element_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">model_cnn</span> <span class="o">=</span> <span class="n">build_model_cnn</span><span class="p">(</span><span class="n">shape_in</span><span class="o">=</span><span class="n">shape_in</span><span class="p">,</span>
                                    <span class="n">nbout</span><span class="o">=</span><span class="n">dset_train</span><span class="o">.</span><span class="n">element_spec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">)</span>
        <span class="n">model_cnn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
                <span class="n">optimizer</span><span class="p">,</span>
                <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> 
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="s1">&#39;mae&#39;</span><span class="p">],</span>
        <span class="p">)</span>

        <span class="n">history</span> <span class="o">=</span> <span class="n">model_cnn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dset_train</span><span class="p">,</span>
                                <span class="n">validation_data</span><span class="o">=</span><span class="n">dset_val</span><span class="p">,</span>
                                <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

        <span class="n">prediction_val</span> <span class="o">=</span> <span class="n">model_cnn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dset_val</span><span class="p">)</span>

        <span class="n">new_results</span><span class="p">[</span><span class="n">site_id</span><span class="p">][</span><span class="n">fold</span><span class="p">]</span> <span class="o">=</span> <span class="n">calculate_stats</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dset_val</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="n">prediction_val</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
        <span class="n">new_results</span><span class="p">[</span><span class="n">site_id</span><span class="p">][</span><span class="n">fold</span><span class="p">][</span><span class="s1">&#39;history&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">history</span>
        <span class="n">new_results</span><span class="p">[</span><span class="n">site_id</span><span class="p">][</span><span class="n">fold</span><span class="p">][</span><span class="s1">&#39;prediction&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prediction_val</span>
        
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -50.11 -49.8 -49.49 ... -43.56 -43.24
  * longitude       (longitude) float32 165.0 165.3 165.6 ... 170.9 171.2 171.6
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -9.86 -9.789 ... -2.205
    vgrd10m         (time, latitude, longitude) float32 11.66 11.62 ... 2.572
    uw2             (time, latitude, longitude) float32 97.21 95.83 ... 4.861
    vw2             (time, latitude, longitude) float32 135.9 135.0 ... 6.617
    wind_magnitude  (time, latitude, longitude) float32 15.27 15.19 ... 3.388
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([119787 119811 119835], shape=(3,), dtype=int64) Times out: tf.Tensor(119835, shape=(), dtype=int64)
Times in: tf.Tensor([127035 127059 127083], shape=(3,), dtype=int64) Times out: tf.Tensor(127083, shape=(), dtype=int64)
Times in: tf.Tensor([81373 81397 81421], shape=(3,), dtype=int64) Times out: tf.Tensor(81421, shape=(), dtype=int64)
Times in: tf.Tensor([97244 97268 97292], shape=(3,), dtype=int64) Times out: tf.Tensor(97292, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_253&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_254 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_506 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_507 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_253 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_506 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_253 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_507 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 51.1239 - mse: 51.0650 - mae: 5.5192 - val_loss: 36.9328 - val_mse: 36.8598 - val_mae: 4.8413
Epoch 2/20
4857/4857 [==============================] - 8s 2ms/step - loss: 39.6420 - mse: 39.5596 - mae: 4.9129 - val_loss: 35.6100 - val_mse: 35.5196 - val_mae: 4.7599
Epoch 3/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.6498 - mse: 38.5524 - mae: 4.8520 - val_loss: 35.1465 - val_mse: 35.0425 - val_mae: 4.7257
Epoch 4/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.2969 - mse: 38.1876 - mae: 4.8255 - val_loss: 34.8703 - val_mse: 34.7554 - val_mae: 4.7053
Epoch 5/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.7381 - mse: 37.6180 - mae: 4.7915 - val_loss: 34.5444 - val_mse: 34.4189 - val_mae: 4.6911
Epoch 6/20
4857/4857 [==============================] - 9s 2ms/step - loss: 37.5160 - mse: 37.3855 - mae: 4.7751 - val_loss: 34.5018 - val_mse: 34.3661 - val_mae: 4.6925
Epoch 7/20
4857/4857 [==============================] - 12s 3ms/step - loss: 37.3241 - mse: 37.1834 - mae: 4.7615 - val_loss: 34.1152 - val_mse: 33.9697 - val_mae: 4.6666
Epoch 8/20
4857/4857 [==============================] - 12s 3ms/step - loss: 36.8714 - mse: 36.7206 - mae: 4.7374 - val_loss: 34.4347 - val_mse: 34.2790 - val_mae: 4.6901
Epoch 9/20
4857/4857 [==============================] - 12s 3ms/step - loss: 36.5764 - mse: 36.4161 - mae: 4.7153 - val_loss: 35.1946 - val_mse: 35.0297 - val_mae: 4.7414
Epoch 10/20
4857/4857 [==============================] - 12s 2ms/step - loss: 36.3419 - mse: 36.1726 - mae: 4.6975 - val_loss: 34.1961 - val_mse: 34.0229 - val_mae: 4.6800
Epoch 11/20
4857/4857 [==============================] - 12s 2ms/step - loss: 36.0694 - mse: 35.8927 - mae: 4.6839 - val_loss: 35.7373 - val_mse: 35.5569 - val_mae: 4.7831
Epoch 12/20
4857/4857 [==============================] - 9s 2ms/step - loss: 35.9318 - mse: 35.7479 - mae: 4.6754 - val_loss: 34.2336 - val_mse: 34.0466 - val_mae: 4.6844
Epoch 13/20
4857/4857 [==============================] - 12s 2ms/step - loss: 35.5747 - mse: 35.3847 - mae: 4.6468 - val_loss: 34.7408 - val_mse: 34.5479 - val_mae: 4.7187
Epoch 14/20
4857/4857 [==============================] - 12s 3ms/step - loss: 35.5102 - mse: 35.3144 - mae: 4.6430 - val_loss: 34.6903 - val_mse: 34.4919 - val_mae: 4.7160
Epoch 15/20
4857/4857 [==============================] - 12s 3ms/step - loss: 35.4207 - mse: 35.2199 - mae: 4.6321 - val_loss: 34.6018 - val_mse: 34.3986 - val_mae: 4.7106
Epoch 16/20
4857/4857 [==============================] - 12s 2ms/step - loss: 34.9937 - mse: 34.7889 - mae: 4.6092 - val_loss: 34.1244 - val_mse: 33.9179 - val_mae: 4.6772
Epoch 17/20
4857/4857 [==============================] - 12s 2ms/step - loss: 35.0886 - mse: 34.8801 - mae: 4.6149 - val_loss: 34.1419 - val_mse: 33.9315 - val_mae: 4.6776
Epoch 18/20
4857/4857 [==============================] - 12s 2ms/step - loss: 34.8675 - mse: 34.6554 - mae: 4.6007 - val_loss: 33.6123 - val_mse: 33.3987 - val_mae: 4.6363
Epoch 19/20
4857/4857 [==============================] - 8s 2ms/step - loss: 34.6063 - mse: 34.3916 - mae: 4.5815 - val_loss: 33.7704 - val_mse: 33.5542 - val_mae: 4.6505
Epoch 20/20
4857/4857 [==============================] - 8s 2ms/step - loss: 34.3471 - mse: 34.1300 - mae: 4.5606 - val_loss: 33.2230 - val_mse: 33.0047 - val_mae: 4.6119
bias -0.012828387
si 0.48975155
rmse 0.057449713
kgeprime [0.53193962]
rmse_95 0.0777748
rmse_99 0.09913381
pearson 0.848115183006452
pearson_95 0.6958499914095382
pearson_99 0.7086672561791206
rscore 0.7042894573040572
rscore_95 -1.0448784778618356
rscore_99 -9.856662702087696
nse [0.70428946]
nse_95 [-1.04487848]
nse_99 [-9.8566627]
kge [0.64294887]
ext_kge_95 [0.60871329]
ext_kge_99 [-0.19349587]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([25860 25884 25908], shape=(3,), dtype=int64) Times out: tf.Tensor(25908, shape=(), dtype=int64)
Times in: tf.Tensor([2205 2229 2253], shape=(3,), dtype=int64) Times out: tf.Tensor(2253, shape=(), dtype=int64)
Times in: tf.Tensor([71341 71365 71389], shape=(3,), dtype=int64) Times out: tf.Tensor(71389, shape=(), dtype=int64)
Times in: tf.Tensor([53354 53378 53402], shape=(3,), dtype=int64) Times out: tf.Tensor(53402, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_254&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_255 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_508 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_509 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_254 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_508 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_254 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_509 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 50.1068 - mse: 50.0477 - mae: 5.4760 - val_loss: 39.5802 - val_mse: 39.5098 - val_mae: 4.9657
Epoch 2/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.5805 - mse: 38.5020 - mae: 4.8617 - val_loss: 37.8939 - val_mse: 37.8067 - val_mae: 4.8536
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.3281 - mse: 37.2339 - mae: 4.7736 - val_loss: 37.1518 - val_mse: 37.0503 - val_mae: 4.7951
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.7407 - mse: 36.6331 - mae: 4.7331 - val_loss: 36.7433 - val_mse: 36.6296 - val_mae: 4.7623
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.3999 - mse: 36.2810 - mae: 4.7119 - val_loss: 37.0393 - val_mse: 36.9157 - val_mae: 4.7732
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.9153 - mse: 35.7862 - mae: 4.6805 - val_loss: 36.7856 - val_mse: 36.6517 - val_mae: 4.7603
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.5296 - mse: 35.3900 - mae: 4.6489 - val_loss: 35.9727 - val_mse: 35.8286 - val_mae: 4.7049
Epoch 8/20
4855/4855 [==============================] - 10s 2ms/step - loss: 34.7924 - mse: 34.6431 - mae: 4.6040 - val_loss: 35.3097 - val_mse: 35.1569 - val_mae: 4.6591
Epoch 9/20
4855/4855 [==============================] - 12s 3ms/step - loss: 34.3433 - mse: 34.1857 - mae: 4.5735 - val_loss: 34.3980 - val_mse: 34.2375 - val_mae: 4.6004
Epoch 10/20
4855/4855 [==============================] - 12s 3ms/step - loss: 34.0461 - mse: 33.8814 - mae: 4.5491 - val_loss: 34.3086 - val_mse: 34.1416 - val_mae: 4.5916
Epoch 11/20
4855/4855 [==============================] - 12s 3ms/step - loss: 33.5625 - mse: 33.3917 - mae: 4.5131 - val_loss: 33.8567 - val_mse: 33.6835 - val_mae: 4.5603
Epoch 12/20
4855/4855 [==============================] - 12s 3ms/step - loss: 33.2367 - mse: 33.0593 - mae: 4.4917 - val_loss: 33.4962 - val_mse: 33.3167 - val_mae: 4.5340
Epoch 13/20
4855/4855 [==============================] - 12s 3ms/step - loss: 33.0959 - mse: 32.9130 - mae: 4.4758 - val_loss: 33.5662 - val_mse: 33.3817 - val_mae: 4.5325
Epoch 14/20
4855/4855 [==============================] - 12s 3ms/step - loss: 32.7274 - mse: 32.5398 - mae: 4.4549 - val_loss: 32.9034 - val_mse: 32.7142 - val_mae: 4.4895
Epoch 15/20
4855/4855 [==============================] - 13s 3ms/step - loss: 32.6067 - mse: 32.4148 - mae: 4.4470 - val_loss: 32.4881 - val_mse: 32.2948 - val_mae: 4.4682
Epoch 16/20
4855/4855 [==============================] - 9s 2ms/step - loss: 32.3060 - mse: 32.1100 - mae: 4.4224 - val_loss: 32.9922 - val_mse: 32.7949 - val_mae: 4.4916
Epoch 17/20
4855/4855 [==============================] - 12s 3ms/step - loss: 32.2955 - mse: 32.0958 - mae: 4.4233 - val_loss: 32.5589 - val_mse: 32.3575 - val_mae: 4.4639
Epoch 18/20
4855/4855 [==============================] - 12s 3ms/step - loss: 32.1411 - mse: 31.9379 - mae: 4.4114 - val_loss: 32.2431 - val_mse: 32.0388 - val_mae: 4.4405
Epoch 19/20
4855/4855 [==============================] - 12s 3ms/step - loss: 32.0097 - mse: 31.8036 - mae: 4.4018 - val_loss: 32.5399 - val_mse: 32.3327 - val_mae: 4.4582
Epoch 20/20
4855/4855 [==============================] - 12s 3ms/step - loss: 32.0485 - mse: 31.8390 - mae: 4.4039 - val_loss: 32.5531 - val_mse: 32.3427 - val_mae: 4.4589
bias 0.0003243396
si 0.4709503
rmse 0.05687064
kgeprime [0.74837427]
rmse_95 0.083566666
rmse_99 0.08912001
pearson 0.8690262843176926
pearson_95 0.5139396821609185
pearson_99 0.17517287581961338
rscore 0.7471416030904823
rscore_95 -4.057378425888483
rscore_99 -17.046814495298133
nse [0.7471416]
nse_95 [-4.05737843]
nse_99 [-17.0468145]
kge [0.74321952]
ext_kge_95 [0.06497173]
ext_kge_99 [-0.9379045]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([68495 68519 68543], shape=(3,), dtype=int64) Times out: tf.Tensor(68543, shape=(), dtype=int64)
Times in: tf.Tensor([27124 27148 27172], shape=(3,), dtype=int64) Times out: tf.Tensor(27172, shape=(), dtype=int64)
Times in: tf.Tensor([58291 58315 58339], shape=(3,), dtype=int64) Times out: tf.Tensor(58339, shape=(), dtype=int64)
Times in: tf.Tensor([15443 15467 15491], shape=(3,), dtype=int64) Times out: tf.Tensor(15491, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_255&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_256 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_510 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_511 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_255 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_510 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_255 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_511 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 10s 2ms/step - loss: 53.8704 - mse: 53.8174 - mae: 5.7109 - val_loss: 37.5125 - val_mse: 37.4468 - val_mae: 4.7226
Epoch 2/20
4856/4856 [==============================] - 8s 2ms/step - loss: 40.0296 - mse: 39.9587 - mae: 4.9690 - val_loss: 35.8804 - val_mse: 35.8041 - val_mae: 4.6099
Epoch 3/20
4856/4856 [==============================] - 8s 2ms/step - loss: 38.9160 - mse: 38.8367 - mae: 4.8968 - val_loss: 35.7338 - val_mse: 35.6497 - val_mae: 4.5912
Epoch 4/20
4856/4856 [==============================] - 8s 2ms/step - loss: 38.6690 - mse: 38.5820 - mae: 4.8772 - val_loss: 35.2386 - val_mse: 35.1479 - val_mae: 4.5657
Epoch 5/20
4856/4856 [==============================] - 8s 2ms/step - loss: 38.3990 - mse: 38.3052 - mae: 4.8582 - val_loss: 35.1101 - val_mse: 35.0126 - val_mae: 4.5523
Epoch 6/20
4856/4856 [==============================] - 8s 2ms/step - loss: 38.0964 - mse: 37.9962 - mae: 4.8379 - val_loss: 35.2502 - val_mse: 35.1463 - val_mae: 4.5559
Epoch 7/20
4856/4856 [==============================] - 8s 2ms/step - loss: 37.9485 - mse: 37.8417 - mae: 4.8337 - val_loss: 35.1677 - val_mse: 35.0571 - val_mae: 4.5522
Epoch 8/20
4856/4856 [==============================] - 8s 2ms/step - loss: 37.7268 - mse: 37.6131 - mae: 4.8192 - val_loss: 34.6922 - val_mse: 34.5747 - val_mae: 4.5278
Epoch 9/20
4856/4856 [==============================] - 8s 2ms/step - loss: 37.3809 - mse: 37.2603 - mae: 4.7943 - val_loss: 34.6338 - val_mse: 34.5094 - val_mae: 4.5244
Epoch 10/20
4856/4856 [==============================] - 8s 2ms/step - loss: 37.2897 - mse: 37.1622 - mae: 4.7884 - val_loss: 34.4087 - val_mse: 34.2774 - val_mae: 4.5056
Epoch 11/20
4856/4856 [==============================] - 8s 2ms/step - loss: 37.0989 - mse: 36.9647 - mae: 4.7750 - val_loss: 34.7258 - val_mse: 34.5881 - val_mae: 4.5195
Epoch 12/20
4856/4856 [==============================] - 10s 2ms/step - loss: 36.9225 - mse: 36.7819 - mae: 4.7611 - val_loss: 34.4581 - val_mse: 34.3142 - val_mae: 4.5026
Epoch 13/20
4856/4856 [==============================] - 12s 3ms/step - loss: 36.7424 - mse: 36.5961 - mae: 4.7491 - val_loss: 34.4542 - val_mse: 34.3047 - val_mae: 4.4976
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 13s 3ms/step - loss: 36.3476 - mse: 36.1953 - mae: 4.7247 - val_loss: 33.5487 - val_mse: 33.3931 - val_mae: 4.4427
Epoch 15/20
4856/4856 [==============================] - 12s 3ms/step - loss: 36.1282 - mse: 35.9700 - mae: 4.7075 - val_loss: 33.1661 - val_mse: 33.0048 - val_mae: 4.4168
Epoch 16/20
4856/4856 [==============================] - 12s 3ms/step - loss: 35.8398 - mse: 35.6764 - mae: 4.6847 - val_loss: 32.4633 - val_mse: 32.2975 - val_mae: 4.3711
Epoch 17/20
4856/4856 [==============================] - 12s 2ms/step - loss: 35.5148 - mse: 35.3475 - mae: 4.6681 - val_loss: 32.3761 - val_mse: 32.2069 - val_mae: 4.3644
Epoch 18/20
4856/4856 [==============================] - 12s 2ms/step - loss: 35.2685 - mse: 35.0976 - mae: 4.6510 - val_loss: 32.6636 - val_mse: 32.4911 - val_mae: 4.3821
Epoch 19/20
4856/4856 [==============================] - 13s 3ms/step - loss: 35.2208 - mse: 35.0471 - mae: 4.6396 - val_loss: 31.7964 - val_mse: 31.6216 - val_mae: 4.3181
Epoch 20/20
4856/4856 [==============================] - 12s 2ms/step - loss: 35.0322 - mse: 34.8562 - mae: 4.6324 - val_loss: 31.5820 - val_mse: 31.4047 - val_mae: 4.3044
bias 0.0037284107
si 0.47071767
rmse 0.05603989
kgeprime [0.79769894]
rmse_95 0.11709115
rmse_99 0.21082576
pearson 0.8652860728277744
pearson_95 0.16752426359350425
pearson_99 -0.2444489950795243
rscore 0.7433998721564699
rscore_95 -1.9978625637075198
rscore_99 -6.262203010873385
nse [0.74339987]
nse_95 [-1.99786256]
nse_99 [-6.26220301]
kge [0.74439394]
ext_kge_95 [0.10376966]
ext_kge_99 [-0.31495991]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([ 9988 10012 10036], shape=(3,), dtype=int64) Times out: tf.Tensor(10036, shape=(), dtype=int64)
Times in: tf.Tensor([14472 14496 14520], shape=(3,), dtype=int64) Times out: tf.Tensor(14520, shape=(), dtype=int64)
Times in: tf.Tensor([37146 37170 37194], shape=(3,), dtype=int64) Times out: tf.Tensor(37194, shape=(), dtype=int64)
Times in: tf.Tensor([38629 38653 38677], shape=(3,), dtype=int64) Times out: tf.Tensor(38677, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_256&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_257 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_512 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_513 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_256 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_512 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_256 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_513 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 13s 3ms/step - loss: 53.3961 - mse: 53.3523 - mae: 5.6642 - val_loss: 37.3595 - val_mse: 37.3064 - val_mae: 4.7539
Epoch 2/20
4855/4855 [==============================] - 12s 3ms/step - loss: 40.5974 - mse: 40.5401 - mae: 4.9899 - val_loss: 36.1805 - val_mse: 36.1184 - val_mae: 4.6757
Epoch 3/20
4855/4855 [==============================] - 12s 3ms/step - loss: 39.4660 - mse: 39.4007 - mae: 4.9121 - val_loss: 36.1941 - val_mse: 36.1250 - val_mae: 4.6791
Epoch 4/20
4855/4855 [==============================] - 11s 2ms/step - loss: 39.0548 - mse: 38.9828 - mae: 4.8918 - val_loss: 35.4298 - val_mse: 35.3537 - val_mae: 4.6300
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.9855 - mse: 38.9063 - mae: 4.8815 - val_loss: 35.4634 - val_mse: 35.3801 - val_mae: 4.6285
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.6684 - mse: 38.5814 - mae: 4.8588 - val_loss: 35.3781 - val_mse: 35.2867 - val_mae: 4.6330
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.3706 - mse: 38.2754 - mae: 4.8437 - val_loss: 35.3558 - val_mse: 35.2559 - val_mae: 4.6257
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.1581 - mse: 38.0540 - mae: 4.8332 - val_loss: 35.0278 - val_mse: 34.9189 - val_mae: 4.6038
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.9334 - mse: 37.8203 - mae: 4.8163 - val_loss: 35.0923 - val_mse: 34.9744 - val_mae: 4.6022
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.5529 - mse: 37.4306 - mae: 4.7914 - val_loss: 34.3659 - val_mse: 34.2389 - val_mae: 4.5504
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.4907 - mse: 37.3597 - mae: 4.7865 - val_loss: 34.2604 - val_mse: 34.1250 - val_mae: 4.5524
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.2098 - mse: 37.0700 - mae: 4.7685 - val_loss: 34.0369 - val_mse: 33.8925 - val_mae: 4.5461
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.1423 - mse: 36.9941 - mae: 4.7627 - val_loss: 34.0248 - val_mse: 33.8726 - val_mae: 4.5416
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.7634 - mse: 36.6073 - mae: 4.7398 - val_loss: 33.5893 - val_mse: 33.4294 - val_mae: 4.5141
Epoch 15/20
4855/4855 [==============================] - 10s 2ms/step - loss: 36.7425 - mse: 36.5786 - mae: 4.7347 - val_loss: 33.4538 - val_mse: 33.2863 - val_mae: 4.4938
Epoch 16/20
4855/4855 [==============================] - 12s 3ms/step - loss: 36.4776 - mse: 36.3063 - mae: 4.7102 - val_loss: 33.0967 - val_mse: 32.9218 - val_mae: 4.4665
Epoch 17/20
4855/4855 [==============================] - 12s 3ms/step - loss: 36.2254 - mse: 36.0471 - mae: 4.6990 - val_loss: 32.4824 - val_mse: 32.3011 - val_mae: 4.4250
Epoch 18/20
4855/4855 [==============================] - 12s 3ms/step - loss: 35.8663 - mse: 35.6819 - mae: 4.6704 - val_loss: 32.3135 - val_mse: 32.1264 - val_mae: 4.4197
Epoch 19/20
4855/4855 [==============================] - 12s 3ms/step - loss: 35.4623 - mse: 35.2727 - mae: 4.6405 - val_loss: 32.5431 - val_mse: 32.3518 - val_mae: 4.4516
Epoch 20/20
4855/4855 [==============================] - 13s 3ms/step - loss: 35.2429 - mse: 35.0491 - mae: 4.6287 - val_loss: 32.1386 - val_mse: 31.9432 - val_mae: 4.4198
bias 0.009248525
si 0.48798564
rmse 0.056518342
kgeprime [0.71679296]
rmse_95 0.09571484
rmse_99 0.10896053
pearson 0.8522727823640246
pearson_95 0.6027665753110135
pearson_99 0.5184291386065779
rscore 0.7187797857486767
rscore_95 -2.177985179644043
rscore_99 -7.169626673111191
nse [0.71877979]
nse_95 [-2.17798518]
nse_99 [-7.16962667]
kge [0.68951903]
ext_kge_95 [0.43605505]
ext_kge_99 [-0.11137088]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([134697 134721 134745], shape=(3,), dtype=int64) Times out: tf.Tensor(134745, shape=(), dtype=int64)
Times in: tf.Tensor([182155 182179 182203], shape=(3,), dtype=int64) Times out: tf.Tensor(182203, shape=(), dtype=int64)
Times in: tf.Tensor([82455 82479 82503], shape=(3,), dtype=int64) Times out: tf.Tensor(82503, shape=(), dtype=int64)
Times in: tf.Tensor([151989 152013 152037], shape=(3,), dtype=int64) Times out: tf.Tensor(152037, shape=(), dtype=int64)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_257&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_258 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_514 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_515 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_257 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_514 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_257 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_515 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 14s 3ms/step - loss: 56.2804 - mse: 56.2338 - mae: 5.7782 - val_loss: 36.2174 - val_mse: 36.1610 - val_mae: 4.7293
Epoch 2/20
4857/4857 [==============================] - 12s 3ms/step - loss: 41.3729 - mse: 41.3108 - mae: 5.0311 - val_loss: 34.1141 - val_mse: 34.0468 - val_mae: 4.5778
Epoch 3/20
4857/4857 [==============================] - 12s 3ms/step - loss: 39.9789 - mse: 39.9071 - mae: 4.9438 - val_loss: 33.5804 - val_mse: 33.5043 - val_mae: 4.5339
Epoch 4/20
4857/4857 [==============================] - 12s 3ms/step - loss: 39.5650 - mse: 39.4856 - mae: 4.9173 - val_loss: 33.3246 - val_mse: 33.2416 - val_mae: 4.5164
Epoch 5/20
4857/4857 [==============================] - 12s 3ms/step - loss: 39.1687 - mse: 39.0825 - mae: 4.8867 - val_loss: 32.9475 - val_mse: 32.8586 - val_mae: 4.4953
Epoch 6/20
4857/4857 [==============================] - 12s 3ms/step - loss: 39.2075 - mse: 39.1159 - mae: 4.8932 - val_loss: 32.8618 - val_mse: 32.7672 - val_mae: 4.4894
Epoch 7/20
4857/4857 [==============================] - 11s 2ms/step - loss: 38.6866 - mse: 38.5894 - mae: 4.8574 - val_loss: 32.4734 - val_mse: 32.3735 - val_mae: 4.4630
Epoch 8/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.5513 - mse: 38.4492 - mae: 4.8495 - val_loss: 32.2570 - val_mse: 32.1523 - val_mae: 4.4432
Epoch 9/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.3259 - mse: 38.2189 - mae: 4.8422 - val_loss: 32.3825 - val_mse: 32.2731 - val_mae: 4.4533
Epoch 10/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.3073 - mse: 38.1955 - mae: 4.8344 - val_loss: 32.0315 - val_mse: 31.9173 - val_mae: 4.4235
Epoch 11/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.2155 - mse: 38.0990 - mae: 4.8300 - val_loss: 31.8955 - val_mse: 31.7766 - val_mae: 4.4152
Epoch 12/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.0641 - mse: 37.9431 - mae: 4.8172 - val_loss: 31.8595 - val_mse: 31.7362 - val_mae: 4.4122
Epoch 13/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.8649 - mse: 37.7392 - mae: 4.8025 - val_loss: 32.0530 - val_mse: 31.9249 - val_mae: 4.4283
Epoch 14/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.7387 - mse: 37.6083 - mae: 4.7927 - val_loss: 31.3633 - val_mse: 31.2305 - val_mae: 4.3730
Epoch 15/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.5422 - mse: 37.4074 - mae: 4.7860 - val_loss: 31.2334 - val_mse: 31.0962 - val_mae: 4.3610
Epoch 16/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.4173 - mse: 37.2778 - mae: 4.7755 - val_loss: 30.9766 - val_mse: 30.8347 - val_mae: 4.3422
Epoch 17/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.1368 - mse: 36.9926 - mae: 4.7558 - val_loss: 30.4667 - val_mse: 30.3202 - val_mae: 4.3028
Epoch 18/20
4857/4857 [==============================] - 10s 2ms/step - loss: 36.8029 - mse: 36.6547 - mae: 4.7354 - val_loss: 30.2950 - val_mse: 30.1447 - val_mae: 4.2916
Epoch 19/20
4857/4857 [==============================] - 12s 3ms/step - loss: 36.6067 - mse: 36.4546 - mae: 4.7214 - val_loss: 29.7722 - val_mse: 29.6184 - val_mae: 4.2533
Epoch 20/20
4857/4857 [==============================] - 12s 3ms/step - loss: 36.3270 - mse: 36.1719 - mae: 4.7009 - val_loss: 29.8026 - val_mse: 29.6462 - val_mae: 4.2542
bias 0.0039751614
si 0.4693175
rmse 0.054448348
kgeprime [0.8021737]
rmse_95 0.08910656
rmse_99 0.11739118
pearson 0.8643396181923318
pearson_95 0.5496398954831371
pearson_99 0.41523521091579724
rscore 0.7424751824238571
rscore_95 -2.3316744418759336
rscore_99 -3.302179584012457
nse [0.74247518]
nse_95 [-2.33167444]
nse_99 [-3.30217958]
kge [0.74721921]
ext_kge_95 [0.47215048]
ext_kge_99 [0.29261173]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -49.49 -49.18 -48.86 ... -42.93 -42.62
  * longitude       (longitude) float32 163.1 163.4 163.8 ... 169.4 169.7 170.0
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -10.61 -10.4 ... -4.54
    vgrd10m         (time, latitude, longitude) float32 10.44 10.63 ... 0.07264
    uw2             (time, latitude, longitude) float32 112.6 108.1 ... 20.61
    vw2             (time, latitude, longitude) float32 109.0 113.0 ... 0.005276
    wind_magnitude  (time, latitude, longitude) float32 14.88 14.87 ... 4.54
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([53516 53540 53564], shape=(3,), dtype=int64) Times out: tf.Tensor(53564, shape=(), dtype=int64)
Times in: tf.Tensor([134701 134725 134749], shape=(3,), dtype=int64) Times out: tf.Tensor(134749, shape=(), dtype=int64)
Times in: tf.Tensor([26382 26406 26430], shape=(3,), dtype=int64) Times out: tf.Tensor(26430, shape=(), dtype=int64)
Times in: tf.Tensor([34133 34157 34181], shape=(3,), dtype=int64) Times out: tf.Tensor(34181, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_258&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_259 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_516 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_517 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_258 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_516 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_258 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_517 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 43.2141 - mse: 43.1664 - mae: 5.0643 - val_loss: 27.2060 - val_mse: 27.1493 - val_mae: 4.1810
Epoch 2/20
4857/4857 [==============================] - 8s 2ms/step - loss: 31.4710 - mse: 31.4118 - mae: 4.3799 - val_loss: 26.2510 - val_mse: 26.1898 - val_mae: 4.1133
Epoch 3/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.6537 - mse: 30.5904 - mae: 4.3199 - val_loss: 25.4965 - val_mse: 25.4310 - val_mae: 4.0545
Epoch 4/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.3571 - mse: 30.2898 - mae: 4.2964 - val_loss: 25.4019 - val_mse: 25.3333 - val_mae: 4.0459
Epoch 5/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.0425 - mse: 29.9727 - mae: 4.2739 - val_loss: 24.9439 - val_mse: 24.8732 - val_mae: 4.0026
Epoch 6/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.9871 - mse: 29.9155 - mae: 4.2660 - val_loss: 25.1202 - val_mse: 25.0477 - val_mae: 4.0216
Epoch 7/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.8446 - mse: 29.7714 - mae: 4.2545 - val_loss: 24.9111 - val_mse: 24.8369 - val_mae: 4.0082
Epoch 8/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.7270 - mse: 29.6521 - mae: 4.2451 - val_loss: 24.9292 - val_mse: 24.8533 - val_mae: 4.0096
Epoch 9/20
4857/4857 [==============================] - 9s 2ms/step - loss: 29.4793 - mse: 29.4028 - mae: 4.2261 - val_loss: 24.4896 - val_mse: 24.4120 - val_mae: 3.9675
Epoch 10/20
4857/4857 [==============================] - 12s 3ms/step - loss: 29.2281 - mse: 29.1496 - mae: 4.2071 - val_loss: 24.2624 - val_mse: 24.1828 - val_mae: 3.9495
Epoch 11/20
4857/4857 [==============================] - 12s 3ms/step - loss: 29.0123 - mse: 28.9317 - mae: 4.1908 - val_loss: 24.3077 - val_mse: 24.2261 - val_mae: 3.9578
Epoch 12/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.8060 - mse: 28.7235 - mae: 4.1720 - val_loss: 23.9386 - val_mse: 23.8551 - val_mae: 3.9264
Epoch 13/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.6288 - mse: 28.5443 - mae: 4.1576 - val_loss: 23.5875 - val_mse: 23.5022 - val_mae: 3.8956
Epoch 14/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.4434 - mse: 28.3573 - mae: 4.1443 - val_loss: 23.1946 - val_mse: 23.1077 - val_mae: 3.8599
Epoch 15/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.3421 - mse: 28.2545 - mae: 4.1398 - val_loss: 23.1205 - val_mse: 23.0324 - val_mae: 3.8553
Epoch 16/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.2694 - mse: 28.1806 - mae: 4.1272 - val_loss: 22.9981 - val_mse: 22.9089 - val_mae: 3.8444
Epoch 17/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.1587 - mse: 28.0686 - mae: 4.1234 - val_loss: 22.9886 - val_mse: 22.8980 - val_mae: 3.8472
Epoch 18/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.0162 - mse: 27.9250 - mae: 4.1053 - val_loss: 22.8795 - val_mse: 22.7874 - val_mae: 3.8363
Epoch 19/20
4857/4857 [==============================] - 12s 3ms/step - loss: 28.0545 - mse: 27.9619 - mae: 4.1086 - val_loss: 23.0621 - val_mse: 22.9688 - val_mae: 3.8570
Epoch 20/20
4857/4857 [==============================] - 12s 3ms/step - loss: 27.9049 - mse: 27.8110 - mae: 4.1004 - val_loss: 22.7340 - val_mse: 22.6394 - val_mae: 3.8220
bias -0.0050381855
si 0.47463933
rmse 0.047580894
kgeprime [0.66648569]
rmse_95 0.063250236
rmse_99 0.073814206
pearson 0.8610595699863389
pearson_95 0.6214049089070305
pearson_99 0.4895707539131495
rscore 0.7376725464204983
rscore_95 -1.7953555529924108
rscore_99 -9.70841559764021
nse [0.73767255]
nse_95 [-1.79535555]
nse_99 [-9.7084156]
kge [0.74162906]
ext_kge_95 [0.46318088]
ext_kge_99 [-0.21566854]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([64217 64241 64265], shape=(3,), dtype=int64) Times out: tf.Tensor(64265, shape=(), dtype=int64)
Times in: tf.Tensor([6071 6095 6119], shape=(3,), dtype=int64) Times out: tf.Tensor(6119, shape=(), dtype=int64)
Times in: tf.Tensor([10333 10357 10381], shape=(3,), dtype=int64) Times out: tf.Tensor(10381, shape=(), dtype=int64)
Times in: tf.Tensor([38423 38447 38471], shape=(3,), dtype=int64) Times out: tf.Tensor(38471, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_259&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_260 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_518 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_519 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_259 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_518 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_259 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_519 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 13s 3ms/step - loss: 43.4731 - mse: 43.4251 - mae: 5.0985 - val_loss: 32.3602 - val_mse: 32.3044 - val_mae: 4.5218
Epoch 2/20
4855/4855 [==============================] - 10s 2ms/step - loss: 32.2581 - mse: 32.1996 - mae: 4.4257 - val_loss: 27.9250 - val_mse: 27.8635 - val_mae: 4.2100
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 30.6245 - mse: 30.5590 - mae: 4.3071 - val_loss: 26.9252 - val_mse: 26.8560 - val_mae: 4.1355
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 30.0795 - mse: 30.0062 - mae: 4.2687 - val_loss: 26.7390 - val_mse: 26.6622 - val_mae: 4.1188
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 29.5753 - mse: 29.4946 - mae: 4.2326 - val_loss: 26.2928 - val_mse: 26.2088 - val_mae: 4.0845
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 29.4310 - mse: 29.3434 - mae: 4.2190 - val_loss: 26.7106 - val_mse: 26.6197 - val_mae: 4.1138
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 29.1742 - mse: 29.0801 - mae: 4.1982 - val_loss: 26.5975 - val_mse: 26.5008 - val_mae: 4.1034
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.8752 - mse: 28.7752 - mae: 4.1766 - val_loss: 25.6704 - val_mse: 25.5678 - val_mae: 4.0315
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.5835 - mse: 28.4778 - mae: 4.1571 - val_loss: 26.4342 - val_mse: 26.3262 - val_mae: 4.0918
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.2724 - mse: 28.1614 - mae: 4.1348 - val_loss: 25.8990 - val_mse: 25.7859 - val_mae: 4.0497
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.2592 - mse: 28.1433 - mae: 4.1305 - val_loss: 25.3417 - val_mse: 25.2238 - val_mae: 4.0077
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 27.8811 - mse: 27.7606 - mae: 4.1023 - val_loss: 24.8836 - val_mse: 24.7613 - val_mae: 3.9727
Epoch 13/20
4855/4855 [==============================] - 9s 2ms/step - loss: 27.8670 - mse: 27.7424 - mae: 4.0975 - val_loss: 24.6589 - val_mse: 24.5329 - val_mae: 3.9551
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 12s 2ms/step - loss: 27.7460 - mse: 27.6180 - mae: 4.0895 - val_loss: 24.3435 - val_mse: 24.2146 - val_mae: 3.9311
Epoch 15/20
4855/4855 [==============================] - 12s 3ms/step - loss: 27.4490 - mse: 27.3183 - mae: 4.0726 - val_loss: 25.7517 - val_mse: 25.6201 - val_mae: 4.0393
Epoch 16/20
4855/4855 [==============================] - 12s 3ms/step - loss: 27.3123 - mse: 27.1788 - mae: 4.0585 - val_loss: 24.4187 - val_mse: 24.2845 - val_mae: 3.9347
Epoch 17/20
4855/4855 [==============================] - 12s 3ms/step - loss: 27.3599 - mse: 27.2241 - mae: 4.0575 - val_loss: 24.9033 - val_mse: 24.7669 - val_mae: 3.9728
Epoch 18/20
4855/4855 [==============================] - 12s 3ms/step - loss: 27.0867 - mse: 26.9489 - mae: 4.0386 - val_loss: 25.0913 - val_mse: 24.9530 - val_mae: 3.9868
Epoch 19/20
4855/4855 [==============================] - 12s 3ms/step - loss: 27.0350 - mse: 26.8953 - mae: 4.0381 - val_loss: 25.8671 - val_mse: 25.7268 - val_mae: 4.0502
Epoch 20/20
4855/4855 [==============================] - 12s 3ms/step - loss: 27.1036 - mse: 26.9620 - mae: 4.0386 - val_loss: 25.3004 - val_mse: 25.1585 - val_mae: 4.0032
bias 0.0050788308
si 0.47098517
rmse 0.05015821
kgeprime [0.76380997]
rmse_95 0.07891216
rmse_99 0.088689335
pearson 0.8792802530418489
pearson_95 0.4683351033483487
pearson_99 0.2418570832210684
rscore 0.7493355952241654
rscore_95 -5.293507559250654
rscore_99 -7.1892502346570435
nse [0.7493356]
nse_95 [-5.29350756]
nse_99 [-7.18925023]
kge [0.67250062]
ext_kge_95 [0.12891517]
ext_kge_99 [0.00406731]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([26644 26668 26692], shape=(3,), dtype=int64) Times out: tf.Tensor(26692, shape=(), dtype=int64)
Times in: tf.Tensor([31013 31037 31061], shape=(3,), dtype=int64) Times out: tf.Tensor(31061, shape=(), dtype=int64)
Times in: tf.Tensor([75105 75129 75153], shape=(3,), dtype=int64) Times out: tf.Tensor(75153, shape=(), dtype=int64)
Times in: tf.Tensor([37693 37717 37741], shape=(3,), dtype=int64) Times out: tf.Tensor(37741, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_260&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_261 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_520 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_521 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_260 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_520 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_260 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_521 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 13s 3ms/step - loss: 40.1922 - mse: 40.1410 - mae: 4.9475 - val_loss: 29.8701 - val_mse: 29.8117 - val_mae: 4.1653
Epoch 2/20
4856/4856 [==============================] - 12s 3ms/step - loss: 30.6949 - mse: 30.6313 - mae: 4.3697 - val_loss: 29.0176 - val_mse: 28.9464 - val_mae: 4.0836
Epoch 3/20
4856/4856 [==============================] - 12s 3ms/step - loss: 29.4304 - mse: 29.3543 - mae: 4.2783 - val_loss: 28.0323 - val_mse: 27.9511 - val_mae: 3.9968
Epoch 4/20
4856/4856 [==============================] - 13s 3ms/step - loss: 28.8598 - mse: 28.7764 - mae: 4.2346 - val_loss: 27.9465 - val_mse: 27.8598 - val_mae: 3.9827
Epoch 5/20
4856/4856 [==============================] - 12s 2ms/step - loss: 28.5858 - mse: 28.4977 - mae: 4.2142 - val_loss: 27.5012 - val_mse: 27.4106 - val_mae: 3.9576
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 28.4269 - mse: 28.3355 - mae: 4.2030 - val_loss: 27.4508 - val_mse: 27.3575 - val_mae: 3.9520
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 28.1760 - mse: 28.0815 - mae: 4.1848 - val_loss: 26.9735 - val_mse: 26.8771 - val_mae: 3.9067
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 27.7370 - mse: 27.6395 - mae: 4.1520 - val_loss: 26.4251 - val_mse: 26.3254 - val_mae: 3.8635
Epoch 9/20
4856/4856 [==============================] - 8s 2ms/step - loss: 27.4191 - mse: 27.3178 - mae: 4.1266 - val_loss: 26.0592 - val_mse: 25.9560 - val_mae: 3.8262
Epoch 10/20
4856/4856 [==============================] - 8s 2ms/step - loss: 27.1282 - mse: 27.0238 - mae: 4.1003 - val_loss: 25.7405 - val_mse: 25.6344 - val_mae: 3.8048
Epoch 11/20
4856/4856 [==============================] - 8s 2ms/step - loss: 27.1027 - mse: 26.9955 - mae: 4.0945 - val_loss: 25.3431 - val_mse: 25.2344 - val_mae: 3.7701
Epoch 12/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.8639 - mse: 26.7543 - mae: 4.0798 - val_loss: 25.1256 - val_mse: 25.0149 - val_mae: 3.7551
Epoch 13/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.7364 - mse: 26.6247 - mae: 4.0730 - val_loss: 25.1694 - val_mse: 25.0564 - val_mae: 3.7615
Epoch 14/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.6172 - mse: 26.5035 - mae: 4.0585 - val_loss: 24.9717 - val_mse: 24.8570 - val_mae: 3.7488
Epoch 15/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.6136 - mse: 26.4981 - mae: 4.0559 - val_loss: 24.6719 - val_mse: 24.5556 - val_mae: 3.7217
Epoch 16/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.4818 - mse: 26.3647 - mae: 4.0480 - val_loss: 24.8485 - val_mse: 24.7307 - val_mae: 3.7404
Epoch 17/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.3285 - mse: 26.2100 - mae: 4.0321 - val_loss: 24.6719 - val_mse: 24.5527 - val_mae: 3.7242
Epoch 18/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.2641 - mse: 26.1441 - mae: 4.0311 - val_loss: 24.7174 - val_mse: 24.5968 - val_mae: 3.7310
Epoch 19/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.2138 - mse: 26.0927 - mae: 4.0247 - val_loss: 25.1041 - val_mse: 24.9826 - val_mae: 3.7635
Epoch 20/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.2469 - mse: 26.1246 - mae: 4.0274 - val_loss: 24.6259 - val_mse: 24.5029 - val_mae: 3.7265
bias -0.0032373848
si 0.4759828
rmse 0.049500443
kgeprime [0.68522395]
rmse_95 0.10049339
rmse_99 0.19170758
pearson 0.8658565421895805
pearson_95 0.016841904297019428
pearson_99 -0.5833668905948702
rscore 0.7442576256834283
rscore_95 -1.9370604035379353
rscore_99 -6.054257456114536
nse [0.74425763]
nse_95 [-1.9370604]
nse_99 [-6.05425746]
kge [0.74139781]
ext_kge_95 [-0.02140406]
ext_kge_99 [-0.63728886]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([17236 17260 17284], shape=(3,), dtype=int64) Times out: tf.Tensor(17284, shape=(), dtype=int64)
Times in: tf.Tensor([9725 9749 9773], shape=(3,), dtype=int64) Times out: tf.Tensor(9773, shape=(), dtype=int64)
Times in: tf.Tensor([34053 34077 34101], shape=(3,), dtype=int64) Times out: tf.Tensor(34101, shape=(), dtype=int64)
Times in: tf.Tensor([12532 12556 12580], shape=(3,), dtype=int64) Times out: tf.Tensor(12580, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_261&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_262 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_522 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_523 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_261 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_522 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_261 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_523 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 39.1026 - mse: 39.0465 - mae: 4.8681 - val_loss: 29.3744 - val_mse: 29.3069 - val_mae: 4.1988
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 30.4443 - mse: 30.3713 - mae: 4.3436 - val_loss: 28.4995 - val_mse: 28.4205 - val_mae: 4.1277
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 29.2022 - mse: 29.1190 - mae: 4.2504 - val_loss: 28.0639 - val_mse: 27.9760 - val_mae: 4.0907
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.6828 - mse: 28.5915 - mae: 4.2059 - val_loss: 27.7721 - val_mse: 27.6776 - val_mae: 4.0668
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.1661 - mse: 28.0689 - mae: 4.1676 - val_loss: 27.2767 - val_mse: 27.1771 - val_mae: 4.0262
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 28.0407 - mse: 27.9391 - mae: 4.1513 - val_loss: 27.1407 - val_mse: 27.0369 - val_mae: 4.0233
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 27.5645 - mse: 27.4585 - mae: 4.1195 - val_loss: 26.2808 - val_mse: 26.1728 - val_mae: 3.9493
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 27.1669 - mse: 27.0567 - mae: 4.0843 - val_loss: 26.6927 - val_mse: 26.5806 - val_mae: 3.9958
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.8966 - mse: 26.7825 - mae: 4.0641 - val_loss: 25.8309 - val_mse: 25.7150 - val_mae: 3.9248
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.6769 - mse: 26.5593 - mae: 4.0431 - val_loss: 25.4034 - val_mse: 25.2847 - val_mae: 3.8866
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.5156 - mse: 26.3952 - mae: 4.0319 - val_loss: 25.1129 - val_mse: 24.9911 - val_mae: 3.8645
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.3637 - mse: 26.2401 - mae: 4.0225 - val_loss: 24.9946 - val_mse: 24.8696 - val_mae: 3.8642
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.1789 - mse: 26.0524 - mae: 4.0087 - val_loss: 25.0821 - val_mse: 24.9544 - val_mae: 3.8590
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.0285 - mse: 25.8992 - mae: 3.9913 - val_loss: 24.7882 - val_mse: 24.6576 - val_mae: 3.8407
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.9474 - mse: 25.8155 - mae: 3.9880 - val_loss: 25.1040 - val_mse: 24.9712 - val_mae: 3.8694
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.8239 - mse: 25.6896 - mae: 3.9755 - val_loss: 24.8393 - val_mse: 24.7041 - val_mae: 3.8477
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.8154 - mse: 25.6789 - mae: 3.9760 - val_loss: 25.2977 - val_mse: 25.1604 - val_mae: 3.8851
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.6427 - mse: 25.5042 - mae: 3.9595 - val_loss: 25.0430 - val_mse: 24.9036 - val_mae: 3.8651
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.7326 - mse: 25.5920 - mae: 3.9677 - val_loss: 24.6184 - val_mse: 24.4771 - val_mae: 3.8235
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.5553 - mse: 25.4127 - mae: 3.9514 - val_loss: 24.6056 - val_mse: 24.4622 - val_mae: 3.8232
bias 0.0005429639
si 0.49063188
rmse 0.049459245
kgeprime [0.78709306]
rmse_95 0.07952307
rmse_99 0.09218851
pearson 0.8553287510638022
pearson_95 0.512058530216252
pearson_99 0.2503461500497808
rscore 0.7309460011599551
rscore_95 -2.416617056046594
rscore_99 -3.4559425342943744
nse [0.730946]
nse_95 [-2.41661706]
nse_99 [-3.45594253]
kge [0.7766535]
ext_kge_95 [0.37597134]
ext_kge_99 [0.21007828]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([110641 110665 110689], shape=(3,), dtype=int64) Times out: tf.Tensor(110689, shape=(), dtype=int64)
Times in: tf.Tensor([143113 143137 143161], shape=(3,), dtype=int64) Times out: tf.Tensor(143161, shape=(), dtype=int64)
Times in: tf.Tensor([45614 45638 45662], shape=(3,), dtype=int64) Times out: tf.Tensor(45662, shape=(), dtype=int64)
Times in: tf.Tensor([148123 148147 148171], shape=(3,), dtype=int64) Times out: tf.Tensor(148171, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_262&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_263 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_524 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_525 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_262 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_524 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_262 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_525 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 43.6107 - mse: 43.5621 - mae: 5.0912 - val_loss: 27.0491 - val_mse: 26.9858 - val_mae: 4.1686
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.7315 - mse: 31.6597 - mae: 4.3903 - val_loss: 25.8073 - val_mse: 25.7280 - val_mae: 4.0667
Epoch 3/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.7904 - mse: 30.7058 - mae: 4.3268 - val_loss: 25.0823 - val_mse: 24.9935 - val_mae: 4.0019
Epoch 4/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.4594 - mse: 30.3672 - mae: 4.2969 - val_loss: 24.5358 - val_mse: 24.4405 - val_mae: 3.9440
Epoch 5/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.1241 - mse: 30.0261 - mae: 4.2737 - val_loss: 24.2450 - val_mse: 24.1443 - val_mae: 3.9153
Epoch 6/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.8800 - mse: 29.7769 - mae: 4.2552 - val_loss: 24.2609 - val_mse: 24.1551 - val_mae: 3.9125
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 7/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.6479 - mse: 29.5395 - mae: 4.2407 - val_loss: 23.9994 - val_mse: 23.8882 - val_mae: 3.8964
Epoch 8/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.5541 - mse: 29.4404 - mae: 4.2296 - val_loss: 23.7375 - val_mse: 23.6215 - val_mae: 3.8687
Epoch 9/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.3422 - mse: 29.2236 - mae: 4.2139 - val_loss: 23.5956 - val_mse: 23.4745 - val_mae: 3.8594
Epoch 10/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.1464 - mse: 29.0229 - mae: 4.1974 - val_loss: 23.2769 - val_mse: 23.1510 - val_mae: 3.8289
Epoch 11/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.0458 - mse: 28.9180 - mae: 4.1900 - val_loss: 23.0078 - val_mse: 22.8778 - val_mae: 3.8034
Epoch 12/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.8768 - mse: 28.7452 - mae: 4.1744 - val_loss: 22.9484 - val_mse: 22.8149 - val_mae: 3.8013
Epoch 13/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.7735 - mse: 28.6387 - mae: 4.1704 - val_loss: 22.9532 - val_mse: 22.8167 - val_mae: 3.8032
Epoch 14/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.6161 - mse: 28.4783 - mae: 4.1564 - val_loss: 22.6434 - val_mse: 22.5041 - val_mae: 3.7751
Epoch 15/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.5140 - mse: 28.3737 - mae: 4.1459 - val_loss: 22.3591 - val_mse: 22.2176 - val_mae: 3.7516
Epoch 16/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.5629 - mse: 28.4204 - mae: 4.1471 - val_loss: 22.5020 - val_mse: 22.3586 - val_mae: 3.7652
Epoch 17/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.4183 - mse: 28.2740 - mae: 4.1380 - val_loss: 22.3025 - val_mse: 22.1572 - val_mae: 3.7523
Epoch 18/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.3262 - mse: 28.1800 - mae: 4.1309 - val_loss: 22.0550 - val_mse: 21.9077 - val_mae: 3.7212
Epoch 19/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.2056 - mse: 28.0578 - mae: 4.1208 - val_loss: 21.8883 - val_mse: 21.7395 - val_mae: 3.7105
Epoch 20/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.1964 - mse: 28.0469 - mae: 4.1252 - val_loss: 21.7437 - val_mse: 21.5934 - val_mae: 3.6968
bias 0.00042006798
si 0.46405146
rmse 0.04646873
kgeprime [0.78265295]
rmse_95 0.07014092
rmse_99 0.068211496
pearson 0.8706505732483492
pearson_95 0.5529906743930073
pearson_99 0.4706341031637561
rscore 0.755002740407845
rscore_95 -5.749610850370261
rscore_99 -15.499001821231115
nse [0.75500274]
nse_95 [-5.74961085]
nse_99 [-15.49900182]
kge [0.77458922]
ext_kge_95 [0.2006254]
ext_kge_99 [-0.28768195]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -49.18 -48.86 -48.55 ... -42.93 -42.62
  * longitude       (longitude) float32 167.5 167.8 168.1 ... 173.4 173.8 174.1
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -8.61 -8.42 ... 4.049
    vgrd10m         (time, latitude, longitude) float32 9.659 9.499 ... 2.632
    uw2             (time, latitude, longitude) float32 74.13 70.89 ... 16.39
    vw2             (time, latitude, longitude) float32 93.29 90.22 ... 6.928
    wind_magnitude  (time, latitude, longitude) float32 12.94 12.69 ... 4.829
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([12086 12110 12134], shape=(3,), dtype=int64) Times out: tf.Tensor(12134, shape=(), dtype=int64)
Times in: tf.Tensor([64666 64690 64714], shape=(3,), dtype=int64) Times out: tf.Tensor(64714, shape=(), dtype=int64)
Times in: tf.Tensor([98179 98203 98227], shape=(3,), dtype=int64) Times out: tf.Tensor(98227, shape=(), dtype=int64)
Times in: tf.Tensor([730 754 778], shape=(3,), dtype=int64) Times out: tf.Tensor(778, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_263&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_264 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_526 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_527 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_263 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_526 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_263 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_527 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 44.7477 - mse: 44.6890 - mae: 5.1905 - val_loss: 34.2130 - val_mse: 34.1416 - val_mae: 4.6599
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.5391 - mse: 35.4599 - mae: 4.6629 - val_loss: 32.6563 - val_mse: 32.5696 - val_mae: 4.5641
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.4852 - mse: 34.3932 - mae: 4.5980 - val_loss: 32.1550 - val_mse: 32.0585 - val_mae: 4.5349
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.0414 - mse: 33.9414 - mae: 4.5693 - val_loss: 32.3533 - val_mse: 32.2505 - val_mae: 4.5348
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.7507 - mse: 33.6454 - mae: 4.5491 - val_loss: 32.2344 - val_mse: 32.1270 - val_mae: 4.5235
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.4820 - mse: 33.3725 - mae: 4.5315 - val_loss: 31.7310 - val_mse: 31.6192 - val_mae: 4.4988
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.1956 - mse: 33.0817 - mae: 4.5120 - val_loss: 31.4617 - val_mse: 31.3452 - val_mae: 4.4780
Epoch 8/20
4857/4857 [==============================] - 8s 2ms/step - loss: 32.7503 - mse: 32.6312 - mae: 4.4789 - val_loss: 31.9601 - val_mse: 31.8383 - val_mae: 4.4952
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.5954 - mse: 32.4712 - mae: 4.4693 - val_loss: 32.2540 - val_mse: 32.1270 - val_mae: 4.5242
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.3134 - mse: 32.1842 - mae: 4.4481 - val_loss: 32.1151 - val_mse: 31.9834 - val_mae: 4.5092
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.1547 - mse: 32.0207 - mae: 4.4384 - val_loss: 31.5699 - val_mse: 31.4333 - val_mae: 4.4738
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.0495 - mse: 31.9105 - mae: 4.4259 - val_loss: 31.5005 - val_mse: 31.3589 - val_mae: 4.4644
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.7933 - mse: 31.6490 - mae: 4.4117 - val_loss: 31.4151 - val_mse: 31.2686 - val_mae: 4.4509
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4857/4857 [==============================] - 7s 2ms/step - loss: 31.7006 - mse: 31.5518 - mae: 4.4025 - val_loss: 30.9225 - val_mse: 30.7714 - val_mae: 4.4228
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.4975 - mse: 31.3445 - mae: 4.3862 - val_loss: 31.1101 - val_mse: 30.9548 - val_mae: 4.4374
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.3847 - mse: 31.2275 - mae: 4.3777 - val_loss: 31.1263 - val_mse: 30.9672 - val_mae: 4.4304
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.3022 - mse: 31.1413 - mae: 4.3686 - val_loss: 31.6503 - val_mse: 31.4875 - val_mae: 4.4723
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.2727 - mse: 31.1080 - mae: 4.3697 - val_loss: 30.9336 - val_mse: 30.7672 - val_mae: 4.4220
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.1332 - mse: 30.9652 - mae: 4.3599 - val_loss: 31.1871 - val_mse: 31.0173 - val_mae: 4.4365
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 31.0599 - mse: 30.8886 - mae: 4.3542 - val_loss: 30.2343 - val_mse: 30.0612 - val_mae: 4.3692
bias -0.006964212
si 0.5509279
rmse 0.054828085
kgeprime [0.58186595]
rmse_95 0.07335246
rmse_99 0.08897951
pearson 0.8144631455255442
pearson_95 0.5628750282028476
pearson_99 0.5527418360434687
rscore 0.6564496935545442
rscore_95 -2.8215876377751377
rscore_99 -13.900583523330043
nse [0.65644969]
nse_95 [-2.82158764]
nse_99 [-13.90058352]
kge [0.67547546]
ext_kge_95 [0.43943534]
ext_kge_99 [-0.04912104]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([81584 81608 81632], shape=(3,), dtype=int64) Times out: tf.Tensor(81632, shape=(), dtype=int64)
Times in: tf.Tensor([84205 84229 84253], shape=(3,), dtype=int64) Times out: tf.Tensor(84253, shape=(), dtype=int64)
Times in: tf.Tensor([63656 63680 63704], shape=(3,), dtype=int64) Times out: tf.Tensor(63704, shape=(), dtype=int64)
Times in: tf.Tensor([53090 53114 53138], shape=(3,), dtype=int64) Times out: tf.Tensor(53138, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_264&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_265 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_528 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_529 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_264 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_528 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_264 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_529 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 45.5611 - mse: 45.5064 - mae: 5.2392 - val_loss: 38.9548 - val_mse: 38.8889 - val_mae: 4.9890
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.2598 - mse: 36.1864 - mae: 4.7034 - val_loss: 38.3382 - val_mse: 38.2569 - val_mae: 4.9644
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.0955 - mse: 35.0073 - mae: 4.6228 - val_loss: 36.7109 - val_mse: 36.6150 - val_mae: 4.8501
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 34.2502 - mse: 34.1497 - mae: 4.5663 - val_loss: 35.8462 - val_mse: 35.7396 - val_mae: 4.7874
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 34.0691 - mse: 33.9588 - mae: 4.5536 - val_loss: 35.7520 - val_mse: 35.6367 - val_mae: 4.7782
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.5954 - mse: 33.4769 - mae: 4.5251 - val_loss: 34.7835 - val_mse: 34.6604 - val_mae: 4.7036
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.3961 - mse: 33.2704 - mae: 4.5114 - val_loss: 35.0848 - val_mse: 34.9552 - val_mae: 4.7244
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.0774 - mse: 32.9456 - mae: 4.4856 - val_loss: 33.6540 - val_mse: 33.5187 - val_mae: 4.6169
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 32.8327 - mse: 32.6952 - mae: 4.4737 - val_loss: 33.4398 - val_mse: 33.2994 - val_mae: 4.6036
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 32.7114 - mse: 32.5690 - mae: 4.4616 - val_loss: 34.6167 - val_mse: 34.4717 - val_mae: 4.6919
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 32.4910 - mse: 32.3436 - mae: 4.4489 - val_loss: 33.0100 - val_mse: 32.8598 - val_mae: 4.5739
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 32.4591 - mse: 32.3073 - mae: 4.4450 - val_loss: 33.4348 - val_mse: 33.2809 - val_mae: 4.6129
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 32.3175 - mse: 32.1616 - mae: 4.4337 - val_loss: 32.9416 - val_mse: 32.7838 - val_mae: 4.5740
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 32.1075 - mse: 31.9475 - mae: 4.4216 - val_loss: 33.1812 - val_mse: 33.0194 - val_mae: 4.5926
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 32.0486 - mse: 31.8847 - mae: 4.4152 - val_loss: 33.1166 - val_mse: 32.9509 - val_mae: 4.5902
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 31.9755 - mse: 31.8080 - mae: 4.4107 - val_loss: 32.3832 - val_mse: 32.2141 - val_mae: 4.5347
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 31.8956 - mse: 31.7248 - mae: 4.4029 - val_loss: 33.1572 - val_mse: 32.9849 - val_mae: 4.5870
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 31.7326 - mse: 31.5586 - mae: 4.3911 - val_loss: 32.7122 - val_mse: 32.5372 - val_mae: 4.5611
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 31.6944 - mse: 31.5177 - mae: 4.3908 - val_loss: 32.8312 - val_mse: 32.6531 - val_mae: 4.5691
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 31.5642 - mse: 31.3847 - mae: 4.3823 - val_loss: 32.3898 - val_mse: 32.2091 - val_mae: 4.5324
bias 0.010258371
si 0.5219155
rmse 0.056753006
kgeprime [0.62944702]
rmse_95 0.084529325
rmse_99 0.07330002
pearson 0.8442265015570178
pearson_95 0.5320445350711491
pearson_99 0.3947758318941032
rscore 0.6912985879547792
rscore_95 -7.982095117159666
rscore_99 -15.635680242657273
nse [0.69129859]
nse_95 [-7.98209512]
nse_99 [-15.63568024]
kge [0.55523578]
ext_kge_95 [-0.27621986]
ext_kge_99 [-1.11758448]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([57344 57368 57392], shape=(3,), dtype=int64) Times out: tf.Tensor(57392, shape=(), dtype=int64)
Times in: tf.Tensor([52517 52541 52565], shape=(3,), dtype=int64) Times out: tf.Tensor(52565, shape=(), dtype=int64)
Times in: tf.Tensor([22172 22196 22220], shape=(3,), dtype=int64) Times out: tf.Tensor(22220, shape=(), dtype=int64)
Times in: tf.Tensor([51860 51884 51908], shape=(3,), dtype=int64) Times out: tf.Tensor(51908, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_265&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_266 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_530 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_531 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_265 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_530 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_265 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_531 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 45.5736 - mse: 45.5296 - mae: 5.2932 - val_loss: 35.4918 - val_mse: 35.4380 - val_mae: 4.5152
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.5205 - mse: 36.4614 - mae: 4.7630 - val_loss: 33.8374 - val_mse: 33.7719 - val_mae: 4.4069
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.4324 - mse: 35.3633 - mae: 4.6952 - val_loss: 33.1592 - val_mse: 33.0863 - val_mae: 4.3657
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.8463 - mse: 34.7709 - mae: 4.6587 - val_loss: 33.0050 - val_mse: 32.9263 - val_mae: 4.3628
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.5667 - mse: 34.4859 - mae: 4.6414 - val_loss: 32.8018 - val_mse: 32.7179 - val_mae: 4.3487
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.2134 - mse: 34.1273 - mae: 4.6180 - val_loss: 32.5015 - val_mse: 32.4125 - val_mae: 4.3321
Epoch 7/20
4856/4856 [==============================] - 8s 2ms/step - loss: 33.9675 - mse: 33.8765 - mae: 4.5959 - val_loss: 32.3282 - val_mse: 32.2343 - val_mae: 4.3226
Epoch 8/20
4856/4856 [==============================] - 8s 2ms/step - loss: 33.7345 - mse: 33.6388 - mae: 4.5828 - val_loss: 32.2246 - val_mse: 32.1261 - val_mae: 4.3232
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.6488 - mse: 33.5488 - mae: 4.5806 - val_loss: 32.1403 - val_mse: 32.0378 - val_mae: 4.3156
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.4549 - mse: 33.3508 - mae: 4.5674 - val_loss: 31.9037 - val_mse: 31.7973 - val_mae: 4.3054
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.3508 - mse: 33.2428 - mae: 4.5562 - val_loss: 31.9564 - val_mse: 31.8458 - val_mae: 4.3021
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.1599 - mse: 33.0478 - mae: 4.5427 - val_loss: 31.7698 - val_mse: 31.6553 - val_mae: 4.2911
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.1529 - mse: 33.0369 - mae: 4.5387 - val_loss: 31.7506 - val_mse: 31.6319 - val_mae: 4.2974
Epoch 14/20
4856/4856 [==============================] - 8s 2ms/step - loss: 32.8165 - mse: 32.6960 - mae: 4.5215 - val_loss: 31.3750 - val_mse: 31.2520 - val_mae: 4.2683
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 32.7093 - mse: 32.5846 - mae: 4.5070 - val_loss: 31.3230 - val_mse: 31.1960 - val_mae: 4.2616
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 32.5522 - mse: 32.4234 - mae: 4.5022 - val_loss: 31.1956 - val_mse: 31.0643 - val_mae: 4.2520
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 32.4680 - mse: 32.3353 - mae: 4.4949 - val_loss: 31.1054 - val_mse: 30.9709 - val_mae: 4.2486
Epoch 18/20
4856/4856 [==============================] - 8s 2ms/step - loss: 32.4067 - mse: 32.2707 - mae: 4.4928 - val_loss: 31.0167 - val_mse: 30.8786 - val_mae: 4.2449
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 32.3166 - mse: 32.1771 - mae: 4.4800 - val_loss: 31.2007 - val_mse: 31.0593 - val_mae: 4.2528
Epoch 20/20
4856/4856 [==============================] - 8s 2ms/step - loss: 32.0830 - mse: 31.9400 - mae: 4.4672 - val_loss: 30.7920 - val_mse: 30.6470 - val_mae: 4.2289
bias 0.001006318
si 0.5255651
rmse 0.05535973
kgeprime [0.74051318]
rmse_95 0.11081184
rmse_99 0.19350706
pearson 0.8362989421618137
pearson_95 0.1399514149755018
pearson_99 -0.5037075180090925
rscore 0.6956500052630774
rscore_95 -2.7254637614635846
rscore_99 -6.563227792240433
nse [0.69565001]
nse_95 [-2.72546376]
nse_99 [-6.56322779]
kge [0.72071536]
ext_kge_95 [0.08045318]
ext_kge_99 [-0.56920534]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([7676 7700 7724], shape=(3,), dtype=int64) Times out: tf.Tensor(7724, shape=(), dtype=int64)
Times in: tf.Tensor([38258 38282 38306], shape=(3,), dtype=int64) Times out: tf.Tensor(38306, shape=(), dtype=int64)
Times in: tf.Tensor([25550 25574 25598], shape=(3,), dtype=int64) Times out: tf.Tensor(25598, shape=(), dtype=int64)
Times in: tf.Tensor([13109 13133 13157], shape=(3,), dtype=int64) Times out: tf.Tensor(13157, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_266&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_267 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_532 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_533 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_266 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_532 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_266 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_533 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 47.5574 - mse: 47.5062 - mae: 5.3597 - val_loss: 34.8676 - val_mse: 34.8063 - val_mae: 4.6485
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.6474 - mse: 37.5811 - mae: 4.8018 - val_loss: 33.7783 - val_mse: 33.7061 - val_mae: 4.5852
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.1504 - mse: 36.0739 - mae: 4.7132 - val_loss: 33.3319 - val_mse: 33.2519 - val_mae: 4.5461
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.5218 - mse: 35.4401 - mae: 4.6723 - val_loss: 33.1470 - val_mse: 33.0638 - val_mae: 4.5335
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.3629 - mse: 35.2789 - mae: 4.6629 - val_loss: 32.5571 - val_mse: 32.4720 - val_mae: 4.4861
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 35.2013 - mse: 35.1156 - mae: 4.6509 - val_loss: 32.5800 - val_mse: 32.4934 - val_mae: 4.4890
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 34.9656 - mse: 34.8784 - mae: 4.6366 - val_loss: 33.2770 - val_mse: 33.1887 - val_mae: 4.5437
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.7363 - mse: 34.6475 - mae: 4.6206 - val_loss: 32.2512 - val_mse: 32.1613 - val_mae: 4.4656
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.6038 - mse: 34.5129 - mae: 4.6100 - val_loss: 32.1372 - val_mse: 32.0450 - val_mae: 4.4588
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.4944 - mse: 34.4013 - mae: 4.6018 - val_loss: 32.1335 - val_mse: 32.0393 - val_mae: 4.4555
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.2337 - mse: 34.1382 - mae: 4.5857 - val_loss: 31.7793 - val_mse: 31.6826 - val_mae: 4.4308
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.0529 - mse: 33.9547 - mae: 4.5720 - val_loss: 31.3404 - val_mse: 31.2408 - val_mae: 4.3981
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 33.9561 - mse: 33.8555 - mae: 4.5641 - val_loss: 31.7851 - val_mse: 31.6832 - val_mae: 4.4304
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 33.9402 - mse: 33.8373 - mae: 4.5640 - val_loss: 31.1147 - val_mse: 31.0108 - val_mae: 4.3800
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 33.6584 - mse: 33.5534 - mae: 4.5475 - val_loss: 31.6662 - val_mse: 31.5603 - val_mae: 4.4221
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 33.6426 - mse: 33.5354 - mae: 4.5418 - val_loss: 31.2635 - val_mse: 31.1552 - val_mae: 4.3919
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.6657 - mse: 33.5563 - mae: 4.5488 - val_loss: 31.0018 - val_mse: 30.8914 - val_mae: 4.3728
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.2973 - mse: 33.1857 - mae: 4.5209 - val_loss: 30.8280 - val_mse: 30.7153 - val_mae: 4.3575
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 33.2263 - mse: 33.1122 - mae: 4.5136 - val_loss: 31.3981 - val_mse: 31.2830 - val_mae: 4.4036
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 33.2769 - mse: 33.1606 - mae: 4.5143 - val_loss: 31.1797 - val_mse: 31.0623 - val_mae: 4.3892
bias 0.007155329
si 0.5605591
rmse 0.05573357
kgeprime [0.68165592]
rmse_95 0.0800704
rmse_99 0.11243054
pearson 0.8080625530500527
pearson_95 0.4096553149253698
pearson_99 0.3720631291990565
rscore 0.6448475657303836
rscore_95 -3.7141468540410756
rscore_99 -19.140321895893496
nse [0.64484757]
nse_95 [-3.71414685]
nse_99 [-19.1403219]
kge [0.66814673]
ext_kge_95 [0.27986972]
ext_kge_99 [-0.14255942]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([93204 93228 93252], shape=(3,), dtype=int64) Times out: tf.Tensor(93252, shape=(), dtype=int64)
Times in: tf.Tensor([119974 119998 120022], shape=(3,), dtype=int64) Times out: tf.Tensor(120022, shape=(), dtype=int64)
Times in: tf.Tensor([145877 145901 145925], shape=(3,), dtype=int64) Times out: tf.Tensor(145925, shape=(), dtype=int64)
Times in: tf.Tensor([160186 160210 160234], shape=(3,), dtype=int64) Times out: tf.Tensor(160234, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_267&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_268 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_534 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_535 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_267 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_534 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_267 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_535 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 45.7454 - mse: 45.7017 - mae: 5.2539 - val_loss: 32.3321 - val_mse: 32.2776 - val_mae: 4.5035
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5869 - mse: 36.5237 - mae: 4.7427 - val_loss: 30.6061 - val_mse: 30.5354 - val_mae: 4.3862
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.4739 - mse: 35.3971 - mae: 4.6720 - val_loss: 29.8001 - val_mse: 29.7182 - val_mae: 4.3257
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.9287 - mse: 34.8426 - mae: 4.6346 - val_loss: 29.5432 - val_mse: 29.4534 - val_mae: 4.3045
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.6900 - mse: 34.5970 - mae: 4.6223 - val_loss: 29.7334 - val_mse: 29.6372 - val_mae: 4.3236
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.3423 - mse: 34.2432 - mae: 4.5977 - val_loss: 28.7353 - val_mse: 28.6328 - val_mae: 4.2395
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.2073 - mse: 34.1021 - mae: 4.5855 - val_loss: 29.0163 - val_mse: 28.9084 - val_mae: 4.2745
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.9227 - mse: 33.8122 - mae: 4.5655 - val_loss: 27.9743 - val_mse: 27.8610 - val_mae: 4.1893
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.6998 - mse: 33.5843 - mae: 4.5498 - val_loss: 27.6736 - val_mse: 27.5555 - val_mae: 4.1657
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.5005 - mse: 33.3802 - mae: 4.5382 - val_loss: 28.4531 - val_mse: 28.3303 - val_mae: 4.2374
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.2860 - mse: 33.1612 - mae: 4.5233 - val_loss: 27.3109 - val_mse: 27.1835 - val_mae: 4.1375
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.2748 - mse: 33.1454 - mae: 4.5222 - val_loss: 27.0781 - val_mse: 26.9463 - val_mae: 4.1171
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.0995 - mse: 32.9659 - mae: 4.5066 - val_loss: 27.2940 - val_mse: 27.1583 - val_mae: 4.1405
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 33.0156 - mse: 32.8780 - mae: 4.5036 - val_loss: 27.3783 - val_mse: 27.2388 - val_mae: 4.1500
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.8709 - mse: 32.7296 - mae: 4.4939 - val_loss: 27.1849 - val_mse: 27.0417 - val_mae: 4.1353
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.7587 - mse: 32.6139 - mae: 4.4824 - val_loss: 27.1832 - val_mse: 27.0366 - val_mae: 4.1357
Epoch 17/20
4857/4857 [==============================] - 8s 2ms/step - loss: 32.5876 - mse: 32.4394 - mae: 4.4756 - val_loss: 27.3000 - val_mse: 27.1499 - val_mae: 4.1486
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.5106 - mse: 32.3587 - mae: 4.4721 - val_loss: 26.8949 - val_mse: 26.7411 - val_mae: 4.1111
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.6122 - mse: 32.4568 - mae: 4.4753 - val_loss: 26.9605 - val_mse: 26.8037 - val_mae: 4.1173
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 32.4885 - mse: 32.3301 - mae: 4.4675 - val_loss: 27.1783 - val_mse: 27.0182 - val_mae: 4.1345
bias 0.0071206302
si 0.50869423
rmse 0.051979076
kgeprime [0.72904296]
rmse_95 0.08763353
rmse_99 0.1138489
pearson 0.8445952332644026
pearson_95 0.537086588309469
pearson_99 0.8065359350587725
rscore 0.7061907057235612
rscore_95 -2.852406855238749
rscore_99 -5.620990133345912
nse [0.70619071]
nse_95 [-2.85240686]
nse_99 [-5.62099013]
kge [0.66708037]
ext_kge_95 [0.43614371]
ext_kge_99 [0.5844192]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -47.61 -47.3 -46.99 ... -41.37 -41.06
  * longitude       (longitude) float32 167.8 168.1 168.4 ... 174.1 174.4 174.7
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -7.45 -7.01 ... -2.5
    vgrd10m         (time, latitude, longitude) float32 9.308 8.61 ... 6.952
    uw2             (time, latitude, longitude) float32 55.5 49.13 ... 6.248
    vw2             (time, latitude, longitude) float32 86.65 74.13 ... 48.33
    wind_magnitude  (time, latitude, longitude) float32 11.92 11.1 ... 7.388
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([81445 81469 81493], shape=(3,), dtype=int64) Times out: tf.Tensor(81493, shape=(), dtype=int64)
Times in: tf.Tensor([65429 65453 65477], shape=(3,), dtype=int64) Times out: tf.Tensor(65477, shape=(), dtype=int64)
Times in: tf.Tensor([123244 123268 123292], shape=(3,), dtype=int64) Times out: tf.Tensor(123292, shape=(), dtype=int64)
Times in: tf.Tensor([25273 25297 25321], shape=(3,), dtype=int64) Times out: tf.Tensor(25321, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_268&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_269 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_536 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_537 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_268 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_536 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_268 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_537 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 50.5619 - mse: 50.5097 - mae: 5.5321 - val_loss: 42.5510 - val_mse: 42.4888 - val_mae: 5.1245
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 41.9533 - mse: 41.8851 - mae: 5.0584 - val_loss: 39.6818 - val_mse: 39.6078 - val_mae: 4.9529
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 40.0337 - mse: 39.9538 - mae: 4.9475 - val_loss: 38.6216 - val_mse: 38.5364 - val_mae: 4.8780
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 39.1810 - mse: 39.0915 - mae: 4.8960 - val_loss: 38.3229 - val_mse: 38.2294 - val_mae: 4.8590
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.5924 - mse: 38.4954 - mae: 4.8600 - val_loss: 37.9758 - val_mse: 37.8753 - val_mae: 4.8423
Epoch 6/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.1492 - mse: 38.0458 - mae: 4.8284 - val_loss: 37.7432 - val_mse: 37.6364 - val_mae: 4.8238
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.6205 - mse: 37.5107 - mae: 4.7968 - val_loss: 37.4683 - val_mse: 37.3559 - val_mae: 4.8084
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.5179 - mse: 37.4027 - mae: 4.7886 - val_loss: 37.3998 - val_mse: 37.2825 - val_mae: 4.8054
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.1533 - mse: 37.0334 - mae: 4.7663 - val_loss: 37.1343 - val_mse: 37.0120 - val_mae: 4.7875
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.7518 - mse: 36.6271 - mae: 4.7433 - val_loss: 36.8048 - val_mse: 36.6781 - val_mae: 4.7660
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5982 - mse: 36.4693 - mae: 4.7311 - val_loss: 36.1770 - val_mse: 36.0460 - val_mae: 4.7263
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.3615 - mse: 36.2284 - mae: 4.7163 - val_loss: 36.0097 - val_mse: 35.8747 - val_mae: 4.7113
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.1642 - mse: 36.0272 - mae: 4.7028 - val_loss: 35.8448 - val_mse: 35.7057 - val_mae: 4.6980
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.9617 - mse: 35.8207 - mae: 4.6876 - val_loss: 36.0932 - val_mse: 35.9502 - val_mae: 4.7176
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.7555 - mse: 35.6108 - mae: 4.6747 - val_loss: 36.2719 - val_mse: 36.1254 - val_mae: 4.7351
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.5746 - mse: 35.4261 - mae: 4.6606 - val_loss: 36.6863 - val_mse: 36.5361 - val_mae: 4.7642
Epoch 17/20
4857/4857 [==============================] - 8s 2ms/step - loss: 35.3112 - mse: 35.1591 - mae: 4.6442 - val_loss: 36.6635 - val_mse: 36.5097 - val_mae: 4.7667
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.2368 - mse: 35.0814 - mae: 4.6370 - val_loss: 36.1932 - val_mse: 36.0361 - val_mae: 4.7382
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.1026 - mse: 34.9438 - mae: 4.6355 - val_loss: 36.0403 - val_mse: 35.8796 - val_mae: 4.7265
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 34.9932 - mse: 34.8311 - mae: 4.6212 - val_loss: 35.2407 - val_mse: 35.0769 - val_mae: 4.6658
bias -0.007432017
si 0.56801856
rmse 0.059225738
kgeprime [0.54181093]
rmse_95 0.09577427
rmse_99 0.11518246
pearson 0.8011595928008021
pearson_95 0.5470940967406724
pearson_99 0.06263176946145896
rscore 0.6361064092543997
rscore_95 -4.127323011837414
rscore_99 -51.17792675852519
nse [0.63610641]
nse_95 [-4.12732301]
nse_99 [-51.17792676]
kge [0.6403288]
ext_kge_95 [0.32212671]
ext_kge_99 [-0.9486879]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([70796 70820 70844], shape=(3,), dtype=int64) Times out: tf.Tensor(70844, shape=(), dtype=int64)
Times in: tf.Tensor([37675 37699 37723], shape=(3,), dtype=int64) Times out: tf.Tensor(37723, shape=(), dtype=int64)
Times in: tf.Tensor([116192 116216 116240], shape=(3,), dtype=int64) Times out: tf.Tensor(116240, shape=(), dtype=int64)
Times in: tf.Tensor([111753 111777 111801], shape=(3,), dtype=int64) Times out: tf.Tensor(111801, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_269&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_270 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_538 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_539 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_269 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_538 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_269 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_539 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 53.9055 - mse: 53.8595 - mae: 5.6964 - val_loss: 45.4230 - val_mse: 45.3634 - val_mae: 5.4128
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 42.6162 - mse: 42.5496 - mae: 5.0718 - val_loss: 42.3756 - val_mse: 42.3015 - val_mae: 5.2479
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 40.7949 - mse: 40.7149 - mae: 4.9613 - val_loss: 40.4423 - val_mse: 40.3564 - val_mae: 5.1345
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.7314 - mse: 39.6416 - mae: 4.8941 - val_loss: 38.7031 - val_mse: 38.6094 - val_mae: 5.0232
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 39.3140 - mse: 39.2171 - mae: 4.8668 - val_loss: 38.6746 - val_mse: 38.5744 - val_mae: 5.0216
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.6749 - mse: 38.5722 - mae: 4.8332 - val_loss: 37.4643 - val_mse: 37.3588 - val_mae: 4.9405
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.3452 - mse: 38.2372 - mae: 4.8090 - val_loss: 37.9816 - val_mse: 37.8709 - val_mae: 4.9749
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.9934 - mse: 37.8803 - mae: 4.7882 - val_loss: 37.4395 - val_mse: 37.3239 - val_mae: 4.9412
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.6102 - mse: 37.4926 - mae: 4.7647 - val_loss: 37.1584 - val_mse: 37.0384 - val_mae: 4.9237
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.3584 - mse: 37.2363 - mae: 4.7475 - val_loss: 36.4976 - val_mse: 36.3734 - val_mae: 4.8755
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.2447 - mse: 37.1187 - mae: 4.7423 - val_loss: 36.7020 - val_mse: 36.5742 - val_mae: 4.8944
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.1059 - mse: 36.9763 - mae: 4.7296 - val_loss: 36.2670 - val_mse: 36.1358 - val_mae: 4.8636
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.8121 - mse: 36.6794 - mae: 4.7087 - val_loss: 35.9997 - val_mse: 35.8655 - val_mae: 4.8417
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.6022 - mse: 36.4663 - mae: 4.6995 - val_loss: 36.3264 - val_mse: 36.1888 - val_mae: 4.8685
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.4148 - mse: 36.2759 - mae: 4.6833 - val_loss: 36.1510 - val_mse: 36.0107 - val_mae: 4.8597
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.1680 - mse: 36.0264 - mae: 4.6699 - val_loss: 35.8146 - val_mse: 35.6716 - val_mae: 4.8360
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.1382 - mse: 35.9939 - mae: 4.6661 - val_loss: 35.4266 - val_mse: 35.2808 - val_mae: 4.8118
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.7886 - mse: 35.6416 - mae: 4.6453 - val_loss: 35.4112 - val_mse: 35.2628 - val_mae: 4.8042
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.8786 - mse: 35.7289 - mae: 4.6488 - val_loss: 35.7140 - val_mse: 35.5630 - val_mae: 4.8310
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.6346 - mse: 35.4823 - mae: 4.6327 - val_loss: 35.3251 - val_mse: 35.1713 - val_mae: 4.8065
bias 0.009797869
si 0.5269312
rmse 0.059305422
kgeprime [0.6605242]
rmse_95 0.09205877
rmse_99 0.08851701
pearson 0.8372294806377014
pearson_95 0.59491559617138
pearson_99 0.5101534242937207
rscore 0.6855937057726095
rscore_95 -5.96241099878028
rscore_99 -4.229011720470456
nse [0.68559371]
nse_95 [-5.962411]
nse_99 [-4.22901172]
kge [0.58628924]
ext_kge_95 [0.21020448]
ext_kge_99 [0.35392353]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([75979 76003 76027], shape=(3,), dtype=int64) Times out: tf.Tensor(76027, shape=(), dtype=int64)
Times in: tf.Tensor([71468 71492 71516], shape=(3,), dtype=int64) Times out: tf.Tensor(71516, shape=(), dtype=int64)
Times in: tf.Tensor([71622 71646 71670], shape=(3,), dtype=int64) Times out: tf.Tensor(71670, shape=(), dtype=int64)
Times in: tf.Tensor([73104 73128 73152], shape=(3,), dtype=int64) Times out: tf.Tensor(73152, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_270&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_271 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_540 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_541 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_270 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_540 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_270 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_541 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 51.6091 - mse: 51.5577 - mae: 5.6677 - val_loss: 42.6180 - val_mse: 42.5515 - val_mae: 4.9208
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 42.5695 - mse: 42.4973 - mae: 5.1428 - val_loss: 41.4327 - val_mse: 41.3531 - val_mae: 4.8466
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 40.9162 - mse: 40.8325 - mae: 5.0453 - val_loss: 40.3018 - val_mse: 40.2121 - val_mae: 4.7832
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 39.7826 - mse: 39.6892 - mae: 4.9726 - val_loss: 38.7325 - val_mse: 38.6334 - val_mae: 4.6954
Epoch 5/20
4856/4856 [==============================] - 8s 2ms/step - loss: 39.0919 - mse: 38.9894 - mae: 4.9281 - val_loss: 38.1034 - val_mse: 37.9952 - val_mae: 4.6577
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 38.4627 - mse: 38.3512 - mae: 4.8900 - val_loss: 37.8869 - val_mse: 37.7701 - val_mae: 4.6437
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.8925 - mse: 37.7728 - mae: 4.8535 - val_loss: 37.4398 - val_mse: 37.3155 - val_mae: 4.6173
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.4747 - mse: 37.3475 - mae: 4.8246 - val_loss: 37.2030 - val_mse: 37.0716 - val_mae: 4.6033
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.1026 - mse: 36.9687 - mae: 4.8008 - val_loss: 36.8361 - val_mse: 36.6982 - val_mae: 4.5811
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.0752 - mse: 36.9352 - mae: 4.7984 - val_loss: 36.7609 - val_mse: 36.6171 - val_mae: 4.5751
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.6940 - mse: 36.5482 - mae: 4.7752 - val_loss: 36.6310 - val_mse: 36.4821 - val_mae: 4.5759
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.5599 - mse: 36.4090 - mae: 4.7656 - val_loss: 36.3835 - val_mse: 36.2291 - val_mae: 4.5546
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.2496 - mse: 36.0932 - mae: 4.7481 - val_loss: 36.2174 - val_mse: 36.0575 - val_mae: 4.5444
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.1075 - mse: 35.9457 - mae: 4.7349 - val_loss: 36.3846 - val_mse: 36.2195 - val_mae: 4.5512
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.7611 - mse: 35.5942 - mae: 4.7156 - val_loss: 35.8800 - val_mse: 35.7100 - val_mae: 4.5201
Epoch 16/20
4856/4856 [==============================] - 8s 2ms/step - loss: 35.6459 - mse: 35.4740 - mae: 4.7036 - val_loss: 35.9912 - val_mse: 35.8162 - val_mae: 4.5270
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.4259 - mse: 35.2490 - mae: 4.6948 - val_loss: 35.7561 - val_mse: 35.5764 - val_mae: 4.5111
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.2829 - mse: 35.1014 - mae: 4.6851 - val_loss: 35.9632 - val_mse: 35.7786 - val_mae: 4.5250
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.0846 - mse: 34.8983 - mae: 4.6708 - val_loss: 35.6014 - val_mse: 35.4122 - val_mae: 4.5001
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.0478 - mse: 34.8570 - mae: 4.6652 - val_loss: 35.5142 - val_mse: 35.3209 - val_mae: 4.4975
bias -0.0055414382
si 0.55364954
rmse 0.05943138
kgeprime [0.59066317]
rmse_95 0.11454495
rmse_99 0.20298326
pearson 0.8138080453082018
pearson_95 0.06768087364364195
pearson_99 -0.5821364818155614
rscore 0.6588415317990947
rscore_95 -2.208610390726591
rscore_99 -6.287952298621554
nse [0.65884153]
nse_95 [-2.20861039]
nse_99 [-6.2879523]
kge [0.67343057]
ext_kge_95 [0.0260032]
ext_kge_99 [-0.63815474]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([4634 4658 4682], shape=(3,), dtype=int64) Times out: tf.Tensor(4682, shape=(), dtype=int64)
Times in: tf.Tensor([5967 5991 6015], shape=(3,), dtype=int64) Times out: tf.Tensor(6015, shape=(), dtype=int64)
Times in: tf.Tensor([23376 23400 23424], shape=(3,), dtype=int64) Times out: tf.Tensor(23424, shape=(), dtype=int64)
Times in: tf.Tensor([ 9969  9993 10017], shape=(3,), dtype=int64) Times out: tf.Tensor(10017, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_271&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_272 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_542 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_543 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_271 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_542 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_271 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_543 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 52.2680 - mse: 52.2167 - mae: 5.6308 - val_loss: 39.4566 - val_mse: 39.3839 - val_mae: 4.9515
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 41.7138 - mse: 41.6290 - mae: 5.0496 - val_loss: 38.3482 - val_mse: 38.2532 - val_mae: 4.8802
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.9072 - mse: 39.8040 - mae: 4.9422 - val_loss: 37.5231 - val_mse: 37.4133 - val_mae: 4.8156
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.0357 - mse: 38.9204 - mae: 4.8885 - val_loss: 37.2366 - val_mse: 37.1163 - val_mae: 4.8010
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.5677 - mse: 38.4426 - mae: 4.8599 - val_loss: 36.5292 - val_mse: 36.3992 - val_mae: 4.7496
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.9760 - mse: 37.8423 - mae: 4.8222 - val_loss: 36.5722 - val_mse: 36.4344 - val_mae: 4.7497
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.6088 - mse: 37.4675 - mae: 4.7957 - val_loss: 36.3502 - val_mse: 36.2054 - val_mae: 4.7408
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.3147 - mse: 37.1670 - mae: 4.7794 - val_loss: 35.9920 - val_mse: 35.8411 - val_mae: 4.7178
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.9013 - mse: 36.7475 - mae: 4.7529 - val_loss: 36.0142 - val_mse: 35.8577 - val_mae: 4.7204
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.6526 - mse: 36.4935 - mae: 4.7383 - val_loss: 35.4346 - val_mse: 35.2730 - val_mae: 4.6827
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.4183 - mse: 36.2541 - mae: 4.7222 - val_loss: 35.4177 - val_mse: 35.2512 - val_mae: 4.6861
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.1326 - mse: 35.9639 - mae: 4.7035 - val_loss: 35.1718 - val_mse: 35.0008 - val_mae: 4.6637
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.9790 - mse: 35.8061 - mae: 4.6920 - val_loss: 35.3629 - val_mse: 35.1878 - val_mae: 4.6777
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.7438 - mse: 35.5664 - mae: 4.6779 - val_loss: 35.2219 - val_mse: 35.0424 - val_mae: 4.6704
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.5095 - mse: 35.3280 - mae: 4.6557 - val_loss: 35.0005 - val_mse: 34.8172 - val_mae: 4.6551
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.2960 - mse: 35.1110 - mae: 4.6447 - val_loss: 35.1736 - val_mse: 34.9867 - val_mae: 4.6678
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.1098 - mse: 34.9209 - mae: 4.6380 - val_loss: 35.3726 - val_mse: 35.1820 - val_mae: 4.6808
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.9484 - mse: 34.7560 - mae: 4.6246 - val_loss: 34.4933 - val_mse: 34.2987 - val_mae: 4.6163
Epoch 19/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 34.7986 - mse: 34.6027 - mae: 4.6146 - val_loss: 34.7430 - val_mse: 34.5452 - val_mae: 4.6358
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.6119 - mse: 34.4125 - mae: 4.5994 - val_loss: 34.6115 - val_mse: 34.4103 - val_mae: 4.6260
bias 0.0018437766
si 0.5884927
rmse 0.058660235
kgeprime [0.77200718]
rmse_95 0.0773412
rmse_99 0.09220124
pearson 0.789291965225118
pearson_95 0.37957583693389163
pearson_99 0.5700278888618908
rscore 0.6142263939123523
rscore_95 -5.269613745203405
rscore_99 -23.160161594355507
nse [0.61422639]
nse_95 [-5.26961375]
nse_99 [-23.16016159]
kge [0.75103375]
ext_kge_95 [-0.06880165]
ext_kge_99 [-1.17335564]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([122403 122427 122451], shape=(3,), dtype=int64) Times out: tf.Tensor(122451, shape=(), dtype=int64)
Times in: tf.Tensor([85185 85209 85233], shape=(3,), dtype=int64) Times out: tf.Tensor(85233, shape=(), dtype=int64)
Times in: tf.Tensor([85602 85626 85650], shape=(3,), dtype=int64) Times out: tf.Tensor(85650, shape=(), dtype=int64)
Times in: tf.Tensor([56521 56545 56569], shape=(3,), dtype=int64) Times out: tf.Tensor(56569, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_272&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_273 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_544 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_545 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_272 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_544 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_272 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_545 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 51.9481 - mse: 51.8998 - mae: 5.6234 - val_loss: 37.3155 - val_mse: 37.2525 - val_mae: 4.7901
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 42.3901 - mse: 42.3179 - mae: 5.0941 - val_loss: 35.2214 - val_mse: 35.1409 - val_mae: 4.6661
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 40.7147 - mse: 40.6272 - mae: 4.9926 - val_loss: 34.0810 - val_mse: 33.9871 - val_mae: 4.5914
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 39.9411 - mse: 39.8418 - mae: 4.9467 - val_loss: 33.5699 - val_mse: 33.4657 - val_mae: 4.5601
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 39.1466 - mse: 39.0375 - mae: 4.8981 - val_loss: 33.0909 - val_mse: 32.9770 - val_mae: 4.5282
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.8671 - mse: 38.7485 - mae: 4.8777 - val_loss: 32.7630 - val_mse: 32.6397 - val_mae: 4.5054
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.4102 - mse: 38.2831 - mae: 4.8477 - val_loss: 32.4016 - val_mse: 32.2705 - val_mae: 4.4734
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.1592 - mse: 38.0248 - mae: 4.8319 - val_loss: 32.2996 - val_mse: 32.1619 - val_mae: 4.4716
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.8349 - mse: 37.6942 - mae: 4.8155 - val_loss: 31.8740 - val_mse: 31.7303 - val_mae: 4.4353
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.5050 - mse: 37.3585 - mae: 4.7946 - val_loss: 31.6715 - val_mse: 31.5224 - val_mae: 4.4233
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.3788 - mse: 37.2269 - mae: 4.7840 - val_loss: 31.5499 - val_mse: 31.3951 - val_mae: 4.4198
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.0618 - mse: 36.9049 - mae: 4.7656 - val_loss: 31.1468 - val_mse: 30.9876 - val_mae: 4.3867
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.0303 - mse: 36.8689 - mae: 4.7618 - val_loss: 30.9550 - val_mse: 30.7913 - val_mae: 4.3746
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.7641 - mse: 36.5984 - mae: 4.7442 - val_loss: 30.8361 - val_mse: 30.6682 - val_mae: 4.3668
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5066 - mse: 36.3368 - mae: 4.7247 - val_loss: 30.7640 - val_mse: 30.5920 - val_mae: 4.3636
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5283 - mse: 36.3545 - mae: 4.7282 - val_loss: 30.7835 - val_mse: 30.6077 - val_mae: 4.3680
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.1036 - mse: 35.9256 - mae: 4.7038 - val_loss: 30.5070 - val_mse: 30.3271 - val_mae: 4.3486
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.2265 - mse: 36.0446 - mae: 4.7120 - val_loss: 30.3420 - val_mse: 30.1582 - val_mae: 4.3324
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.9466 - mse: 35.7611 - mae: 4.6947 - val_loss: 30.2624 - val_mse: 30.0750 - val_mae: 4.3254
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.8502 - mse: 35.6611 - mae: 4.6809 - val_loss: 30.2370 - val_mse: 30.0458 - val_mae: 4.3311
bias 0.0028178783
si 0.53785634
rmse 0.05481409
kgeprime [0.76427846]
rmse_95 0.091557525
rmse_99 0.1259246
pearson 0.8228379013095736
pearson_95 0.6145804949369272
pearson_99 0.6586365515285378
rscore 0.6754775563065245
rscore_95 -1.655577024773006
rscore_99 -2.7599374965648558
nse [0.67547756]
nse_95 [-1.65557702]
nse_99 [-2.7599375]
kge [0.71667846]
ext_kge_95 [0.49994173]
ext_kge_99 [0.51270158]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -47.3 -46.99 -46.68 ... -40.75 -40.43
  * longitude       (longitude) float32 165.3 165.6 165.9 ... 171.2 171.6 171.9
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -7.529 -7.42 ... -5.433
    vgrd10m         (time, latitude, longitude) float32 9.009 9.17 ... -5.151
    uw2             (time, latitude, longitude) float32 56.68 55.05 ... 29.52
    vw2             (time, latitude, longitude) float32 81.17 84.08 ... 26.53
    wind_magnitude  (time, latitude, longitude) float32 11.74 11.8 ... 7.487
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([61393 61417 61441], shape=(3,), dtype=int64) Times out: tf.Tensor(61441, shape=(), dtype=int64)
Times in: tf.Tensor([133456 133480 133504], shape=(3,), dtype=int64) Times out: tf.Tensor(133504, shape=(), dtype=int64)
Times in: tf.Tensor([32382 32406 32430], shape=(3,), dtype=int64) Times out: tf.Tensor(32430, shape=(), dtype=int64)
Times in: tf.Tensor([18499 18523 18547], shape=(3,), dtype=int64) Times out: tf.Tensor(18547, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_273&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_274 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_546 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_547 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_273 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_546 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_273 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_547 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 30.6796 - mse: 30.6271 - mae: 4.2610 - val_loss: 19.0805 - val_mse: 19.0188 - val_mae: 3.4493
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.2628 - mse: 23.1957 - mae: 3.7627 - val_loss: 18.3244 - val_mse: 18.2517 - val_mae: 3.3818
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.5427 - mse: 22.4657 - mae: 3.6992 - val_loss: 17.8447 - val_mse: 17.7636 - val_mae: 3.3315
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2987 - mse: 22.2149 - mae: 3.6757 - val_loss: 17.7552 - val_mse: 17.6688 - val_mae: 3.3287
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0837 - mse: 21.9948 - mae: 3.6566 - val_loss: 18.0522 - val_mse: 17.9608 - val_mae: 3.3644
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.8831 - mse: 21.7894 - mae: 3.6361 - val_loss: 18.3552 - val_mse: 18.2590 - val_mae: 3.3979
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5953 - mse: 21.4969 - mae: 3.6116 - val_loss: 17.5478 - val_mse: 17.4469 - val_mae: 3.3118
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4081 - mse: 21.3048 - mae: 3.5960 - val_loss: 17.5376 - val_mse: 17.4320 - val_mae: 3.3156
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1943 - mse: 21.0872 - mae: 3.5775 - val_loss: 17.4238 - val_mse: 17.3149 - val_mae: 3.3034
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.0724 - mse: 20.9617 - mae: 3.5627 - val_loss: 17.2000 - val_mse: 17.0877 - val_mae: 3.2811
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8754 - mse: 20.7615 - mae: 3.5468 - val_loss: 17.3121 - val_mse: 17.1965 - val_mae: 3.2936
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7707 - mse: 20.6537 - mae: 3.5380 - val_loss: 17.1066 - val_mse: 16.9882 - val_mae: 3.2730
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7277 - mse: 20.6080 - mae: 3.5310 - val_loss: 17.2360 - val_mse: 17.1150 - val_mae: 3.2871
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5995 - mse: 20.4773 - mae: 3.5190 - val_loss: 17.1103 - val_mse: 16.9871 - val_mae: 3.2747
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5471 - mse: 20.4226 - mae: 3.5097 - val_loss: 17.0818 - val_mse: 16.9561 - val_mae: 3.2707
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3809 - mse: 20.2544 - mae: 3.4940 - val_loss: 17.0328 - val_mse: 16.9054 - val_mae: 3.2667
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4020 - mse: 20.2734 - mae: 3.4986 - val_loss: 17.5699 - val_mse: 17.4405 - val_mae: 3.3246
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3137 - mse: 20.1836 - mae: 3.4905 - val_loss: 16.8900 - val_mse: 16.7591 - val_mae: 3.2457
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3230 - mse: 20.1912 - mae: 3.4897 - val_loss: 16.9172 - val_mse: 16.7845 - val_mae: 3.2507
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.2621 - mse: 20.1286 - mae: 3.4812 - val_loss: 17.0321 - val_mse: 16.8980 - val_mae: 3.2654
bias -0.0053036767
si 0.465565
rmse 0.04110716
kgeprime [0.66370022]
rmse_95 0.05506122
rmse_99 0.066841185
pearson 0.8682866086594393
pearson_95 0.49313320775149044
pearson_99 0.04127367560248807
rscore 0.7497009483908087
rscore_95 -2.5801932201804556
rscore_99 -40.32095005236404
nse [0.74970095]
nse_95 [-2.58019322]
nse_99 [-40.32095005]
kge [0.74790851]
ext_kge_95 [0.28355477]
ext_kge_99 [-2.12944138]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([3531 3555 3579], shape=(3,), dtype=int64) Times out: tf.Tensor(3579, shape=(), dtype=int64)
Times in: tf.Tensor([77701 77725 77749], shape=(3,), dtype=int64) Times out: tf.Tensor(77749, shape=(), dtype=int64)
Times in: tf.Tensor([101748 101772 101796], shape=(3,), dtype=int64) Times out: tf.Tensor(101796, shape=(), dtype=int64)
Times in: tf.Tensor([76505 76529 76553], shape=(3,), dtype=int64) Times out: tf.Tensor(76553, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_274&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_275 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_548 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_549 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_274 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_548 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_274 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_549 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.4949 - mse: 35.4502 - mae: 4.6064 - val_loss: 24.7607 - val_mse: 24.7085 - val_mae: 3.9250
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.1488 - mse: 25.0916 - mae: 3.9020 - val_loss: 21.4394 - val_mse: 21.3780 - val_mae: 3.6764
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.5210 - mse: 23.4536 - mae: 3.7685 - val_loss: 21.1581 - val_mse: 21.0861 - val_mae: 3.6610
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.7621 - mse: 22.6846 - mae: 3.7055 - val_loss: 20.3990 - val_mse: 20.3181 - val_mae: 3.5802
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3934 - mse: 22.3084 - mae: 3.6753 - val_loss: 19.5827 - val_mse: 19.4957 - val_mae: 3.5053
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 22.1690 - mse: 22.0788 - mae: 3.6569 - val_loss: 19.5711 - val_mse: 19.4794 - val_mae: 3.5061
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0082 - mse: 21.9139 - mae: 3.6415 - val_loss: 19.2834 - val_mse: 19.1879 - val_mae: 3.4754
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8305 - mse: 21.7328 - mae: 3.6214 - val_loss: 19.4645 - val_mse: 19.3659 - val_mae: 3.4811
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.6228 - mse: 21.5219 - mae: 3.6054 - val_loss: 18.8848 - val_mse: 18.7831 - val_mae: 3.4316
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.4817 - mse: 21.3778 - mae: 3.5924 - val_loss: 18.9182 - val_mse: 18.8137 - val_mae: 3.4349
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3316 - mse: 21.2248 - mae: 3.5789 - val_loss: 18.6090 - val_mse: 18.5016 - val_mae: 3.4043
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.1854 - mse: 21.0760 - mae: 3.5653 - val_loss: 18.4874 - val_mse: 18.3776 - val_mae: 3.3927
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.0368 - mse: 20.9251 - mae: 3.5521 - val_loss: 18.3877 - val_mse: 18.2757 - val_mae: 3.3812
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.9630 - mse: 20.8491 - mae: 3.5413 - val_loss: 18.5971 - val_mse: 18.4829 - val_mae: 3.4029
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.7962 - mse: 20.6803 - mae: 3.5308 - val_loss: 18.5999 - val_mse: 18.4838 - val_mae: 3.4058
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.6879 - mse: 20.5703 - mae: 3.5186 - val_loss: 18.6636 - val_mse: 18.5459 - val_mae: 3.4093
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.5865 - mse: 20.4672 - mae: 3.5127 - val_loss: 18.5744 - val_mse: 18.4550 - val_mae: 3.4038
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.4789 - mse: 20.3580 - mae: 3.5019 - val_loss: 18.0878 - val_mse: 17.9670 - val_mae: 3.3487
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.4776 - mse: 20.3554 - mae: 3.4971 - val_loss: 18.3537 - val_mse: 18.2316 - val_mae: 3.3848
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.4671 - mse: 20.3437 - mae: 3.4991 - val_loss: 17.8265 - val_mse: 17.7030 - val_mae: 3.3340
bias 0.0016537647
si 0.43377513
rmse 0.042075
kgeprime [0.82872913]
rmse_95 0.06219839
rmse_99 0.07292264
pearson 0.8917370994274384
pearson_95 0.5435894134585343
pearson_99 0.20430614472820588
rscore 0.7911863987727048
rscore_95 -2.2004319292116814
rscore_99 -7.587662775596128
nse [0.7911864]
nse_95 [-2.20043193]
nse_99 [-7.58766278]
kge [0.79201103]
ext_kge_95 [0.3211573]
ext_kge_99 [-0.16363176]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([60621 60645 60669], shape=(3,), dtype=int64) Times out: tf.Tensor(60669, shape=(), dtype=int64)
Times in: tf.Tensor([9441 9465 9489], shape=(3,), dtype=int64) Times out: tf.Tensor(9489, shape=(), dtype=int64)
Times in: tf.Tensor([39391 39415 39439], shape=(3,), dtype=int64) Times out: tf.Tensor(39439, shape=(), dtype=int64)
Times in: tf.Tensor([6069 6093 6117], shape=(3,), dtype=int64) Times out: tf.Tensor(6117, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_275&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_276 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_550 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_551 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_275 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_550 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_275 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_551 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 31.5123 - mse: 31.4610 - mae: 4.3539 - val_loss: 23.0742 - val_mse: 23.0108 - val_mae: 3.6481
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.6790 - mse: 23.6096 - mae: 3.8233 - val_loss: 22.3468 - val_mse: 22.2711 - val_mae: 3.5925
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.6450 - mse: 22.5640 - mae: 3.7403 - val_loss: 21.9519 - val_mse: 21.8662 - val_mae: 3.5599
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.0517 - mse: 21.9628 - mae: 3.6862 - val_loss: 21.4313 - val_mse: 21.3387 - val_mae: 3.5130
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.7149 - mse: 21.6205 - mae: 3.6625 - val_loss: 21.3284 - val_mse: 21.2316 - val_mae: 3.5010
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.5950 - mse: 21.4968 - mae: 3.6489 - val_loss: 21.1892 - val_mse: 21.0893 - val_mae: 3.4820
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.3905 - mse: 21.2892 - mae: 3.6287 - val_loss: 20.7588 - val_mse: 20.6557 - val_mae: 3.4421
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.1778 - mse: 21.0738 - mae: 3.6125 - val_loss: 20.4158 - val_mse: 20.3101 - val_mae: 3.4105
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.0378 - mse: 20.9315 - mae: 3.6000 - val_loss: 20.4116 - val_mse: 20.3042 - val_mae: 3.4076
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.8806 - mse: 20.7723 - mae: 3.5865 - val_loss: 20.0977 - val_mse: 19.9881 - val_mae: 3.3786
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.7758 - mse: 20.6655 - mae: 3.5725 - val_loss: 20.0501 - val_mse: 19.9386 - val_mae: 3.3728
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.6209 - mse: 20.5086 - mae: 3.5641 - val_loss: 19.7265 - val_mse: 19.6134 - val_mae: 3.3393
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.5138 - mse: 20.3999 - mae: 3.5560 - val_loss: 19.7784 - val_mse: 19.6635 - val_mae: 3.3427
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.4897 - mse: 20.3740 - mae: 3.5514 - val_loss: 19.7356 - val_mse: 19.6190 - val_mae: 3.3376
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.4569 - mse: 20.3395 - mae: 3.5464 - val_loss: 19.4917 - val_mse: 19.3734 - val_mae: 3.3133
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2505 - mse: 20.1314 - mae: 3.5301 - val_loss: 19.7024 - val_mse: 19.5826 - val_mae: 3.3332
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.3620 - mse: 20.2415 - mae: 3.5367 - val_loss: 19.5074 - val_mse: 19.3862 - val_mae: 3.3148
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2465 - mse: 20.1246 - mae: 3.5278 - val_loss: 19.3678 - val_mse: 19.2454 - val_mae: 3.3019
Epoch 19/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 20.2382 - mse: 20.1151 - mae: 3.5251 - val_loss: 19.6818 - val_mse: 19.5583 - val_mae: 3.3327
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2111 - mse: 20.0870 - mae: 3.5225 - val_loss: 19.8603 - val_mse: 19.7355 - val_mae: 3.3517
bias 0.0048688496
si 0.47695276
rmse 0.0444247
kgeprime [0.78016746]
rmse_95 0.09394951
rmse_99 0.16249572
pearson 0.8683857145661098
pearson_95 0.09921733635530838
pearson_99 -0.7670998288051133
rscore 0.7440894076625257
rscore_95 -2.9638015437145597
rscore_99 -10.309404968433002
nse [0.74408941]
nse_95 [-2.96380154]
nse_99 [-10.30940497]
kge [0.69822179]
ext_kge_95 [0.05228937]
ext_kge_99 [-0.86428467]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([30090 30114 30138], shape=(3,), dtype=int64) Times out: tf.Tensor(30138, shape=(), dtype=int64)
Times in: tf.Tensor([14583 14607 14631], shape=(3,), dtype=int64) Times out: tf.Tensor(14631, shape=(), dtype=int64)
Times in: tf.Tensor([35277 35301 35325], shape=(3,), dtype=int64) Times out: tf.Tensor(35325, shape=(), dtype=int64)
Times in: tf.Tensor([38068 38092 38116], shape=(3,), dtype=int64) Times out: tf.Tensor(38116, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_276&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_277 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_552 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_553 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_276 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_552 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_276 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_553 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.3341 - mse: 33.2815 - mae: 4.4327 - val_loss: 23.6343 - val_mse: 23.5676 - val_mae: 3.8100
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.7426 - mse: 22.6704 - mae: 3.7132 - val_loss: 22.8931 - val_mse: 22.8173 - val_mae: 3.7484
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.9288 - mse: 21.8501 - mae: 3.6462 - val_loss: 22.4350 - val_mse: 22.3539 - val_mae: 3.7046
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5942 - mse: 21.5109 - mae: 3.6186 - val_loss: 22.2527 - val_mse: 22.1669 - val_mae: 3.6934
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3257 - mse: 21.2375 - mae: 3.5912 - val_loss: 22.7563 - val_mse: 22.6654 - val_mae: 3.7564
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.1616 - mse: 21.0680 - mae: 3.5805 - val_loss: 21.8550 - val_mse: 21.7586 - val_mae: 3.6497
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.9198 - mse: 20.8211 - mae: 3.5574 - val_loss: 21.8416 - val_mse: 21.7402 - val_mae: 3.6539
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.8159 - mse: 20.7120 - mae: 3.5498 - val_loss: 21.9172 - val_mse: 21.8110 - val_mae: 3.6687
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.6509 - mse: 20.5422 - mae: 3.5350 - val_loss: 21.8721 - val_mse: 21.7611 - val_mae: 3.6620
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.5191 - mse: 20.4059 - mae: 3.5220 - val_loss: 21.6730 - val_mse: 21.5574 - val_mae: 3.6384
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.3493 - mse: 20.2318 - mae: 3.5067 - val_loss: 22.0321 - val_mse: 21.9125 - val_mae: 3.6834
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.2725 - mse: 20.1512 - mae: 3.4994 - val_loss: 21.3308 - val_mse: 21.2074 - val_mae: 3.5952
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.1208 - mse: 19.9958 - mae: 3.4842 - val_loss: 21.3131 - val_mse: 21.1863 - val_mae: 3.6035
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0739 - mse: 19.9454 - mae: 3.4745 - val_loss: 21.1993 - val_mse: 21.0689 - val_mae: 3.5862
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.9527 - mse: 19.8207 - mae: 3.4672 - val_loss: 21.7192 - val_mse: 21.5859 - val_mae: 3.6558
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.8553 - mse: 19.7207 - mae: 3.4598 - val_loss: 21.0561 - val_mse: 20.9201 - val_mae: 3.5839
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7907 - mse: 19.6537 - mae: 3.4532 - val_loss: 21.0114 - val_mse: 20.8734 - val_mae: 3.5804
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7196 - mse: 19.5801 - mae: 3.4497 - val_loss: 20.9296 - val_mse: 20.7891 - val_mae: 3.5709
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.6073 - mse: 19.4657 - mae: 3.4347 - val_loss: 21.5266 - val_mse: 21.3839 - val_mae: 3.6484
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.5168 - mse: 19.3731 - mae: 3.4265 - val_loss: 21.2475 - val_mse: 21.1029 - val_mae: 3.6117
bias 0.0063361465
si 0.50479585
rmse 0.045937896
kgeprime [0.67212384]
rmse_95 0.065860465
rmse_99 0.081815645
pearson 0.8498927262727892
pearson_95 0.5869022992077653
pearson_99 0.5444455413044726
rscore 0.7124856335157255
rscore_95 -1.8769313662117098
rscore_99 -10.737572578281211
nse [0.71248563]
nse_95 [-1.87693137]
nse_99 [-10.73757258]
kge [0.71576026]
ext_kge_95 [0.42698728]
ext_kge_99 [-0.06911092]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([79976 80000 80024], shape=(3,), dtype=int64) Times out: tf.Tensor(80024, shape=(), dtype=int64)
Times in: tf.Tensor([187854 187878 187902], shape=(3,), dtype=int64) Times out: tf.Tensor(187902, shape=(), dtype=int64)
Times in: tf.Tensor([144063 144087 144111], shape=(3,), dtype=int64) Times out: tf.Tensor(144111, shape=(), dtype=int64)
Times in: tf.Tensor([77068 77092 77116], shape=(3,), dtype=int64) Times out: tf.Tensor(77116, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_277&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_278 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_554 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_555 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_277 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_554 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_277 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_555 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 31.5014 - mse: 31.4457 - mae: 4.3068 - val_loss: 20.4478 - val_mse: 20.3802 - val_mae: 3.5978
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.9437 - mse: 23.8697 - mae: 3.7998 - val_loss: 19.6741 - val_mse: 19.5951 - val_mae: 3.5272
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.3528 - mse: 23.2705 - mae: 3.7514 - val_loss: 19.1193 - val_mse: 19.0346 - val_mae: 3.4703
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0640 - mse: 22.9773 - mae: 3.7298 - val_loss: 19.1121 - val_mse: 19.0234 - val_mae: 3.4701
Epoch 5/20
4857/4857 [==============================] - 8s 2ms/step - loss: 22.9578 - mse: 22.8676 - mae: 3.7192 - val_loss: 18.9197 - val_mse: 18.8279 - val_mae: 3.4509
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7435 - mse: 22.6500 - mae: 3.7024 - val_loss: 18.9305 - val_mse: 18.8357 - val_mae: 3.4526
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6139 - mse: 22.5173 - mae: 3.6945 - val_loss: 18.6301 - val_mse: 18.5318 - val_mae: 3.4200
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4670 - mse: 22.3670 - mae: 3.6761 - val_loss: 18.7210 - val_mse: 18.6192 - val_mae: 3.4318
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4207 - mse: 22.3174 - mae: 3.6718 - val_loss: 18.5939 - val_mse: 18.4888 - val_mae: 3.4225
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2724 - mse: 22.1655 - mae: 3.6538 - val_loss: 18.3853 - val_mse: 18.2765 - val_mae: 3.4046
Epoch 11/20
4857/4857 [==============================] - 8s 2ms/step - loss: 22.1510 - mse: 22.0405 - mae: 3.6460 - val_loss: 18.3295 - val_mse: 18.2172 - val_mae: 3.4017
Epoch 12/20
4857/4857 [==============================] - 8s 2ms/step - loss: 21.9149 - mse: 21.8008 - mae: 3.6241 - val_loss: 18.1042 - val_mse: 17.9885 - val_mae: 3.3808
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9035 - mse: 21.7861 - mae: 3.6255 - val_loss: 18.1631 - val_mse: 18.0443 - val_mae: 3.3901
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.8473 - mse: 21.7271 - mae: 3.6156 - val_loss: 17.7692 - val_mse: 17.6475 - val_mae: 3.3516
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.6883 - mse: 21.5654 - mae: 3.5997 - val_loss: 17.4785 - val_mse: 17.3543 - val_mae: 3.3214
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5636 - mse: 21.4384 - mae: 3.5917 - val_loss: 17.4693 - val_mse: 17.3429 - val_mae: 3.3210
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5435 - mse: 21.4161 - mae: 3.5909 - val_loss: 17.4432 - val_mse: 17.3149 - val_mae: 3.3198
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4992 - mse: 21.3700 - mae: 3.5840 - val_loss: 17.2441 - val_mse: 17.1139 - val_mae: 3.2989
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.3740 - mse: 21.2431 - mae: 3.5733 - val_loss: 17.1700 - val_mse: 17.0383 - val_mae: 3.2885
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2653 - mse: 21.1330 - mae: 3.5659 - val_loss: 17.5856 - val_mse: 17.4530 - val_mae: 3.3375
bias 0.008008819
si 0.45069772
rmse 0.04177676
kgeprime [0.6840257]
rmse_95 0.062559076
rmse_99 0.06988288
pearson 0.8793487044662892
pearson_95 0.43506982995695226
pearson_99 0.7187582500021658
rscore 0.7623349064766947
rscore_95 -6.118432751574448
rscore_99 -15.0966241790629
nse [0.76233491]
nse_95 [-6.11843275]
nse_99 [-15.09662418]
kge [0.66632516]
ext_kge_95 [0.13973488]
ext_kge_99 [0.08390455]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -46.99 -46.68 -46.37 ... -40.43 -40.12
  * longitude       (longitude) float32 169.4 169.7 170.0 ... 175.6 175.9 176.2
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -2.549 ... 0.03632
    vgrd10m         (time, latitude, longitude) float32 1.649 1.478 ... -0.3653
    uw2             (time, latitude, longitude) float32 6.496 2.921 ... 0.001319
    vw2             (time, latitude, longitude) float32 2.72 2.186 ... 0.1335
    wind_magnitude  (time, latitude, longitude) float32 3.036 2.26 ... 0.3671
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([130425 130449 130473], shape=(3,), dtype=int64) Times out: tf.Tensor(130473, shape=(), dtype=int64)
Times in: tf.Tensor([23382 23406 23430], shape=(3,), dtype=int64) Times out: tf.Tensor(23430, shape=(), dtype=int64)
Times in: tf.Tensor([151863 151887 151911], shape=(3,), dtype=int64) Times out: tf.Tensor(151911, shape=(), dtype=int64)
Times in: tf.Tensor([21184 21208 21232], shape=(3,), dtype=int64) Times out: tf.Tensor(21232, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_278&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_279 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_556 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_557 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_278 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_556 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_278 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_557 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 51.1620 - mse: 51.1060 - mae: 5.5704 - val_loss: 41.6104 - val_mse: 41.5360 - val_mae: 5.0362
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 40.7049 - mse: 40.6198 - mae: 4.9901 - val_loss: 38.0461 - val_mse: 37.9509 - val_mae: 4.8368
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.9234 - mse: 38.8202 - mae: 4.8765 - val_loss: 36.7760 - val_mse: 36.6651 - val_mae: 4.7571
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.1053 - mse: 37.9892 - mae: 4.8227 - val_loss: 36.7121 - val_mse: 36.5907 - val_mae: 4.7514
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.7369 - mse: 37.6118 - mae: 4.8019 - val_loss: 36.2051 - val_mse: 36.0761 - val_mae: 4.7255
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.2840 - mse: 37.1524 - mae: 4.7717 - val_loss: 36.0060 - val_mse: 35.8713 - val_mae: 4.7138
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.2320 - mse: 37.0950 - mae: 4.7631 - val_loss: 35.9197 - val_mse: 35.7799 - val_mae: 4.7109
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.8989 - mse: 36.7570 - mae: 4.7433 - val_loss: 35.6837 - val_mse: 35.5391 - val_mae: 4.6988
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.8680 - mse: 36.7212 - mae: 4.7365 - val_loss: 35.7834 - val_mse: 35.6340 - val_mae: 4.7028
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5317 - mse: 36.3803 - mae: 4.7155 - val_loss: 35.8018 - val_mse: 35.6482 - val_mae: 4.7083
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.4334 - mse: 36.2779 - mae: 4.7083 - val_loss: 35.3440 - val_mse: 35.1862 - val_mae: 4.6821
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.3059 - mse: 36.1461 - mae: 4.6994 - val_loss: 35.6710 - val_mse: 35.5091 - val_mae: 4.7010
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.1426 - mse: 35.9792 - mae: 4.6887 - val_loss: 35.4050 - val_mse: 35.2395 - val_mae: 4.6864
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.9281 - mse: 35.7608 - mae: 4.6728 - val_loss: 35.3649 - val_mse: 35.1954 - val_mae: 4.6799
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.8652 - mse: 35.6941 - mae: 4.6689 - val_loss: 35.3165 - val_mse: 35.1432 - val_mae: 4.6788
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.8965 - mse: 35.7218 - mae: 4.6687 - val_loss: 34.8974 - val_mse: 34.7208 - val_mae: 4.6553
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.5852 - mse: 35.4069 - mae: 4.6501 - val_loss: 36.0826 - val_mse: 35.9023 - val_mae: 4.7301
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.4589 - mse: 35.2773 - mae: 4.6412 - val_loss: 35.4608 - val_mse: 35.2772 - val_mae: 4.6909
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.3825 - mse: 35.1974 - mae: 4.6320 - val_loss: 34.9610 - val_mse: 34.7739 - val_mae: 4.6548
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.1791 - mse: 34.9905 - mae: 4.6216 - val_loss: 35.3119 - val_mse: 35.1214 - val_mae: 4.6797
bias -0.009247181
si 0.584208
rmse 0.059263315
kgeprime [0.46733591]
rmse_95 0.092109874
rmse_99 0.1156516
pearson 0.7890646069015776
pearson_95 0.6157584478269645
pearson_99 0.4390846951490213
rscore 0.6131682106168539
rscore_95 -2.6502360279949193
rscore_99 -8.904226300847343
nse [0.61316821]
nse_95 [-2.65023603]
nse_99 [-8.9042263]
kge [0.58109558]
ext_kge_95 [0.4455914]
ext_kge_99 [-0.03926883]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([74857 74881 74905], shape=(3,), dtype=int64) Times out: tf.Tensor(74905, shape=(), dtype=int64)
Times in: tf.Tensor([97252 97276 97300], shape=(3,), dtype=int64) Times out: tf.Tensor(97300, shape=(), dtype=int64)
Times in: tf.Tensor([68421 68445 68469], shape=(3,), dtype=int64) Times out: tf.Tensor(68469, shape=(), dtype=int64)
Times in: tf.Tensor([68425 68449 68473], shape=(3,), dtype=int64) Times out: tf.Tensor(68473, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_279&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_280 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_558 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_559 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_279 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_558 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_279 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_559 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 48.1162 - mse: 48.0673 - mae: 5.3877 - val_loss: 40.7201 - val_mse: 40.6557 - val_mae: 5.1094
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.8406 - mse: 39.7700 - mae: 4.9145 - val_loss: 37.8457 - val_mse: 37.7667 - val_mae: 4.9187
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.4014 - mse: 38.3175 - mae: 4.8201 - val_loss: 36.8123 - val_mse: 36.7222 - val_mae: 4.8443
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.7208 - mse: 37.6273 - mae: 4.7778 - val_loss: 36.3703 - val_mse: 36.2718 - val_mae: 4.8160
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.3578 - mse: 37.2570 - mae: 4.7488 - val_loss: 35.7578 - val_mse: 35.6526 - val_mae: 4.7725
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.0246 - mse: 36.9175 - mae: 4.7302 - val_loss: 36.0585 - val_mse: 35.9477 - val_mae: 4.7883
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.7029 - mse: 36.5902 - mae: 4.7080 - val_loss: 35.9148 - val_mse: 35.7985 - val_mae: 4.7873
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.5095 - mse: 36.3916 - mae: 4.6951 - val_loss: 35.3706 - val_mse: 35.2497 - val_mae: 4.7538
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.3737 - mse: 36.2511 - mae: 4.6839 - val_loss: 35.2187 - val_mse: 35.0930 - val_mae: 4.7263
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.0907 - mse: 35.9636 - mae: 4.6641 - val_loss: 35.1127 - val_mse: 34.9825 - val_mae: 4.7278
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.8862 - mse: 35.7544 - mae: 4.6523 - val_loss: 35.2960 - val_mse: 35.1612 - val_mae: 4.7294
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.6517 - mse: 35.5156 - mae: 4.6384 - val_loss: 35.8433 - val_mse: 35.7044 - val_mae: 4.7807
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.5187 - mse: 35.3785 - mae: 4.6263 - val_loss: 35.0466 - val_mse: 34.9036 - val_mae: 4.7347
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.4164 - mse: 35.2722 - mae: 4.6185 - val_loss: 35.1091 - val_mse: 34.9624 - val_mae: 4.7257
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.1295 - mse: 34.9819 - mae: 4.6009 - val_loss: 35.3639 - val_mse: 35.2137 - val_mae: 4.7447
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.0294 - mse: 34.8780 - mae: 4.5912 - val_loss: 34.9799 - val_mse: 34.8260 - val_mae: 4.7243
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.9716 - mse: 34.8166 - mae: 4.5903 - val_loss: 34.8192 - val_mse: 34.6618 - val_mae: 4.7044
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.8188 - mse: 34.6604 - mae: 4.5790 - val_loss: 35.3462 - val_mse: 35.1855 - val_mae: 4.7441
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.5843 - mse: 34.4226 - mae: 4.5624 - val_loss: 35.3899 - val_mse: 35.2257 - val_mae: 4.7499
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 34.5978 - mse: 34.4327 - mae: 4.5612 - val_loss: 35.0194 - val_mse: 34.8518 - val_mae: 4.7096
bias 0.0014814149
si 0.5511889
rmse 0.059035365
kgeprime [0.71045396]
rmse_95 0.084524505
rmse_99 0.07741797
pearson 0.8197651299441735
pearson_95 0.6873646387497694
pearson_99 0.7838918293057912
rscore 0.665803256478861
rscore_95 -5.3772111261305096
rscore_99 -7.26693330012699
nse [0.66580326]
nse_95 [-5.37721113]
nse_99 [-7.2669333]
kge [0.68217366]
ext_kge_95 [0.02845689]
ext_kge_99 [-0.2990858]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([44742 44766 44790], shape=(3,), dtype=int64) Times out: tf.Tensor(44790, shape=(), dtype=int64)
Times in: tf.Tensor([70680 70704 70728], shape=(3,), dtype=int64) Times out: tf.Tensor(70728, shape=(), dtype=int64)
Times in: tf.Tensor([66426 66450 66474], shape=(3,), dtype=int64) Times out: tf.Tensor(66474, shape=(), dtype=int64)
Times in: tf.Tensor([67900 67924 67948], shape=(3,), dtype=int64) Times out: tf.Tensor(67948, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_280&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_281 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_560 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_561 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_280 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_560 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_280 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_561 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 48.2928 - mse: 48.2463 - mae: 5.4657 - val_loss: 40.2731 - val_mse: 40.2114 - val_mae: 4.8050
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 39.0288 - mse: 38.9610 - mae: 4.9338 - val_loss: 39.1821 - val_mse: 39.1065 - val_mae: 4.7334
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.5501 - mse: 37.4707 - mae: 4.8377 - val_loss: 38.6514 - val_mse: 38.5667 - val_mae: 4.7048
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.0036 - mse: 36.9157 - mae: 4.7995 - val_loss: 37.8212 - val_mse: 37.7287 - val_mae: 4.6526
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.5124 - mse: 36.4171 - mae: 4.7699 - val_loss: 37.3313 - val_mse: 37.2320 - val_mae: 4.6110
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.2002 - mse: 36.0982 - mae: 4.7553 - val_loss: 37.3829 - val_mse: 37.2771 - val_mae: 4.6057
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.9973 - mse: 35.8891 - mae: 4.7384 - val_loss: 37.0289 - val_mse: 36.9171 - val_mae: 4.5806
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.6551 - mse: 35.5411 - mae: 4.7177 - val_loss: 37.1073 - val_mse: 36.9897 - val_mae: 4.5834
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.4830 - mse: 35.3633 - mae: 4.7062 - val_loss: 37.1138 - val_mse: 36.9911 - val_mae: 4.5802
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.1919 - mse: 35.0672 - mae: 4.6902 - val_loss: 36.4653 - val_mse: 36.3377 - val_mae: 4.5318
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.0471 - mse: 34.9176 - mae: 4.6797 - val_loss: 36.5240 - val_mse: 36.3916 - val_mae: 4.5360
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.8184 - mse: 34.6842 - mae: 4.6625 - val_loss: 35.9862 - val_mse: 35.8490 - val_mae: 4.4991
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.7181 - mse: 34.5792 - mae: 4.6580 - val_loss: 36.1360 - val_mse: 35.9941 - val_mae: 4.5060
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.4768 - mse: 34.3330 - mae: 4.6380 - val_loss: 35.8965 - val_mse: 35.7499 - val_mae: 4.4937
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.2779 - mse: 34.1298 - mae: 4.6248 - val_loss: 36.1179 - val_mse: 35.9674 - val_mae: 4.4983
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 34.0964 - mse: 33.9442 - mae: 4.6126 - val_loss: 35.5058 - val_mse: 35.3512 - val_mae: 4.4554
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.9675 - mse: 33.8115 - mae: 4.6038 - val_loss: 35.6611 - val_mse: 35.5029 - val_mae: 4.4707
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.8577 - mse: 33.6980 - mae: 4.5939 - val_loss: 35.6480 - val_mse: 35.4857 - val_mae: 4.4658
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.7254 - mse: 33.5618 - mae: 4.5838 - val_loss: 35.5950 - val_mse: 35.4290 - val_mae: 4.4671
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 33.6121 - mse: 33.4450 - mae: 4.5774 - val_loss: 35.6835 - val_mse: 35.5142 - val_mae: 4.4692
bias -0.006735961
si 0.57713974
rmse 0.059593793
kgeprime [0.52037647]
rmse_95 0.113679305
rmse_99 0.20958367
pearson 0.7959622665903056
pearson_95 -0.060327558410243856
pearson_99 -0.693290019247669
rscore 0.6269961275041858
rscore_95 -2.6933043016170872
rscore_99 -7.810161476658198
nse [0.62699613]
nse_95 [-2.6933043]
nse_99 [-7.81016148]
kge [0.61593699]
ext_kge_95 [-0.09950745]
ext_kge_99 [-0.7613519]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([32267 32291 32315], shape=(3,), dtype=int64) Times out: tf.Tensor(32315, shape=(), dtype=int64)
Times in: tf.Tensor([33684 33708 33732], shape=(3,), dtype=int64) Times out: tf.Tensor(33732, shape=(), dtype=int64)
Times in: tf.Tensor([36250 36274 36298], shape=(3,), dtype=int64) Times out: tf.Tensor(36298, shape=(), dtype=int64)
Times in: tf.Tensor([24671 24695 24719], shape=(3,), dtype=int64) Times out: tf.Tensor(24719, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_281&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_282 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_562 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_563 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_281 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_562 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_281 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_563 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 53.3061 - mse: 53.2594 - mae: 5.6765 - val_loss: 38.9255 - val_mse: 38.8605 - val_mae: 4.9387
Epoch 2/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 41.7670 - mse: 41.6940 - mae: 5.0453 - val_loss: 36.6252 - val_mse: 36.5441 - val_mae: 4.7493
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.9936 - mse: 39.9070 - mae: 4.9350 - val_loss: 36.1372 - val_mse: 36.0451 - val_mae: 4.7009
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.2525 - mse: 39.1571 - mae: 4.8904 - val_loss: 35.3440 - val_mse: 35.2450 - val_mae: 4.6508
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.8383 - mse: 38.7364 - mae: 4.8636 - val_loss: 35.1301 - val_mse: 35.0253 - val_mae: 4.6321
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.4460 - mse: 38.3387 - mae: 4.8407 - val_loss: 34.9987 - val_mse: 34.8889 - val_mae: 4.6227
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.2512 - mse: 38.1391 - mae: 4.8248 - val_loss: 34.7150 - val_mse: 34.6006 - val_mae: 4.6062
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.0259 - mse: 37.9094 - mae: 4.8117 - val_loss: 34.8479 - val_mse: 34.7294 - val_mae: 4.6142
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.7906 - mse: 37.6700 - mae: 4.7969 - val_loss: 34.3995 - val_mse: 34.2770 - val_mae: 4.5885
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.7254 - mse: 37.6008 - mae: 4.7959 - val_loss: 34.3061 - val_mse: 34.1798 - val_mae: 4.5813
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.3452 - mse: 37.2167 - mae: 4.7692 - val_loss: 34.2241 - val_mse: 34.0940 - val_mae: 4.5799
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.2038 - mse: 37.0714 - mae: 4.7597 - val_loss: 34.2158 - val_mse: 34.0821 - val_mae: 4.5774
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.0711 - mse: 36.9355 - mae: 4.7502 - val_loss: 34.1760 - val_mse: 34.0387 - val_mae: 4.5752
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.9712 - mse: 36.8319 - mae: 4.7446 - val_loss: 34.4389 - val_mse: 34.2981 - val_mae: 4.5939
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.8508 - mse: 36.7083 - mae: 4.7339 - val_loss: 34.1815 - val_mse: 34.0376 - val_mae: 4.5774
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.7947 - mse: 36.6488 - mae: 4.7336 - val_loss: 33.9442 - val_mse: 33.7970 - val_mae: 4.5638
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.4940 - mse: 36.3451 - mae: 4.7105 - val_loss: 34.1824 - val_mse: 34.0319 - val_mae: 4.5776
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.5743 - mse: 36.4221 - mae: 4.7172 - val_loss: 34.0785 - val_mse: 33.9252 - val_mae: 4.5685
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.4156 - mse: 36.2606 - mae: 4.7067 - val_loss: 34.0317 - val_mse: 33.8755 - val_mae: 4.5714
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.1802 - mse: 36.0225 - mae: 4.6919 - val_loss: 34.3547 - val_mse: 34.1959 - val_mae: 4.5904
bias 0.005024205
si 0.6093585
rmse 0.058477256
kgeprime [0.71624989]
rmse_95 0.08586377
rmse_99 0.077298425
pearson 0.7694149655962912
pearson_95 0.5759774451613411
pearson_99 0.4071000875885093
rscore 0.5838084517456461
rscore_95 -4.376799588022156
rscore_99 -4.191117684468322
nse [0.58380845]
nse_95 [-4.37679959]
nse_99 [-4.19111768]
kge [0.67480253]
ext_kge_95 [0.08687537]
ext_kge_99 [0.15863761]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([169371 169395 169419], shape=(3,), dtype=int64) Times out: tf.Tensor(169419, shape=(), dtype=int64)
Times in: tf.Tensor([97439 97463 97487], shape=(3,), dtype=int64) Times out: tf.Tensor(97487, shape=(), dtype=int64)
Times in: tf.Tensor([48122 48146 48170], shape=(3,), dtype=int64) Times out: tf.Tensor(48170, shape=(), dtype=int64)
Times in: tf.Tensor([40089 40113 40137], shape=(3,), dtype=int64) Times out: tf.Tensor(40137, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_282&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_283 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_564 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_565 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_282 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_564 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_282 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_565 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 54.1880 - mse: 54.1386 - mae: 5.7412 - val_loss: 39.4802 - val_mse: 39.4132 - val_mae: 4.9582
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 43.4716 - mse: 43.3907 - mae: 5.1484 - val_loss: 36.5767 - val_mse: 36.4814 - val_mae: 4.7674
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 40.7434 - mse: 40.6369 - mae: 4.9778 - val_loss: 34.1281 - val_mse: 34.0131 - val_mae: 4.6240
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 39.1186 - mse: 38.9970 - mae: 4.8768 - val_loss: 33.0854 - val_mse: 32.9593 - val_mae: 4.5442
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.5173 - mse: 38.3879 - mae: 4.8441 - val_loss: 32.7113 - val_mse: 32.5795 - val_mae: 4.5126
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.1507 - mse: 38.0163 - mae: 4.8170 - val_loss: 32.5075 - val_mse: 32.3709 - val_mae: 4.5053
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.7828 - mse: 37.6437 - mae: 4.7972 - val_loss: 32.4306 - val_mse: 32.2897 - val_mae: 4.5088
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.6052 - mse: 37.4617 - mae: 4.7820 - val_loss: 31.9504 - val_mse: 31.8051 - val_mae: 4.4654
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.2417 - mse: 37.0936 - mae: 4.7624 - val_loss: 31.8100 - val_mse: 31.6601 - val_mae: 4.4582
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.2162 - mse: 37.0639 - mae: 4.7567 - val_loss: 32.3236 - val_mse: 32.1696 - val_mae: 4.5112
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.9627 - mse: 36.8062 - mae: 4.7443 - val_loss: 31.6559 - val_mse: 31.4975 - val_mae: 4.4511
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.7953 - mse: 36.6350 - mae: 4.7335 - val_loss: 31.3017 - val_mse: 31.1399 - val_mae: 4.4192
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5909 - mse: 36.4269 - mae: 4.7173 - val_loss: 31.3103 - val_mse: 31.1447 - val_mae: 4.4243
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.3186 - mse: 36.1513 - mae: 4.7036 - val_loss: 31.4172 - val_mse: 31.2487 - val_mae: 4.4393
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.2075 - mse: 36.0368 - mae: 4.6908 - val_loss: 31.5826 - val_mse: 31.4104 - val_mae: 4.4550
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.1695 - mse: 35.9956 - mae: 4.6926 - val_loss: 31.8429 - val_mse: 31.6679 - val_mae: 4.4798
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.0092 - mse: 35.8327 - mae: 4.6762 - val_loss: 31.0154 - val_mse: 30.8376 - val_mae: 4.4035
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.7446 - mse: 35.5653 - mae: 4.6613 - val_loss: 30.7549 - val_mse: 30.5742 - val_mae: 4.3808
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.6462 - mse: 35.4640 - mae: 4.6548 - val_loss: 30.4474 - val_mse: 30.2641 - val_mae: 4.3550
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.6021 - mse: 35.4170 - mae: 4.6489 - val_loss: 30.3770 - val_mse: 30.1906 - val_mae: 4.3469
bias 0.0010519649
si 0.5690264
rmse 0.054945994
kgeprime [0.72036326]
rmse_95 0.08769006
rmse_99 0.11176121
pearson 0.7986358749761335
pearson_95 0.579802882714932
pearson_99 0.443529402981138
rscore 0.6374053356797307
rscore_95 -1.977034420087922
rscore_99 -3.501406113940857
nse [0.63740534]
nse_95 [-1.97703442]
nse_99 [-3.50140611]
kge [0.70126925]
ext_kge_95 [0.46632464]
ext_kge_99 [0.34148963]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -46.68 -46.37 -46.05 ... -40.43 -40.12
  * longitude       (longitude) float32 169.4 169.7 170.0 ... 175.6 175.9 176.2
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -1.568 ... 0.03632
    vgrd10m         (time, latitude, longitude) float32 0.2393 ... -0.3653
    uw2             (time, latitude, longitude) float32 2.459 ... 0.001319
    vw2             (time, latitude, longitude) float32 0.05725 ... 0.1335
    wind_magnitude  (time, latitude, longitude) float32 1.586 0.4669 ... 0.3671
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([126161 126185 126209], shape=(3,), dtype=int64) Times out: tf.Tensor(126209, shape=(), dtype=int64)
Times in: tf.Tensor([85567 85591 85615], shape=(3,), dtype=int64) Times out: tf.Tensor(85615, shape=(), dtype=int64)
Times in: tf.Tensor([98549 98573 98597], shape=(3,), dtype=int64) Times out: tf.Tensor(98597, shape=(), dtype=int64)
Times in: tf.Tensor([83621 83645 83669], shape=(3,), dtype=int64) Times out: tf.Tensor(83669, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_283&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_284 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_566 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_567 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_283 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_566 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_283 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_567 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 52.8303 - mse: 52.7841 - mae: 5.6580 - val_loss: 46.2779 - val_mse: 46.2203 - val_mae: 5.2864
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 42.6749 - mse: 42.6087 - mae: 5.1021 - val_loss: 39.8528 - val_mse: 39.7789 - val_mae: 4.9236
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 40.0395 - mse: 39.9598 - mae: 4.9507 - val_loss: 38.7223 - val_mse: 38.6375 - val_mae: 4.8660
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 39.2933 - mse: 39.2045 - mae: 4.8980 - val_loss: 38.1423 - val_mse: 38.0502 - val_mae: 4.8354
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.7125 - mse: 38.6176 - mae: 4.8632 - val_loss: 37.7474 - val_mse: 37.6500 - val_mae: 4.8116
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.4570 - mse: 38.3570 - mae: 4.8456 - val_loss: 37.1978 - val_mse: 37.0952 - val_mae: 4.7724
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.2048 - mse: 38.0997 - mae: 4.8275 - val_loss: 37.2784 - val_mse: 37.1708 - val_mae: 4.7817
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.7236 - mse: 37.6136 - mae: 4.7977 - val_loss: 36.8386 - val_mse: 36.7263 - val_mae: 4.7507
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.6794 - mse: 37.5647 - mae: 4.7927 - val_loss: 36.6062 - val_mse: 36.4891 - val_mae: 4.7347
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.4510 - mse: 37.3318 - mae: 4.7729 - val_loss: 36.9666 - val_mse: 36.8451 - val_mae: 4.7581
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.1487 - mse: 37.0251 - mae: 4.7573 - val_loss: 36.5899 - val_mse: 36.4641 - val_mae: 4.7362
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.9666 - mse: 36.8388 - mae: 4.7447 - val_loss: 36.6903 - val_mse: 36.5605 - val_mae: 4.7441
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.6967 - mse: 36.5649 - mae: 4.7274 - val_loss: 36.3806 - val_mse: 36.2469 - val_mae: 4.7235
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5869 - mse: 36.4512 - mae: 4.7208 - val_loss: 35.9045 - val_mse: 35.7668 - val_mae: 4.6923
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.4711 - mse: 36.3316 - mae: 4.7113 - val_loss: 35.7313 - val_mse: 35.5901 - val_mae: 4.6831
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.2413 - mse: 36.0982 - mae: 4.6944 - val_loss: 36.1904 - val_mse: 36.0455 - val_mae: 4.7145
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.0984 - mse: 35.9518 - mae: 4.6868 - val_loss: 36.0169 - val_mse: 35.8691 - val_mae: 4.7055
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.9634 - mse: 35.8137 - mae: 4.6726 - val_loss: 36.3646 - val_mse: 36.2132 - val_mae: 4.7296
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 35.7552 - mse: 35.6024 - mae: 4.6620 - val_loss: 36.1296 - val_mse: 35.9752 - val_mae: 4.7115
Epoch 20/20
4857/4857 [==============================] - 8s 2ms/step - loss: 35.7911 - mse: 35.6351 - mae: 4.6630 - val_loss: 36.1668 - val_mse: 36.0095 - val_mae: 4.7159
bias -0.0097468505
si 0.5887386
rmse 0.06000789
kgeprime [0.44135597]
rmse_95 0.0984949
rmse_99 0.12859738
pearson 0.7855844036809055
pearson_95 0.6193888926299972
pearson_99 0.45261932706644203
rscore 0.6066988127352374
rscore_95 -2.587701016890346
rscore_99 -8.042054422998993
nse [0.60669881]
nse_95 [-2.58770102]
nse_99 [-8.04205442]
kge [0.55887157]
ext_kge_95 [0.47817632]
ext_kge_99 [0.1578942]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([71740 71764 71788], shape=(3,), dtype=int64) Times out: tf.Tensor(71788, shape=(), dtype=int64)
Times in: tf.Tensor([1432 1456 1480], shape=(3,), dtype=int64) Times out: tf.Tensor(1480, shape=(), dtype=int64)
Times in: tf.Tensor([99066 99090 99114], shape=(3,), dtype=int64) Times out: tf.Tensor(99114, shape=(), dtype=int64)
Times in: tf.Tensor([30433 30457 30481], shape=(3,), dtype=int64) Times out: tf.Tensor(30481, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_284&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_285 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_568 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_569 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_284 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_568 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_284 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_569 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 51.4441 - mse: 51.3992 - mae: 5.5512 - val_loss: 44.6026 - val_mse: 44.5427 - val_mae: 5.3473
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 41.7820 - mse: 41.7150 - mae: 5.0204 - val_loss: 40.5507 - val_mse: 40.4759 - val_mae: 5.1132
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 40.0595 - mse: 39.9802 - mae: 4.9131 - val_loss: 39.1449 - val_mse: 39.0606 - val_mae: 5.0207
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.2490 - mse: 39.1613 - mae: 4.8639 - val_loss: 38.6336 - val_mse: 38.5424 - val_mae: 4.9907
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.8271 - mse: 38.7334 - mae: 4.8348 - val_loss: 37.3270 - val_mse: 37.2304 - val_mae: 4.8861
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.4207 - mse: 38.3222 - mae: 4.8122 - val_loss: 37.7077 - val_mse: 37.6069 - val_mae: 4.9234
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 38.1211 - mse: 38.0185 - mae: 4.7968 - val_loss: 37.6151 - val_mse: 37.5102 - val_mae: 4.9195
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.8496 - mse: 37.7433 - mae: 4.7767 - val_loss: 37.3486 - val_mse: 37.2404 - val_mae: 4.8926
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 37.8660 - mse: 37.7563 - mae: 4.7728 - val_loss: 37.4103 - val_mse: 37.2990 - val_mae: 4.9102
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.5440 - mse: 37.4312 - mae: 4.7506 - val_loss: 36.9367 - val_mse: 36.8223 - val_mae: 4.8697
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.4695 - mse: 37.3537 - mae: 4.7470 - val_loss: 37.1882 - val_mse: 37.0708 - val_mae: 4.8882
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.2384 - mse: 37.1198 - mae: 4.7367 - val_loss: 36.4968 - val_mse: 36.3768 - val_mae: 4.8429
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.0967 - mse: 36.9752 - mae: 4.7263 - val_loss: 37.2636 - val_mse: 37.1407 - val_mae: 4.8923
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.0067 - mse: 36.8824 - mae: 4.7202 - val_loss: 36.2331 - val_mse: 36.1075 - val_mae: 4.8221
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.7575 - mse: 36.6305 - mae: 4.7012 - val_loss: 36.5975 - val_mse: 36.4690 - val_mae: 4.8469
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.6941 - mse: 36.5643 - mae: 4.6967 - val_loss: 36.6232 - val_mse: 36.4920 - val_mae: 4.8501
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.5272 - mse: 36.3948 - mae: 4.6849 - val_loss: 36.5042 - val_mse: 36.3704 - val_mae: 4.8436
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.4126 - mse: 36.2777 - mae: 4.6801 - val_loss: 37.7283 - val_mse: 37.5920 - val_mae: 4.9344
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.3542 - mse: 36.2166 - mae: 4.6758 - val_loss: 36.6434 - val_mse: 36.5045 - val_mae: 4.8551
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 36.2116 - mse: 36.0716 - mae: 4.6659 - val_loss: 37.8748 - val_mse: 37.7334 - val_mae: 4.9398
bias 0.013166317
si 0.55960166
rmse 0.06142753
kgeprime [0.50902337]
rmse_95 0.101880014
rmse_99 0.0893921
pearson 0.8172350972107612
pearson_95 0.7075251487723653
pearson_99 0.6269950152966013
rscore 0.6393733638969317
rscore_95 -7.274947574689456
rscore_99 -8.779472230277705
nse [0.63937336]
nse_95 [-7.27494757]
nse_99 [-8.77947223]
kge [0.46470283]
ext_kge_95 [0.13756838]
ext_kge_99 [0.07269342]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([9466 9490 9514], shape=(3,), dtype=int64) Times out: tf.Tensor(9514, shape=(), dtype=int64)
Times in: tf.Tensor([65868 65892 65916], shape=(3,), dtype=int64) Times out: tf.Tensor(65916, shape=(), dtype=int64)
Times in: tf.Tensor([22058 22082 22106], shape=(3,), dtype=int64) Times out: tf.Tensor(22106, shape=(), dtype=int64)
Times in: tf.Tensor([829 853 877], shape=(3,), dtype=int64) Times out: tf.Tensor(877, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_285&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_286 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_570 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_571 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_285 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_570 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_285 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_571 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 50.6649 - mse: 50.6130 - mae: 5.5843 - val_loss: 41.1489 - val_mse: 41.0769 - val_mae: 4.8589
Epoch 2/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 40.4065 - mse: 40.3268 - mae: 5.0150 - val_loss: 39.2002 - val_mse: 39.1112 - val_mae: 4.7347
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 38.7603 - mse: 38.6667 - mae: 4.9084 - val_loss: 38.4423 - val_mse: 38.3422 - val_mae: 4.6906
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 38.1877 - mse: 38.0840 - mae: 4.8724 - val_loss: 38.2533 - val_mse: 38.1445 - val_mae: 4.6795
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.8970 - mse: 37.7852 - mae: 4.8496 - val_loss: 37.8598 - val_mse: 37.7437 - val_mae: 4.6513
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.5052 - mse: 37.3868 - mae: 4.8262 - val_loss: 37.7415 - val_mse: 37.6197 - val_mae: 4.6345
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.3695 - mse: 37.2455 - mae: 4.8165 - val_loss: 37.5094 - val_mse: 37.3823 - val_mae: 4.6192
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 37.0580 - mse: 36.9290 - mae: 4.7975 - val_loss: 37.2175 - val_mse: 37.0854 - val_mae: 4.5998
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.8166 - mse: 36.6828 - mae: 4.7813 - val_loss: 37.1711 - val_mse: 37.0344 - val_mae: 4.5899
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.5579 - mse: 36.4197 - mae: 4.7608 - val_loss: 37.0155 - val_mse: 36.8746 - val_mae: 4.5726
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.4424 - mse: 36.2999 - mae: 4.7584 - val_loss: 37.0884 - val_mse: 36.9432 - val_mae: 4.5759
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.1913 - mse: 36.0449 - mae: 4.7416 - val_loss: 36.9407 - val_mse: 36.7920 - val_mae: 4.5611
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 36.1046 - mse: 35.9543 - mae: 4.7352 - val_loss: 36.7959 - val_mse: 36.6435 - val_mae: 4.5513
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.9251 - mse: 35.7710 - mae: 4.7220 - val_loss: 36.9540 - val_mse: 36.7976 - val_mae: 4.5617
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.8241 - mse: 35.6663 - mae: 4.7180 - val_loss: 36.5502 - val_mse: 36.3905 - val_mae: 4.5340
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.6397 - mse: 35.4786 - mae: 4.7068 - val_loss: 36.3442 - val_mse: 36.1811 - val_mae: 4.5163
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.4888 - mse: 35.3241 - mae: 4.6964 - val_loss: 36.2587 - val_mse: 36.0918 - val_mae: 4.5070
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.2949 - mse: 35.1264 - mae: 4.6839 - val_loss: 36.1650 - val_mse: 35.9947 - val_mae: 4.4928
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.3038 - mse: 35.1322 - mae: 4.6814 - val_loss: 36.2651 - val_mse: 36.0920 - val_mae: 4.4993
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 35.1278 - mse: 34.9534 - mae: 4.6689 - val_loss: 36.0841 - val_mse: 35.9079 - val_mae: 4.4917
bias -0.0035198887
si 0.58266413
rmse 0.05992316
kgeprime [0.63899051]
rmse_95 0.114759736
rmse_99 0.21120645
pearson 0.7907799224100869
pearson_95 -0.0905019856047607
pearson_99 -0.6757161631219619
rscore 0.623709263721642
rscore_95 -2.7478105230958323
rscore_99 -8.190799792094188
nse [0.62370926]
nse_95 [-2.74781052]
nse_99 [-8.19079979]
kge [0.6954626]
ext_kge_95 [-0.12684927]
ext_kge_99 [-0.75644653]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([3423 3447 3471], shape=(3,), dtype=int64) Times out: tf.Tensor(3471, shape=(), dtype=int64)
Times in: tf.Tensor([13769 13793 13817], shape=(3,), dtype=int64) Times out: tf.Tensor(13817, shape=(), dtype=int64)
Times in: tf.Tensor([33272 33296 33320], shape=(3,), dtype=int64) Times out: tf.Tensor(33320, shape=(), dtype=int64)
Times in: tf.Tensor([32629 32653 32677], shape=(3,), dtype=int64) Times out: tf.Tensor(32677, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_286&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_287 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_572 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_573 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_286 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_572 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_286 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_573 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 50.2789 - mse: 50.2305 - mae: 5.5006 - val_loss: 38.2719 - val_mse: 38.2089 - val_mae: 4.8872
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 40.8914 - mse: 40.8217 - mae: 4.9861 - val_loss: 36.6920 - val_mse: 36.6161 - val_mae: 4.7593
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 39.4807 - mse: 39.4007 - mae: 4.9035 - val_loss: 36.1037 - val_mse: 36.0201 - val_mae: 4.7132
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.7434 - mse: 38.6576 - mae: 4.8579 - val_loss: 35.7273 - val_mse: 35.6392 - val_mae: 4.6840
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.3458 - mse: 38.2554 - mae: 4.8321 - val_loss: 35.5458 - val_mse: 35.4533 - val_mae: 4.6672
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 38.1381 - mse: 38.0439 - mae: 4.8182 - val_loss: 35.2719 - val_mse: 35.1759 - val_mae: 4.6534
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.9714 - mse: 37.8734 - mae: 4.8099 - val_loss: 35.4478 - val_mse: 35.3481 - val_mae: 4.6515
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.5941 - mse: 37.4923 - mae: 4.7861 - val_loss: 35.0851 - val_mse: 34.9815 - val_mae: 4.6295
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.4981 - mse: 37.3925 - mae: 4.7794 - val_loss: 34.8485 - val_mse: 34.7413 - val_mae: 4.6110
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 37.0972 - mse: 36.9879 - mae: 4.7564 - val_loss: 35.3347 - val_mse: 35.2237 - val_mae: 4.6363
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.9793 - mse: 36.8661 - mae: 4.7456 - val_loss: 35.0158 - val_mse: 34.9005 - val_mae: 4.6150
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.7682 - mse: 36.6511 - mae: 4.7338 - val_loss: 34.4608 - val_mse: 34.3420 - val_mae: 4.5818
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.5515 - mse: 36.4304 - mae: 4.7159 - val_loss: 34.3826 - val_mse: 34.2598 - val_mae: 4.5755
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.3988 - mse: 36.2740 - mae: 4.7073 - val_loss: 34.3003 - val_mse: 34.1739 - val_mae: 4.5693
Epoch 15/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 36.3568 - mse: 36.2282 - mae: 4.7030 - val_loss: 34.3139 - val_mse: 34.1838 - val_mae: 4.5658
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 36.1396 - mse: 36.0072 - mae: 4.6906 - val_loss: 35.1088 - val_mse: 34.9746 - val_mae: 4.6139
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.9257 - mse: 35.7894 - mae: 4.6783 - val_loss: 34.3057 - val_mse: 34.1678 - val_mae: 4.5668
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.8099 - mse: 35.6698 - mae: 4.6669 - val_loss: 34.4921 - val_mse: 34.3505 - val_mae: 4.5698
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.7228 - mse: 35.5788 - mae: 4.6617 - val_loss: 34.0818 - val_mse: 33.9362 - val_mae: 4.5484
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 35.6558 - mse: 35.5082 - mae: 4.6576 - val_loss: 34.0169 - val_mse: 33.8679 - val_mae: 4.5465
bias 0.0031265595
si 0.60633904
rmse 0.058196146
kgeprime [0.72596528]
rmse_95 0.08698497
rmse_99 0.07892541
pearson 0.7696660449854045
pearson_95 0.6107024951844416
pearson_99 0.48039489111730216
rscore 0.5902155960751121
rscore_95 -3.962268723518142
rscore_99 -3.9773121894205437
nse [0.5902156]
nse_95 [-3.96226872]
nse_99 [-3.97731219]
kge [0.6786954]
ext_kge_95 [0.19044883]
ext_kge_99 [0.31220233]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([163152 163176 163200], shape=(3,), dtype=int64) Times out: tf.Tensor(163200, shape=(), dtype=int64)
Times in: tf.Tensor([77545 77569 77593], shape=(3,), dtype=int64) Times out: tf.Tensor(77593, shape=(), dtype=int64)
Times in: tf.Tensor([148548 148572 148596], shape=(3,), dtype=int64) Times out: tf.Tensor(148596, shape=(), dtype=int64)
Times in: tf.Tensor([180900 180924 180948], shape=(3,), dtype=int64) Times out: tf.Tensor(180948, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_287&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_288 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_574 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_575 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_287 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_574 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_287 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_575 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 52.4000 - mse: 52.3536 - mae: 5.6294 - val_loss: 36.8652 - val_mse: 36.8051 - val_mae: 4.8071
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 41.2221 - mse: 41.1548 - mae: 5.0159 - val_loss: 35.4592 - val_mse: 35.3853 - val_mae: 4.7414
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 39.5968 - mse: 39.5178 - mae: 4.9125 - val_loss: 34.1690 - val_mse: 34.0853 - val_mae: 4.6461
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.9969 - mse: 38.9096 - mae: 4.8715 - val_loss: 33.9497 - val_mse: 33.8589 - val_mae: 4.6356
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.4257 - mse: 38.3320 - mae: 4.8335 - val_loss: 33.7377 - val_mse: 33.6412 - val_mae: 4.6204
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.2896 - mse: 38.1909 - mae: 4.8258 - val_loss: 33.3647 - val_mse: 33.2638 - val_mae: 4.5943
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 38.0802 - mse: 37.9773 - mae: 4.8122 - val_loss: 33.2048 - val_mse: 33.0997 - val_mae: 4.5840
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.7852 - mse: 37.6782 - mae: 4.7881 - val_loss: 32.8022 - val_mse: 32.6928 - val_mae: 4.5519
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.6282 - mse: 37.5168 - mae: 4.7811 - val_loss: 33.0675 - val_mse: 32.9542 - val_mae: 4.5815
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.4609 - mse: 37.3455 - mae: 4.7727 - val_loss: 33.3701 - val_mse: 33.2528 - val_mae: 4.6086
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 37.3380 - mse: 37.2188 - mae: 4.7610 - val_loss: 32.2069 - val_mse: 32.0858 - val_mae: 4.5081
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.9829 - mse: 36.8600 - mae: 4.7443 - val_loss: 31.9306 - val_mse: 31.8058 - val_mae: 4.4910
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.9283 - mse: 36.8016 - mae: 4.7350 - val_loss: 32.6061 - val_mse: 32.4777 - val_mae: 4.5485
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.8223 - mse: 36.6920 - mae: 4.7292 - val_loss: 31.5971 - val_mse: 31.4650 - val_mae: 4.4617
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.6494 - mse: 36.5156 - mae: 4.7187 - val_loss: 31.6483 - val_mse: 31.5125 - val_mae: 4.4698
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.5889 - mse: 36.4515 - mae: 4.7154 - val_loss: 31.8153 - val_mse: 31.6763 - val_mae: 4.4889
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.3172 - mse: 36.1765 - mae: 4.6946 - val_loss: 31.3110 - val_mse: 31.1686 - val_mae: 4.4450
Epoch 18/20
4857/4857 [==============================] - 8s 2ms/step - loss: 36.2498 - mse: 36.1059 - mae: 4.6884 - val_loss: 31.1705 - val_mse: 31.0250 - val_mae: 4.4342
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.1441 - mse: 35.9968 - mae: 4.6807 - val_loss: 31.3054 - val_mse: 31.1565 - val_mae: 4.4406
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 36.0534 - mse: 35.9029 - mae: 4.6742 - val_loss: 31.4592 - val_mse: 31.3070 - val_mae: 4.4599
bias 0.009426685
si 0.5714021
rmse 0.05595267
kgeprime [0.619968]
rmse_95 0.09748324
rmse_99 0.12468039
pearson 0.7968468378566652
pearson_95 0.5851245406990466
pearson_99 0.4515115911111995
rscore 0.6240452581254394
rscore_95 -2.631176560521924
rscore_99 -4.687494419461636
nse [0.62404526]
nse_95 [-2.63117656]
nse_99 [-4.68749442]
kge [0.57687892]
ext_kge_95 [0.44151869]
ext_kge_99 [0.32370589]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -45.74 -45.43 -45.12 ... -39.5 -39.18
  * longitude       (longitude) float32 170.3 170.6 170.9 ... 176.6 176.9 177.2
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 1.59 3.1 ... 2.468 2.854
    vgrd10m         (time, latitude, longitude) float32 3.239 5.13 ... -1.498
    uw2             (time, latitude, longitude) float32 2.527 9.61 ... 8.147
    vw2             (time, latitude, longitude) float32 10.49 26.31 ... 2.243
    wind_magnitude  (time, latitude, longitude) float32 3.608 5.994 ... 3.223
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([48156 48180 48204], shape=(3,), dtype=int64) Times out: tf.Tensor(48204, shape=(), dtype=int64)
Times in: tf.Tensor([47102 47126 47150], shape=(3,), dtype=int64) Times out: tf.Tensor(47150, shape=(), dtype=int64)
Times in: tf.Tensor([93907 93931 93955], shape=(3,), dtype=int64) Times out: tf.Tensor(93955, shape=(), dtype=int64)
Times in: tf.Tensor([43968 43992 44016], shape=(3,), dtype=int64) Times out: tf.Tensor(44016, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_288&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_289 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_576 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_577 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_288 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_576 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_288 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_577 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.7008 - mse: 28.6544 - mae: 4.1928 - val_loss: 21.3347 - val_mse: 21.2780 - val_mae: 3.6748
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.8285 - mse: 23.7660 - mae: 3.8457 - val_loss: 20.4980 - val_mse: 20.4312 - val_mae: 3.5937
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.4493 - mse: 23.3802 - mae: 3.8119 - val_loss: 20.3834 - val_mse: 20.3120 - val_mae: 3.5837
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.2500 - mse: 23.1774 - mae: 3.7922 - val_loss: 20.0294 - val_mse: 19.9556 - val_mae: 3.5465
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.1789 - mse: 23.1042 - mae: 3.7848 - val_loss: 20.4143 - val_mse: 20.3385 - val_mae: 3.5875
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0958 - mse: 23.0192 - mae: 3.7794 - val_loss: 20.2463 - val_mse: 20.1688 - val_mae: 3.5686
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0431 - mse: 22.9647 - mae: 3.7737 - val_loss: 20.1435 - val_mse: 20.0641 - val_mae: 3.5648
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.8497 - mse: 22.7695 - mae: 3.7568 - val_loss: 20.5024 - val_mse: 20.4208 - val_mae: 3.6022
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7130 - mse: 22.6304 - mae: 3.7429 - val_loss: 19.7949 - val_mse: 19.7109 - val_mae: 3.5319
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6110 - mse: 22.5258 - mae: 3.7334 - val_loss: 19.7267 - val_mse: 19.6405 - val_mae: 3.5293
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4248 - mse: 22.3373 - mae: 3.7174 - val_loss: 19.4823 - val_mse: 19.3935 - val_mae: 3.5050
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.3498 - mse: 22.2599 - mae: 3.7125 - val_loss: 19.5298 - val_mse: 19.4386 - val_mae: 3.5100
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.1923 - mse: 22.1001 - mae: 3.6955 - val_loss: 19.5471 - val_mse: 19.4538 - val_mae: 3.5130
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.1509 - mse: 22.0567 - mae: 3.6919 - val_loss: 19.3981 - val_mse: 19.3026 - val_mae: 3.5006
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9839 - mse: 21.8877 - mae: 3.6801 - val_loss: 19.4496 - val_mse: 19.3526 - val_mae: 3.5033
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9003 - mse: 21.8023 - mae: 3.6692 - val_loss: 19.4101 - val_mse: 19.3111 - val_mae: 3.4995
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.8297 - mse: 21.7297 - mae: 3.6628 - val_loss: 19.4937 - val_mse: 19.3927 - val_mae: 3.5095
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.7256 - mse: 21.6238 - mae: 3.6566 - val_loss: 19.2136 - val_mse: 19.1109 - val_mae: 3.4838
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.7124 - mse: 21.6086 - mae: 3.6556 - val_loss: 19.2613 - val_mse: 19.1565 - val_mae: 3.4896
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.6739 - mse: 21.5683 - mae: 3.6521 - val_loss: 19.4648 - val_mse: 19.3582 - val_mae: 3.5085
bias -0.0077672503
si 0.565309
rmse 0.043998018
kgeprime [0.46255216]
rmse_95 0.06462343
rmse_99 0.07297852
pearson 0.7986250389882181
pearson_95 0.27026891546583276
pearson_99 0.25480948345477394
rscore 0.6254548051630112
rscore_95 -5.830525106233566
rscore_99 -29.007923424801803
nse [0.62545481]
nse_95 [-5.83052511]
nse_99 [-29.00792342]
kge [0.57763637]
ext_kge_95 [0.10307321]
ext_kge_99 [-0.13133163]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([47017 47041 47065], shape=(3,), dtype=int64) Times out: tf.Tensor(47065, shape=(), dtype=int64)
Times in: tf.Tensor([57517 57541 57565], shape=(3,), dtype=int64) Times out: tf.Tensor(57565, shape=(), dtype=int64)
Times in: tf.Tensor([43553 43577 43601], shape=(3,), dtype=int64) Times out: tf.Tensor(43601, shape=(), dtype=int64)
Times in: tf.Tensor([870 894 918], shape=(3,), dtype=int64) Times out: tf.Tensor(918, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_289&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_290 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_578 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_579 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_289 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_578 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_289 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_579 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 8s 2ms/step - loss: 27.2345 - mse: 27.1921 - mae: 4.0949 - val_loss: 25.1409 - val_mse: 25.0873 - val_mae: 3.9357
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.9297 - mse: 22.8708 - mae: 3.7787 - val_loss: 24.5501 - val_mse: 24.4852 - val_mae: 3.8826
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5723 - mse: 22.5040 - mae: 3.7454 - val_loss: 23.0754 - val_mse: 23.0035 - val_mae: 3.7481
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3467 - mse: 22.2730 - mae: 3.7234 - val_loss: 23.8212 - val_mse: 23.7448 - val_mae: 3.8187
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.2188 - mse: 22.1413 - mae: 3.7133 - val_loss: 23.3968 - val_mse: 23.3172 - val_mae: 3.7808
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.1271 - mse: 22.0463 - mae: 3.7065 - val_loss: 23.3878 - val_mse: 23.3048 - val_mae: 3.7817
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0405 - mse: 21.9562 - mae: 3.6973 - val_loss: 23.3490 - val_mse: 23.2626 - val_mae: 3.7785
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8973 - mse: 21.8095 - mae: 3.6853 - val_loss: 22.5619 - val_mse: 22.4721 - val_mae: 3.7068
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7576 - mse: 21.6665 - mae: 3.6744 - val_loss: 22.5950 - val_mse: 22.5017 - val_mae: 3.7145
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.6388 - mse: 21.5439 - mae: 3.6633 - val_loss: 22.8602 - val_mse: 22.7634 - val_mae: 3.7395
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5453 - mse: 21.4470 - mae: 3.6506 - val_loss: 22.2087 - val_mse: 22.1086 - val_mae: 3.6802
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3885 - mse: 21.2870 - mae: 3.6396 - val_loss: 21.7888 - val_mse: 21.6857 - val_mae: 3.6425
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.2609 - mse: 21.1563 - mae: 3.6279 - val_loss: 22.1092 - val_mse: 22.0033 - val_mae: 3.6748
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.1351 - mse: 21.0278 - mae: 3.6172 - val_loss: 21.9601 - val_mse: 21.8515 - val_mae: 3.6631
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.0509 - mse: 20.9409 - mae: 3.6107 - val_loss: 21.8879 - val_mse: 21.7767 - val_mae: 3.6581
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.0042 - mse: 20.8916 - mae: 3.6041 - val_loss: 21.7852 - val_mse: 21.6715 - val_mae: 3.6510
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.8707 - mse: 20.7557 - mae: 3.5913 - val_loss: 22.0339 - val_mse: 21.9179 - val_mae: 3.6772
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.7726 - mse: 20.6552 - mae: 3.5826 - val_loss: 21.9958 - val_mse: 21.8777 - val_mae: 3.6772
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.7175 - mse: 20.5982 - mae: 3.5785 - val_loss: 21.7934 - val_mse: 21.6734 - val_mae: 3.6602
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.6689 - mse: 20.5478 - mae: 3.5751 - val_loss: 22.0478 - val_mse: 21.9257 - val_mae: 3.6861
bias 0.010619597
si 0.53590053
rmse 0.04682483
kgeprime [0.5087919]
rmse_95 0.08024935
rmse_99 0.10754737
pearson 0.8301399778309332
pearson_95 0.2815549949789132
pearson_99 0.21716740260583156
rscore 0.6641806830629684
rscore_95 -7.506768937840709
rscore_99 -27.03346398974334
nse [0.66418068]
nse_95 [-7.50676894]
nse_99 [-27.03346399]
kge [0.49665959]
ext_kge_95 [0.06107197]
ext_kge_99 [-0.73952117]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([38570 38594 38618], shape=(3,), dtype=int64) Times out: tf.Tensor(38618, shape=(), dtype=int64)
Times in: tf.Tensor([42888 42912 42936], shape=(3,), dtype=int64) Times out: tf.Tensor(42936, shape=(), dtype=int64)
Times in: tf.Tensor([43509 43533 43557], shape=(3,), dtype=int64) Times out: tf.Tensor(43557, shape=(), dtype=int64)
Times in: tf.Tensor([8128 8152 8176], shape=(3,), dtype=int64) Times out: tf.Tensor(8176, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_290&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_291 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_580 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_581 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_290 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_580 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_290 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_581 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.6273 - mse: 26.5736 - mae: 4.0628 - val_loss: 22.1422 - val_mse: 22.0746 - val_mae: 3.6735
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.9792 - mse: 22.9072 - mae: 3.7897 - val_loss: 21.6440 - val_mse: 21.5657 - val_mae: 3.6300
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.6027 - mse: 22.5226 - mae: 3.7543 - val_loss: 21.5122 - val_mse: 21.4275 - val_mae: 3.6175
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.4326 - mse: 22.3470 - mae: 3.7381 - val_loss: 21.2531 - val_mse: 21.1637 - val_mae: 3.6014
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.2404 - mse: 22.1505 - mae: 3.7244 - val_loss: 21.1201 - val_mse: 21.0265 - val_mae: 3.5905
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.0835 - mse: 21.9894 - mae: 3.7087 - val_loss: 20.9073 - val_mse: 20.8096 - val_mae: 3.5692
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.8654 - mse: 21.7675 - mae: 3.6898 - val_loss: 20.7952 - val_mse: 20.6937 - val_mae: 3.5567
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.6619 - mse: 21.5602 - mae: 3.6722 - val_loss: 20.5950 - val_mse: 20.4903 - val_mae: 3.5396
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.5274 - mse: 21.4224 - mae: 3.6621 - val_loss: 20.4785 - val_mse: 20.3708 - val_mae: 3.5274
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.4789 - mse: 21.3710 - mae: 3.6548 - val_loss: 20.4547 - val_mse: 20.3443 - val_mae: 3.5242
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.3352 - mse: 21.2243 - mae: 3.6429 - val_loss: 20.2664 - val_mse: 20.1529 - val_mae: 3.5070
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.1989 - mse: 21.0847 - mae: 3.6332 - val_loss: 20.1421 - val_mse: 20.0255 - val_mae: 3.4969
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.1300 - mse: 21.0128 - mae: 3.6283 - val_loss: 20.0899 - val_mse: 19.9707 - val_mae: 3.4921
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 21.0598 - mse: 20.9400 - mae: 3.6163 - val_loss: 20.0822 - val_mse: 19.9604 - val_mae: 3.4903
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.0051 - mse: 20.8825 - mae: 3.6154 - val_loss: 20.0710 - val_mse: 19.9468 - val_mae: 3.4881
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.9348 - mse: 20.8097 - mae: 3.6111 - val_loss: 20.0577 - val_mse: 19.9309 - val_mae: 3.4859
Epoch 17/20
4856/4856 [==============================] - 8s 2ms/step - loss: 20.8570 - mse: 20.7295 - mae: 3.6004 - val_loss: 19.9935 - val_mse: 19.8645 - val_mae: 3.4802
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.7919 - mse: 20.6620 - mae: 3.5947 - val_loss: 19.9442 - val_mse: 19.8127 - val_mae: 3.4752
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.7427 - mse: 20.6100 - mae: 3.5902 - val_loss: 20.1468 - val_mse: 20.0126 - val_mae: 3.4919
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.6650 - mse: 20.5298 - mae: 3.5859 - val_loss: 20.1067 - val_mse: 19.9701 - val_mae: 3.4890
bias 0.004710062
si 0.5465518
rmse 0.04468791
kgeprime [0.73925262]
rmse_95 0.0746179
rmse_99 0.08437398
pearson 0.8165582322488358
pearson_95 0.3496355008647696
pearson_99 -0.2144749927114503
rscore 0.6614371461234507
rscore_95 -5.8128076394761825
rscore_99 -21.33604444459692
nse [0.66143715]
nse_95 [-5.81280764]
nse_99 [-21.33604444]
kge [0.66156134]
ext_kge_95 [0.16691654]
ext_kge_99 [-0.39999974]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([29655 29679 29703], shape=(3,), dtype=int64) Times out: tf.Tensor(29703, shape=(), dtype=int64)
Times in: tf.Tensor([36085 36109 36133], shape=(3,), dtype=int64) Times out: tf.Tensor(36133, shape=(), dtype=int64)
Times in: tf.Tensor([28240 28264 28288], shape=(3,), dtype=int64) Times out: tf.Tensor(28288, shape=(), dtype=int64)
Times in: tf.Tensor([34914 34938 34962], shape=(3,), dtype=int64) Times out: tf.Tensor(34962, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_291&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_292 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_582 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_583 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_291 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_582 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_291 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_583 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 27.8574 - mse: 27.8209 - mae: 4.1399 - val_loss: 23.9357 - val_mse: 23.8908 - val_mae: 3.8749
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.5393 - mse: 23.4900 - mae: 3.8189 - val_loss: 23.4062 - val_mse: 23.3526 - val_mae: 3.8196
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 23.0006 - mse: 22.9440 - mae: 3.7781 - val_loss: 23.2428 - val_mse: 23.1829 - val_mae: 3.7918
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.7693 - mse: 22.7078 - mae: 3.7562 - val_loss: 23.1482 - val_mse: 23.0844 - val_mae: 3.7776
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 22.7235 - mse: 22.6586 - mae: 3.7557 - val_loss: 23.2292 - val_mse: 23.1627 - val_mae: 3.7877
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 22.6125 - mse: 22.5444 - mae: 3.7439 - val_loss: 23.0873 - val_mse: 23.0176 - val_mae: 3.7749
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5128 - mse: 22.4416 - mae: 3.7361 - val_loss: 22.9502 - val_mse: 22.8771 - val_mae: 3.7615
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3095 - mse: 22.2352 - mae: 3.7161 - val_loss: 22.7471 - val_mse: 22.6712 - val_mae: 3.7429
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0284 - mse: 21.9512 - mae: 3.6972 - val_loss: 22.6128 - val_mse: 22.5341 - val_mae: 3.7302
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0727 - mse: 21.9928 - mae: 3.6986 - val_loss: 22.7724 - val_mse: 22.6913 - val_mae: 3.7497
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8910 - mse: 21.8086 - mae: 3.6826 - val_loss: 22.4801 - val_mse: 22.3964 - val_mae: 3.7224
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8153 - mse: 21.7306 - mae: 3.6762 - val_loss: 22.4054 - val_mse: 22.3195 - val_mae: 3.7181
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7612 - mse: 21.6742 - mae: 3.6694 - val_loss: 22.2731 - val_mse: 22.1850 - val_mae: 3.7045
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.6179 - mse: 21.5289 - mae: 3.6607 - val_loss: 22.2905 - val_mse: 22.2003 - val_mae: 3.7054
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5461 - mse: 21.4549 - mae: 3.6536 - val_loss: 22.1891 - val_mse: 22.0969 - val_mae: 3.6977
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.5556 - mse: 21.4624 - mae: 3.6510 - val_loss: 22.1329 - val_mse: 22.0386 - val_mae: 3.6966
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.5104 - mse: 21.4152 - mae: 3.6476 - val_loss: 22.0828 - val_mse: 21.9867 - val_mae: 3.6932
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.4018 - mse: 21.3049 - mae: 3.6373 - val_loss: 22.0618 - val_mse: 21.9638 - val_mae: 3.6927
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3878 - mse: 21.2889 - mae: 3.6325 - val_loss: 22.2280 - val_mse: 22.1281 - val_mae: 3.7069
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3503 - mse: 21.2494 - mae: 3.6323 - val_loss: 22.0620 - val_mse: 21.9601 - val_mae: 3.6931
bias 0.0017702486
si 0.604231
rmse 0.046861652
kgeprime [0.72903643]
rmse_95 0.07414191
rmse_99 0.091538616
pearson 0.7714344958631256
pearson_95 0.3348071198744111
pearson_99 0.1369165228228603
rscore 0.5929833917631124
rscore_95 -7.558965462847633
rscore_99 -25.486896869072687
nse [0.59298339]
nse_95 [-7.55896546]
nse_99 [-25.48689687]
kge [0.69474405]
ext_kge_95 [0.05623422]
ext_kge_99 [-0.44177589]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([53933 53957 53981], shape=(3,), dtype=int64) Times out: tf.Tensor(53981, shape=(), dtype=int64)
Times in: tf.Tensor([41461 41485 41509], shape=(3,), dtype=int64) Times out: tf.Tensor(41509, shape=(), dtype=int64)
Times in: tf.Tensor([173294 173318 173342], shape=(3,), dtype=int64) Times out: tf.Tensor(173342, shape=(), dtype=int64)
Times in: tf.Tensor([106305 106329 106353], shape=(3,), dtype=int64) Times out: tf.Tensor(106353, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_292&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_293 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_584 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_585 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_292 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_584 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_292 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_585 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.7854 - mse: 28.7406 - mae: 4.2014 - val_loss: 22.0937 - val_mse: 22.0404 - val_mae: 3.7249
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.8918 - mse: 23.8342 - mae: 3.8433 - val_loss: 20.7412 - val_mse: 20.6795 - val_mae: 3.6298
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.4269 - mse: 23.3625 - mae: 3.7991 - val_loss: 20.5939 - val_mse: 20.5272 - val_mae: 3.6076
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.2946 - mse: 23.2264 - mae: 3.7848 - val_loss: 20.5447 - val_mse: 20.4750 - val_mae: 3.6098
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.1512 - mse: 23.0801 - mae: 3.7746 - val_loss: 20.3764 - val_mse: 20.3038 - val_mae: 3.5987
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0639 - mse: 22.9898 - mae: 3.7668 - val_loss: 20.2707 - val_mse: 20.1948 - val_mae: 3.5777
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.9527 - mse: 22.8753 - mae: 3.7614 - val_loss: 20.2839 - val_mse: 20.2049 - val_mae: 3.5800
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7775 - mse: 22.6972 - mae: 3.7409 - val_loss: 20.2279 - val_mse: 20.1458 - val_mae: 3.5700
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6363 - mse: 22.5529 - mae: 3.7276 - val_loss: 19.8875 - val_mse: 19.8024 - val_mae: 3.5524
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4865 - mse: 22.4001 - mae: 3.7180 - val_loss: 19.8703 - val_mse: 19.7824 - val_mae: 3.5416
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4606 - mse: 22.3718 - mae: 3.7181 - val_loss: 19.6116 - val_mse: 19.5211 - val_mae: 3.5306
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.3100 - mse: 22.2186 - mae: 3.7045 - val_loss: 19.7674 - val_mse: 19.6747 - val_mae: 3.5350
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2196 - mse: 22.1258 - mae: 3.6959 - val_loss: 19.5908 - val_mse: 19.4957 - val_mae: 3.5185
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.1293 - mse: 22.0332 - mae: 3.6908 - val_loss: 19.5158 - val_mse: 19.4183 - val_mae: 3.5196
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0528 - mse: 21.9543 - mae: 3.6822 - val_loss: 19.4898 - val_mse: 19.3902 - val_mae: 3.5185
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9930 - mse: 21.8924 - mae: 3.6775 - val_loss: 19.2868 - val_mse: 19.1850 - val_mae: 3.4998
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9345 - mse: 21.8317 - mae: 3.6726 - val_loss: 19.3392 - val_mse: 19.2352 - val_mae: 3.4953
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.8396 - mse: 21.7345 - mae: 3.6653 - val_loss: 19.1825 - val_mse: 19.0762 - val_mae: 3.4916
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.7965 - mse: 21.6892 - mae: 3.6573 - val_loss: 19.2278 - val_mse: 19.1193 - val_mae: 3.4965
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.7231 - mse: 21.6136 - mae: 3.6543 - val_loss: 19.2733 - val_mse: 19.1625 - val_mae: 3.5003
bias -0.0022327092
si 0.5654914
rmse 0.043774936
kgeprime [0.61426909]
rmse_95 0.0694018
rmse_99 0.09103583
pearson 0.8005851174916729
pearson_95 0.5266079232257755
pearson_99 0.6486378075893765
rscore 0.6360138694356827
rscore_95 -3.61332527221212
rscore_99 -9.907580782385905
nse [0.63601387]
nse_95 [-3.61332527]
nse_99 [-9.90758078]
kge [0.65991811]
ext_kge_95 [0.41630965]
ext_kge_99 [0.4736608]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -45.12 -44.8 -44.49 ... -38.87 -38.56
  * longitude       (longitude) float32 168.1 168.4 168.8 ... 174.1 174.4 174.7
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -1.679 -2.549 ... -2.487
    vgrd10m         (time, latitude, longitude) float32 3.758 3.549 ... -0.1581
    uw2             (time, latitude, longitude) float32 2.82 6.496 ... 6.184
    vw2             (time, latitude, longitude) float32 14.12 12.59 ... 0.02499
    wind_magnitude  (time, latitude, longitude) float32 4.116 4.369 ... 2.492
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([32211 32235 32259], shape=(3,), dtype=int64) Times out: tf.Tensor(32259, shape=(), dtype=int64)
Times in: tf.Tensor([46304 46328 46352], shape=(3,), dtype=int64) Times out: tf.Tensor(46352, shape=(), dtype=int64)
Times in: tf.Tensor([94844 94868 94892], shape=(3,), dtype=int64) Times out: tf.Tensor(94892, shape=(), dtype=int64)
Times in: tf.Tensor([150677 150701 150725], shape=(3,), dtype=int64) Times out: tf.Tensor(150725, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_293&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_294 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_586 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_587 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_293 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_586 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_293 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_587 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.0715 - mse: 29.0255 - mae: 4.1400 - val_loss: 20.4079 - val_mse: 20.3544 - val_mae: 3.4942
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.4400 - mse: 23.3822 - mae: 3.7508 - val_loss: 19.6536 - val_mse: 19.5921 - val_mae: 3.4341
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7469 - mse: 22.6822 - mae: 3.6922 - val_loss: 19.2239 - val_mse: 19.1562 - val_mae: 3.4258
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2225 - mse: 22.1525 - mae: 3.6504 - val_loss: 18.7821 - val_mse: 18.7096 - val_mae: 3.3740
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9780 - mse: 21.9031 - mae: 3.6282 - val_loss: 18.5164 - val_mse: 18.4390 - val_mae: 3.3580
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5525 - mse: 21.4728 - mae: 3.5916 - val_loss: 18.2665 - val_mse: 18.1846 - val_mae: 3.3414
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2833 - mse: 21.1995 - mae: 3.5702 - val_loss: 18.0753 - val_mse: 17.9897 - val_mae: 3.3244
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1497 - mse: 21.0623 - mae: 3.5600 - val_loss: 17.8423 - val_mse: 17.7529 - val_mae: 3.3037
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8826 - mse: 20.7914 - mae: 3.5352 - val_loss: 17.7496 - val_mse: 17.6565 - val_mae: 3.2943
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7078 - mse: 20.6129 - mae: 3.5207 - val_loss: 17.3385 - val_mse: 17.2418 - val_mae: 3.2532
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3920 - mse: 20.2935 - mae: 3.4924 - val_loss: 17.1356 - val_mse: 17.0355 - val_mae: 3.2367
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1761 - mse: 20.0745 - mae: 3.4756 - val_loss: 17.0452 - val_mse: 16.9421 - val_mae: 3.2257
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9585 - mse: 19.8539 - mae: 3.4562 - val_loss: 16.8354 - val_mse: 16.7296 - val_mae: 3.2083
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8588 - mse: 19.7516 - mae: 3.4441 - val_loss: 16.9362 - val_mse: 16.8277 - val_mae: 3.2129
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.7754 - mse: 19.6657 - mae: 3.4403 - val_loss: 16.7037 - val_mse: 16.5928 - val_mae: 3.1988
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6418 - mse: 19.5296 - mae: 3.4251 - val_loss: 16.3889 - val_mse: 16.2755 - val_mae: 3.1596
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5707 - mse: 19.4561 - mae: 3.4183 - val_loss: 16.3612 - val_mse: 16.2453 - val_mae: 3.1623
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3879 - mse: 19.2710 - mae: 3.4024 - val_loss: 16.2470 - val_mse: 16.1290 - val_mae: 3.1512
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2696 - mse: 19.1505 - mae: 3.3932 - val_loss: 16.2349 - val_mse: 16.1148 - val_mae: 3.1530
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2645 - mse: 19.1432 - mae: 3.3884 - val_loss: 16.1674 - val_mse: 16.0451 - val_mae: 3.1409
bias 0.0011514381
si 0.4714587
rmse 0.0400563
kgeprime [0.77824491]
rmse_95 0.06813381
rmse_99 0.10139458
pearson 0.8631256799112017
pearson_95 0.7103672078007462
pearson_99 0.5566254173519716
rscore 0.7407162406020371
rscore_95 -0.46594246654695226
rscore_99 -2.4960708816113866
nse [0.74071624]
nse_95 [-0.46594247]
nse_99 [-2.49607088]
kge [0.75453766]
ext_kge_95 [0.6260102]
ext_kge_99 [0.46032438]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([14346 14370 14394], shape=(3,), dtype=int64) Times out: tf.Tensor(14394, shape=(), dtype=int64)
Times in: tf.Tensor([5607 5631 5655], shape=(3,), dtype=int64) Times out: tf.Tensor(5655, shape=(), dtype=int64)
Times in: tf.Tensor([39363 39387 39411], shape=(3,), dtype=int64) Times out: tf.Tensor(39411, shape=(), dtype=int64)
Times in: tf.Tensor([64567 64591 64615], shape=(3,), dtype=int64) Times out: tf.Tensor(64615, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_294&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_295 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_588 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_589 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_294 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_588 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_294 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_589 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.5381 - mse: 26.4822 - mae: 3.9575 - val_loss: 22.0411 - val_mse: 21.9746 - val_mae: 3.6218
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5638 - mse: 21.4925 - mae: 3.6113 - val_loss: 20.8881 - val_mse: 20.8106 - val_mae: 3.5341
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.8334 - mse: 20.7514 - mae: 3.5466 - val_loss: 20.3068 - val_mse: 20.2189 - val_mae: 3.4786
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.4296 - mse: 20.3376 - mae: 3.5107 - val_loss: 19.8766 - val_mse: 19.7797 - val_mae: 3.4463
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.1191 - mse: 20.0187 - mae: 3.4867 - val_loss: 19.5411 - val_mse: 19.4366 - val_mae: 3.4214
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.8126 - mse: 19.7050 - mae: 3.4581 - val_loss: 19.3803 - val_mse: 19.2690 - val_mae: 3.4032
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5286 - mse: 19.4143 - mae: 3.4345 - val_loss: 19.1319 - val_mse: 19.0144 - val_mae: 3.3797
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2663 - mse: 19.1461 - mae: 3.4141 - val_loss: 18.8276 - val_mse: 18.7044 - val_mae: 3.3562
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.0750 - mse: 18.9493 - mae: 3.3921 - val_loss: 18.6331 - val_mse: 18.5051 - val_mae: 3.3339
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8711 - mse: 18.7410 - mae: 3.3770 - val_loss: 18.6250 - val_mse: 18.4926 - val_mae: 3.3267
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6397 - mse: 18.5052 - mae: 3.3589 - val_loss: 18.3484 - val_mse: 18.2118 - val_mae: 3.2997
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.5504 - mse: 18.4119 - mae: 3.3442 - val_loss: 18.2178 - val_mse: 18.0772 - val_mae: 3.2933
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3082 - mse: 18.1659 - mae: 3.3251 - val_loss: 18.2507 - val_mse: 18.1066 - val_mae: 3.2946
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 18.1373 - mse: 17.9916 - mae: 3.3079 - val_loss: 17.8413 - val_mse: 17.6940 - val_mae: 3.2540
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.0469 - mse: 17.8979 - mae: 3.3004 - val_loss: 17.8593 - val_mse: 17.7089 - val_mae: 3.2534
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.9735 - mse: 17.8216 - mae: 3.2933 - val_loss: 17.8166 - val_mse: 17.6632 - val_mae: 3.2519
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.8484 - mse: 17.6938 - mae: 3.2815 - val_loss: 17.7287 - val_mse: 17.5727 - val_mae: 3.2444
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.7550 - mse: 17.5981 - mae: 3.2738 - val_loss: 17.6411 - val_mse: 17.4830 - val_mae: 3.2368
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.6459 - mse: 17.4867 - mae: 3.2620 - val_loss: 17.6792 - val_mse: 17.5189 - val_mae: 3.2357
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.6585 - mse: 17.4973 - mae: 3.2571 - val_loss: 17.9376 - val_mse: 17.7756 - val_mae: 3.2547
bias 0.0010436838
si 0.45631993
rmse 0.042161085
kgeprime [0.7799687]
rmse_95 0.07076549
rmse_99 0.089376815
pearson 0.8776823122676802
pearson_95 0.6155149802467119
pearson_99 0.6871804377285761
rscore 0.7630670721920867
rscore_95 -1.1955340408595951
rscore_99 -11.999957222858393
nse [0.76306707]
nse_95 [-1.19553404]
nse_99 [-11.99995722]
kge [0.7576584]
ext_kge_95 [0.52925416]
ext_kge_99 [-0.05340496]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([37101 37125 37149], shape=(3,), dtype=int64) Times out: tf.Tensor(37149, shape=(), dtype=int64)
Times in: tf.Tensor([33336 33360 33384], shape=(3,), dtype=int64) Times out: tf.Tensor(33384, shape=(), dtype=int64)
Times in: tf.Tensor([37168 37192 37216], shape=(3,), dtype=int64) Times out: tf.Tensor(37216, shape=(), dtype=int64)
Times in: tf.Tensor([55259 55283 55307], shape=(3,), dtype=int64) Times out: tf.Tensor(55307, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_295&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_296 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_590 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_591 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_295 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_590 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_295 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_591 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 27.1372 - mse: 27.0847 - mae: 4.0170 - val_loss: 21.3889 - val_mse: 21.3233 - val_mae: 3.5592
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.1085 - mse: 22.0368 - mae: 3.6622 - val_loss: 20.3708 - val_mse: 20.2905 - val_mae: 3.4684
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.2431 - mse: 21.1575 - mae: 3.5877 - val_loss: 20.0804 - val_mse: 19.9878 - val_mae: 3.4467
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.6191 - mse: 20.5222 - mae: 3.5332 - val_loss: 19.3142 - val_mse: 19.2118 - val_mae: 3.3768
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2507 - mse: 20.1452 - mae: 3.4993 - val_loss: 18.9256 - val_mse: 18.8158 - val_mae: 3.3408
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.9322 - mse: 19.8196 - mae: 3.4757 - val_loss: 18.3556 - val_mse: 18.2392 - val_mae: 3.2886
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.5455 - mse: 19.4265 - mae: 3.4402 - val_loss: 18.3216 - val_mse: 18.1996 - val_mae: 3.2814
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3877 - mse: 19.2635 - mae: 3.4254 - val_loss: 18.2036 - val_mse: 18.0766 - val_mae: 3.2680
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.1086 - mse: 18.9797 - mae: 3.3994 - val_loss: 17.7190 - val_mse: 17.5877 - val_mae: 3.2210
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.9201 - mse: 18.7868 - mae: 3.3850 - val_loss: 17.6234 - val_mse: 17.4877 - val_mae: 3.2157
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.7041 - mse: 18.5670 - mae: 3.3671 - val_loss: 17.6939 - val_mse: 17.5548 - val_mae: 3.2179
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.4804 - mse: 18.3399 - mae: 3.3477 - val_loss: 17.2860 - val_mse: 17.1435 - val_mae: 3.1750
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.4017 - mse: 18.2580 - mae: 3.3386 - val_loss: 17.1999 - val_mse: 17.0543 - val_mae: 3.1665
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.2671 - mse: 18.1203 - mae: 3.3287 - val_loss: 17.3499 - val_mse: 17.2014 - val_mae: 3.1842
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.2037 - mse: 18.0543 - mae: 3.3190 - val_loss: 17.2753 - val_mse: 17.1245 - val_mae: 3.1745
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.0808 - mse: 17.9293 - mae: 3.3065 - val_loss: 16.8727 - val_mse: 16.7198 - val_mae: 3.1347
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.9943 - mse: 17.8406 - mae: 3.3011 - val_loss: 17.3443 - val_mse: 17.1895 - val_mae: 3.1801
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.0104 - mse: 17.8548 - mae: 3.3021 - val_loss: 17.1254 - val_mse: 16.9684 - val_mae: 3.1621
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.8718 - mse: 17.7143 - mae: 3.2861 - val_loss: 16.9821 - val_mse: 16.8232 - val_mae: 3.1483
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.8639 - mse: 17.7047 - mae: 3.2871 - val_loss: 17.1190 - val_mse: 16.9586 - val_mae: 3.1581
bias 0.006054591
si 0.4703076
rmse 0.041180775
kgeprime [0.75378757]
rmse_95 0.07813559
rmse_99 0.1194641
pearson 0.8654564389210712
pearson_95 0.4174881980601214
pearson_99 0.20873040342718238
rscore 0.7426736956849931
rscore_95 -2.140377857709252
rscore_99 -17.118618719827946
nse [0.7426737]
nse_95 [-2.14037786]
nse_99 [-17.11861872]
kge [0.7089115]
ext_kge_95 [0.29594101]
ext_kge_99 [-1.06216156]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([2699 2723 2747], shape=(3,), dtype=int64) Times out: tf.Tensor(2747, shape=(), dtype=int64)
Times in: tf.Tensor([34179 34203 34227], shape=(3,), dtype=int64) Times out: tf.Tensor(34227, shape=(), dtype=int64)
Times in: tf.Tensor([27491 27515 27539], shape=(3,), dtype=int64) Times out: tf.Tensor(27539, shape=(), dtype=int64)
Times in: tf.Tensor([5716 5740 5764], shape=(3,), dtype=int64) Times out: tf.Tensor(5764, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_296&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_297 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_592 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_593 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_296 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_592 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_296 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_593 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 30.5197 - mse: 30.4682 - mae: 4.2211 - val_loss: 23.0340 - val_mse: 22.9713 - val_mae: 3.7675
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5318 - mse: 22.4655 - mae: 3.6766 - val_loss: 22.5541 - val_mse: 22.4843 - val_mae: 3.7139
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8039 - mse: 21.7306 - mae: 3.6183 - val_loss: 22.4728 - val_mse: 22.3956 - val_mae: 3.6941
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3098 - mse: 21.2286 - mae: 3.5743 - val_loss: 22.2244 - val_mse: 22.1392 - val_mae: 3.6676
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.9743 - mse: 20.8855 - mae: 3.5476 - val_loss: 21.9448 - val_mse: 21.8525 - val_mae: 3.6446
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.6452 - mse: 20.5495 - mae: 3.5215 - val_loss: 21.5660 - val_mse: 21.4673 - val_mae: 3.6144
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.3821 - mse: 20.2804 - mae: 3.4998 - val_loss: 21.5341 - val_mse: 21.4297 - val_mae: 3.6065
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.1910 - mse: 20.0841 - mae: 3.4821 - val_loss: 21.2310 - val_mse: 21.1219 - val_mae: 3.5823
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.8572 - mse: 19.7455 - mae: 3.4525 - val_loss: 21.2055 - val_mse: 21.0916 - val_mae: 3.5719
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6440 - mse: 19.5282 - mae: 3.4327 - val_loss: 20.8198 - val_mse: 20.7023 - val_mae: 3.5413
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4037 - mse: 19.2845 - mae: 3.4123 - val_loss: 20.6623 - val_mse: 20.5416 - val_mae: 3.5285
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2716 - mse: 19.1495 - mae: 3.4007 - val_loss: 20.6699 - val_mse: 20.5463 - val_mae: 3.5236
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.0265 - mse: 18.9016 - mae: 3.3806 - val_loss: 20.1884 - val_mse: 20.0625 - val_mae: 3.4915
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8660 - mse: 18.7385 - mae: 3.3636 - val_loss: 20.1510 - val_mse: 20.0222 - val_mae: 3.4821
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7959 - mse: 18.6656 - mae: 3.3610 - val_loss: 20.6821 - val_mse: 20.5505 - val_mae: 3.5121
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7049 - mse: 18.5720 - mae: 3.3474 - val_loss: 20.3246 - val_mse: 20.1907 - val_mae: 3.4869
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.5957 - mse: 18.4606 - mae: 3.3395 - val_loss: 20.0690 - val_mse: 19.9328 - val_mae: 3.4679
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.4408 - mse: 18.3033 - mae: 3.3275 - val_loss: 20.6640 - val_mse: 20.5255 - val_mae: 3.5066
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3231 - mse: 18.1836 - mae: 3.3173 - val_loss: 20.0105 - val_mse: 19.8699 - val_mae: 3.4581
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3629 - mse: 18.2213 - mae: 3.3168 - val_loss: 20.0382 - val_mse: 19.8955 - val_mae: 3.4633
bias -0.0066005555
si 0.5307155
rmse 0.044604383
kgeprime [0.59839492]
rmse_95 0.05804095
rmse_99 0.062696576
pearson 0.8279192117516572
pearson_95 0.5493987688998271
pearson_99 0.6358581357145502
rscore 0.6743625915661525
rscore_95 -1.5266745930169274
rscore_99 -2.7164873262708924
nse [0.67436259]
nse_95 [-1.52667459]
nse_99 [-2.71648733]
kge [0.69009862]
ext_kge_95 [0.31376838]
ext_kge_99 [0.09372026]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([87599 87623 87647], shape=(3,), dtype=int64) Times out: tf.Tensor(87647, shape=(), dtype=int64)
Times in: tf.Tensor([130303 130327 130351], shape=(3,), dtype=int64) Times out: tf.Tensor(130351, shape=(), dtype=int64)
Times in: tf.Tensor([76537 76561 76585], shape=(3,), dtype=int64) Times out: tf.Tensor(76585, shape=(), dtype=int64)
Times in: tf.Tensor([48658 48682 48706], shape=(3,), dtype=int64) Times out: tf.Tensor(48706, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_297&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_298 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_594 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_595 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_297 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_594 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_297 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_595 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.3500 - mse: 37.3055 - mae: 4.6620 - val_loss: 22.9861 - val_mse: 22.9304 - val_mae: 3.7011
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.6149 - mse: 25.5556 - mae: 3.9015 - val_loss: 20.6128 - val_mse: 20.5501 - val_mae: 3.5181
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.4840 - mse: 24.4184 - mae: 3.8154 - val_loss: 20.9871 - val_mse: 20.9189 - val_mae: 3.5548
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.1161 - mse: 24.0454 - mae: 3.7877 - val_loss: 20.5769 - val_mse: 20.5040 - val_mae: 3.5176
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.6728 - mse: 23.5978 - mae: 3.7570 - val_loss: 20.5972 - val_mse: 20.5202 - val_mae: 3.5234
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.5131 - mse: 23.4340 - mae: 3.7477 - val_loss: 19.8700 - val_mse: 19.7889 - val_mae: 3.4572
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.2862 - mse: 23.2034 - mae: 3.7254 - val_loss: 19.9729 - val_mse: 19.8883 - val_mae: 3.4706
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.1886 - mse: 23.1025 - mae: 3.7189 - val_loss: 19.9349 - val_mse: 19.8475 - val_mae: 3.4646
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0458 - mse: 22.9567 - mae: 3.7011 - val_loss: 19.4857 - val_mse: 19.3953 - val_mae: 3.4260
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.8219 - mse: 22.7301 - mae: 3.6911 - val_loss: 19.1229 - val_mse: 19.0296 - val_mae: 3.3976
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7607 - mse: 22.6663 - mae: 3.6862 - val_loss: 19.6875 - val_mse: 19.5920 - val_mae: 3.4417
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6963 - mse: 22.5997 - mae: 3.6775 - val_loss: 18.9730 - val_mse: 18.8754 - val_mae: 3.3829
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.5533 - mse: 22.4545 - mae: 3.6614 - val_loss: 18.9691 - val_mse: 18.8691 - val_mae: 3.3820
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4642 - mse: 22.3633 - mae: 3.6601 - val_loss: 18.9422 - val_mse: 18.8403 - val_mae: 3.3804
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4137 - mse: 22.3107 - mae: 3.6571 - val_loss: 18.5715 - val_mse: 18.4674 - val_mae: 3.3523
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2616 - mse: 22.1566 - mae: 3.6391 - val_loss: 18.9321 - val_mse: 18.8263 - val_mae: 3.3827
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2526 - mse: 22.1456 - mae: 3.6375 - val_loss: 18.4032 - val_mse: 18.2950 - val_mae: 3.3395
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2248 - mse: 22.1155 - mae: 3.6310 - val_loss: 18.3444 - val_mse: 18.2339 - val_mae: 3.3364
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0007 - mse: 21.8892 - mae: 3.6158 - val_loss: 18.0546 - val_mse: 17.9419 - val_mae: 3.3124
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9147 - mse: 21.8010 - mae: 3.6091 - val_loss: 17.9380 - val_mse: 17.8234 - val_mae: 3.3022
bias 0.004414164
si 0.48782584
rmse 0.042217717
kgeprime [0.77947848]
rmse_95 0.075615436
rmse_99 0.08448879
pearson 0.8543451447633056
pearson_95 0.7136814833079461
pearson_99 0.6788056568504887
rscore 0.7218923786846665
rscore_95 -1.973143549766025
rscore_99 -3.916080341766566
nse [0.72189238]
nse_95 [-1.97314355]
nse_99 [-3.91608034]
kge [0.70427733]
ext_kge_95 [0.54712662]
ext_kge_99 [0.39560968]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -44.8 -44.49 -44.18 ... -38.25 -37.94
  * longitude       (longitude) float32 171.6 171.9 172.2 ... 177.5 177.8 178.1
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 5.6 5.339 ... 1.592
    vgrd10m         (time, latitude, longitude) float32 4.828 5.048 ... 1.064
    uw2             (time, latitude, longitude) float32 31.36 28.5 ... 2.533
    vw2             (time, latitude, longitude) float32 23.31 25.49 ... 1.132
    wind_magnitude  (time, latitude, longitude) float32 7.394 7.348 ... 1.914
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([86353 86377 86401], shape=(3,), dtype=int64) Times out: tf.Tensor(86401, shape=(), dtype=int64)
Times in: tf.Tensor([49440 49464 49488], shape=(3,), dtype=int64) Times out: tf.Tensor(49488, shape=(), dtype=int64)
Times in: tf.Tensor([30705 30729 30753], shape=(3,), dtype=int64) Times out: tf.Tensor(30753, shape=(), dtype=int64)
Times in: tf.Tensor([18191 18215 18239], shape=(3,), dtype=int64) Times out: tf.Tensor(18239, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_298&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_299 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_596 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_597 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_298 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_596 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_298 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_597 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 33.1660 - mse: 33.1129 - mae: 4.4885 - val_loss: 24.1877 - val_mse: 24.1228 - val_mae: 3.8875
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 27.1625 - mse: 27.0902 - mae: 4.1021 - val_loss: 23.4108 - val_mse: 23.3317 - val_mae: 3.8314
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.3098 - mse: 26.2257 - mae: 4.0299 - val_loss: 23.3732 - val_mse: 23.2843 - val_mae: 3.8304
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.7152 - mse: 25.6220 - mae: 3.9865 - val_loss: 22.8494 - val_mse: 22.7524 - val_mae: 3.7740
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.3579 - mse: 25.2573 - mae: 3.9585 - val_loss: 23.2069 - val_mse: 23.1030 - val_mae: 3.8060
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.0995 - mse: 24.9922 - mae: 3.9315 - val_loss: 22.4334 - val_mse: 22.3231 - val_mae: 3.7344
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.6822 - mse: 24.5690 - mae: 3.8992 - val_loss: 22.2629 - val_mse: 22.1472 - val_mae: 3.7189
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.3428 - mse: 24.2245 - mae: 3.8723 - val_loss: 22.0947 - val_mse: 21.9743 - val_mae: 3.7061
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.1125 - mse: 23.9901 - mae: 3.8502 - val_loss: 21.8622 - val_mse: 21.7378 - val_mae: 3.6847
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.8153 - mse: 23.6889 - mae: 3.8268 - val_loss: 21.6457 - val_mse: 21.5176 - val_mae: 3.6621
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.5941 - mse: 23.4642 - mae: 3.8088 - val_loss: 21.5074 - val_mse: 21.3762 - val_mae: 3.6481
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.3647 - mse: 23.2319 - mae: 3.7896 - val_loss: 21.2977 - val_mse: 21.1638 - val_mae: 3.6311
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.2171 - mse: 23.0818 - mae: 3.7814 - val_loss: 21.1931 - val_mse: 21.0567 - val_mae: 3.6225
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0878 - mse: 22.9503 - mae: 3.7699 - val_loss: 21.0559 - val_mse: 20.9177 - val_mae: 3.6088
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.9788 - mse: 22.8396 - mae: 3.7589 - val_loss: 21.3860 - val_mse: 21.2460 - val_mae: 3.6386
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.9130 - mse: 22.7718 - mae: 3.7530 - val_loss: 21.4203 - val_mse: 21.2783 - val_mae: 3.6432
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.8938 - mse: 22.7508 - mae: 3.7516 - val_loss: 20.8440 - val_mse: 20.7004 - val_mae: 3.5919
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7910 - mse: 22.6464 - mae: 3.7450 - val_loss: 21.1527 - val_mse: 21.0074 - val_mae: 3.6186
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7544 - mse: 22.6082 - mae: 3.7397 - val_loss: 21.5832 - val_mse: 21.4363 - val_mae: 3.6598
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6889 - mse: 22.5410 - mae: 3.7315 - val_loss: 20.9905 - val_mse: 20.8418 - val_mae: 3.6028
bias -0.0060310997
si 0.5050272
rmse 0.04565283
kgeprime [0.61102223]
rmse_95 0.07303242
rmse_99 0.07735202
pearson 0.8402750816001928
pearson_95 0.4061057213806278
pearson_99 0.197042191874348
rscore 0.7006306404876934
rscore_95 -3.8910884067039166
rscore_99 -9.501615454346316
nse [0.70063064]
nse_95 [-3.89108841]
nse_99 [-9.50161545]
kge [0.70014445]
ext_kge_95 [0.21597721]
ext_kge_99 [0.15591843]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([114224 114248 114272], shape=(3,), dtype=int64) Times out: tf.Tensor(114272, shape=(), dtype=int64)
Times in: tf.Tensor([88360 88384 88408], shape=(3,), dtype=int64) Times out: tf.Tensor(88408, shape=(), dtype=int64)
Times in: tf.Tensor([60495 60519 60543], shape=(3,), dtype=int64) Times out: tf.Tensor(60543, shape=(), dtype=int64)
Times in: tf.Tensor([10486 10510 10534], shape=(3,), dtype=int64) Times out: tf.Tensor(10534, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_299&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_300 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_598 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_599 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_299 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_598 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_299 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_599 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 30.9199 - mse: 30.8749 - mae: 4.3494 - val_loss: 26.4317 - val_mse: 26.3755 - val_mae: 4.0013
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.7385 - mse: 25.6766 - mae: 3.9968 - val_loss: 26.3530 - val_mse: 26.2867 - val_mae: 3.9860
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.2653 - mse: 25.1953 - mae: 3.9597 - val_loss: 25.0038 - val_mse: 24.9315 - val_mae: 3.8787
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.9774 - mse: 24.9025 - mae: 3.9326 - val_loss: 25.0682 - val_mse: 24.9916 - val_mae: 3.8819
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.8042 - mse: 24.7248 - mae: 3.9197 - val_loss: 24.8766 - val_mse: 24.7955 - val_mae: 3.8683
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.4405 - mse: 24.3565 - mae: 3.8901 - val_loss: 23.9836 - val_mse: 23.8976 - val_mae: 3.7971
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.0509 - mse: 23.9622 - mae: 3.8542 - val_loss: 23.3599 - val_mse: 23.2699 - val_mae: 3.7505
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.7075 - mse: 23.6148 - mae: 3.8268 - val_loss: 23.1138 - val_mse: 23.0197 - val_mae: 3.7282
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.3792 - mse: 23.2827 - mae: 3.7993 - val_loss: 22.5395 - val_mse: 22.4416 - val_mae: 3.6842
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.1553 - mse: 23.0551 - mae: 3.7821 - val_loss: 22.3178 - val_mse: 22.2163 - val_mae: 3.6645
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.8937 - mse: 22.7901 - mae: 3.7604 - val_loss: 22.0511 - val_mse: 21.9466 - val_mae: 3.6436
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.6400 - mse: 22.5334 - mae: 3.7400 - val_loss: 21.9277 - val_mse: 21.8203 - val_mae: 3.6346
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5846 - mse: 22.4753 - mae: 3.7307 - val_loss: 21.7308 - val_mse: 21.6207 - val_mae: 3.6192
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3805 - mse: 22.2688 - mae: 3.7182 - val_loss: 21.8251 - val_mse: 21.7126 - val_mae: 3.6266
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3784 - mse: 22.2642 - mae: 3.7155 - val_loss: 21.9180 - val_mse: 21.8028 - val_mae: 3.6327
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.2221 - mse: 22.1054 - mae: 3.7034 - val_loss: 21.6090 - val_mse: 21.4917 - val_mae: 3.6089
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.2359 - mse: 22.1169 - mae: 3.7066 - val_loss: 21.4730 - val_mse: 21.3531 - val_mae: 3.5994
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0261 - mse: 21.9047 - mae: 3.6850 - val_loss: 21.7856 - val_mse: 21.6635 - val_mae: 3.6261
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0148 - mse: 21.8913 - mae: 3.6869 - val_loss: 21.7042 - val_mse: 21.5799 - val_mae: 3.6189
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0171 - mse: 21.8916 - mae: 3.6874 - val_loss: 21.2369 - val_mse: 21.1104 - val_mae: 3.5792
bias 0.003823238
si 0.4709278
rmse 0.045946047
kgeprime [0.80416417]
rmse_95 0.07522273
rmse_99 0.10283606
pearson 0.8685263237877008
pearson_95 0.45961300532785754
pearson_99 0.6429152423443597
rscore 0.7488770135772405
rscore_95 -4.236366161745548
rscore_99 -13.964024478370913
nse [0.74887701]
nse_95 [-4.23636616]
nse_99 [-13.96402448]
kge [0.73865498]
ext_kge_95 [0.26182813]
ext_kge_99 [0.01290892]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([49939 49963 49987], shape=(3,), dtype=int64) Times out: tf.Tensor(49987, shape=(), dtype=int64)
Times in: tf.Tensor([62220 62244 62268], shape=(3,), dtype=int64) Times out: tf.Tensor(62268, shape=(), dtype=int64)
Times in: tf.Tensor([3984 4008 4032], shape=(3,), dtype=int64) Times out: tf.Tensor(4032, shape=(), dtype=int64)
Times in: tf.Tensor([1566 1590 1614], shape=(3,), dtype=int64) Times out: tf.Tensor(1614, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_300&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_301 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_600 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_601 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_300 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_600 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_300 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_601 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 31.7363 - mse: 31.6924 - mae: 4.4192 - val_loss: 24.2674 - val_mse: 24.2143 - val_mae: 3.8451
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 26.0323 - mse: 25.9731 - mae: 4.0331 - val_loss: 23.4385 - val_mse: 23.3727 - val_mae: 3.7552
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 25.3467 - mse: 25.2770 - mae: 3.9723 - val_loss: 23.2142 - val_mse: 23.1398 - val_mae: 3.7364
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.9480 - mse: 24.8710 - mae: 3.9392 - val_loss: 22.7916 - val_mse: 22.7112 - val_mae: 3.6938
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.6238 - mse: 24.5410 - mae: 3.9132 - val_loss: 23.0832 - val_mse: 22.9976 - val_mae: 3.7219
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.2931 - mse: 24.2054 - mae: 3.8849 - val_loss: 22.4941 - val_mse: 22.4039 - val_mae: 3.6666
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.0299 - mse: 23.9379 - mae: 3.8641 - val_loss: 22.0750 - val_mse: 21.9805 - val_mae: 3.6223
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.8390 - mse: 23.7429 - mae: 3.8483 - val_loss: 22.0312 - val_mse: 21.9330 - val_mae: 3.6295
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.5041 - mse: 23.4042 - mae: 3.8222 - val_loss: 21.5479 - val_mse: 21.4461 - val_mae: 3.5773
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.3772 - mse: 23.2740 - mae: 3.8122 - val_loss: 21.7775 - val_mse: 21.6723 - val_mae: 3.6080
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.0597 - mse: 22.9532 - mae: 3.7831 - val_loss: 21.2976 - val_mse: 21.1894 - val_mae: 3.5555
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.9500 - mse: 22.8403 - mae: 3.7744 - val_loss: 21.0499 - val_mse: 20.9383 - val_mae: 3.5275
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.8530 - mse: 22.7403 - mae: 3.7685 - val_loss: 21.1095 - val_mse: 20.9953 - val_mae: 3.5345
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.7492 - mse: 22.6341 - mae: 3.7578 - val_loss: 20.9865 - val_mse: 20.8700 - val_mae: 3.5209
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.7074 - mse: 22.5899 - mae: 3.7537 - val_loss: 20.9993 - val_mse: 20.8805 - val_mae: 3.5187
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.5329 - mse: 22.4132 - mae: 3.7401 - val_loss: 21.1931 - val_mse: 21.0721 - val_mae: 3.5427
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.3762 - mse: 22.2543 - mae: 3.7284 - val_loss: 21.0720 - val_mse: 20.9491 - val_mae: 3.5246
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.4515 - mse: 22.3277 - mae: 3.7335 - val_loss: 21.0285 - val_mse: 20.9035 - val_mae: 3.5214
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.3748 - mse: 22.2489 - mae: 3.7290 - val_loss: 21.1213 - val_mse: 20.9943 - val_mae: 3.5356
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.3997 - mse: 22.2720 - mae: 3.7291 - val_loss: 20.8801 - val_mse: 20.7514 - val_mae: 3.5054
bias -0.0011895706
si 0.49897212
rmse 0.04555369
kgeprime [0.7522921]
rmse_95 0.070945166
rmse_99 0.12461723
pearson 0.846723642827358
pearson_95 0.20420296804463553
pearson_99 0.05122517251175708
rscore 0.7166945764245665
rscore_95 -2.6071707088188565
rscore_99 -16.052106980758488
nse [0.71669458]
nse_95 [-2.60717071]
nse_99 [-16.05210698]
kge [0.77486794]
ext_kge_95 [0.16268286]
ext_kge_99 [-0.49470023]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([4648 4672 4696], shape=(3,), dtype=int64) Times out: tf.Tensor(4696, shape=(), dtype=int64)
Times in: tf.Tensor([34148 34172 34196], shape=(3,), dtype=int64) Times out: tf.Tensor(34196, shape=(), dtype=int64)
Times in: tf.Tensor([7332 7356 7380], shape=(3,), dtype=int64) Times out: tf.Tensor(7380, shape=(), dtype=int64)
Times in: tf.Tensor([29871 29895 29919], shape=(3,), dtype=int64) Times out: tf.Tensor(29919, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_301&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_302 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_602 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_603 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_301 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_602 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_301 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_603 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 35.7386 - mse: 35.6875 - mae: 4.6270 - val_loss: 26.5372 - val_mse: 26.4725 - val_mae: 4.1004
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 27.6084 - mse: 27.5396 - mae: 4.1159 - val_loss: 25.6002 - val_mse: 25.5283 - val_mae: 4.0001
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 27.1892 - mse: 27.1162 - mae: 4.0786 - val_loss: 25.3963 - val_mse: 25.3218 - val_mae: 3.9745
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 26.7979 - mse: 26.7225 - mae: 4.0543 - val_loss: 25.3336 - val_mse: 25.2567 - val_mae: 3.9629
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 26.6807 - mse: 26.6029 - mae: 4.0431 - val_loss: 25.1365 - val_mse: 25.0574 - val_mae: 3.9485
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 26.4655 - mse: 26.3853 - mae: 4.0279 - val_loss: 24.9641 - val_mse: 24.8825 - val_mae: 3.9391
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 26.2922 - mse: 26.2096 - mae: 4.0135 - val_loss: 24.8235 - val_mse: 24.7394 - val_mae: 3.9286
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 26.1433 - mse: 26.0581 - mae: 4.0001 - val_loss: 24.6559 - val_mse: 24.5693 - val_mae: 3.9126
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 26.0605 - mse: 25.9728 - mae: 3.9918 - val_loss: 24.5560 - val_mse: 24.4668 - val_mae: 3.9012
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.8002 - mse: 25.7103 - mae: 3.9755 - val_loss: 24.4014 - val_mse: 24.3101 - val_mae: 3.8932
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.6372 - mse: 25.5449 - mae: 3.9654 - val_loss: 24.4687 - val_mse: 24.3748 - val_mae: 3.8917
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.5503 - mse: 25.4556 - mae: 3.9588 - val_loss: 24.1450 - val_mse: 24.0490 - val_mae: 3.8707
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.4480 - mse: 25.3509 - mae: 3.9463 - val_loss: 24.0726 - val_mse: 23.9740 - val_mae: 3.8677
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.2551 - mse: 25.1557 - mae: 3.9292 - val_loss: 24.0394 - val_mse: 23.9387 - val_mae: 3.8571
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.1607 - mse: 25.0588 - mae: 3.9272 - val_loss: 24.1625 - val_mse: 24.0593 - val_mae: 3.8599
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.0935 - mse: 24.9893 - mae: 3.9168 - val_loss: 23.8763 - val_mse: 23.7707 - val_mae: 3.8438
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.0211 - mse: 24.9146 - mae: 3.9093 - val_loss: 23.7770 - val_mse: 23.6693 - val_mae: 3.8302
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.8813 - mse: 24.7724 - mae: 3.9019 - val_loss: 23.6501 - val_mse: 23.5400 - val_mae: 3.8253
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.7275 - mse: 24.6166 - mae: 3.8878 - val_loss: 23.5582 - val_mse: 23.4462 - val_mae: 3.8191
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.7491 - mse: 24.6361 - mae: 3.8883 - val_loss: 23.4859 - val_mse: 23.3717 - val_mae: 3.8183
bias 0.0012942132
si 0.5451524
rmse 0.04834433
kgeprime [0.76381245]
rmse_95 0.07672051
rmse_99 0.08566679
pearson 0.8152682821159137
pearson_95 0.48276364305888986
pearson_99 0.16452695287228558
rscore 0.6643743936108988
rscore_95 -6.2174988857743045
rscore_99 -13.987582262525972
nse [0.66437439]
nse_95 [-6.21749889]
nse_99 [-13.98758226]
kge [0.73995059]
ext_kge_95 [0.15387004]
ext_kge_99 [-0.19632318]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([82173 82197 82221], shape=(3,), dtype=int64) Times out: tf.Tensor(82221, shape=(), dtype=int64)
Times in: tf.Tensor([56529 56553 56577], shape=(3,), dtype=int64) Times out: tf.Tensor(56577, shape=(), dtype=int64)
Times in: tf.Tensor([185043 185067 185091], shape=(3,), dtype=int64) Times out: tf.Tensor(185091, shape=(), dtype=int64)
Times in: tf.Tensor([189344 189368 189392], shape=(3,), dtype=int64) Times out: tf.Tensor(189392, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_302&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_303 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_604 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_605 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_302 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_604 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_302 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_605 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 34.0970 - mse: 34.0500 - mae: 4.5317 - val_loss: 24.0844 - val_mse: 24.0280 - val_mae: 3.9088
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 27.2006 - mse: 27.1418 - mae: 4.0899 - val_loss: 23.9560 - val_mse: 23.8957 - val_mae: 3.9041
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.8719 - mse: 26.8107 - mae: 4.0601 - val_loss: 23.3584 - val_mse: 23.2960 - val_mae: 3.8485
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.7101 - mse: 26.6468 - mae: 4.0454 - val_loss: 23.2281 - val_mse: 23.1637 - val_mae: 3.8413
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.5010 - mse: 26.4355 - mae: 4.0313 - val_loss: 23.3796 - val_mse: 23.3129 - val_mae: 3.8563
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.3780 - mse: 26.3102 - mae: 4.0186 - val_loss: 22.9356 - val_mse: 22.8666 - val_mae: 3.8138
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.3264 - mse: 26.2561 - mae: 4.0138 - val_loss: 22.8392 - val_mse: 22.7676 - val_mae: 3.8116
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.0926 - mse: 26.0195 - mae: 3.9941 - val_loss: 22.5453 - val_mse: 22.4705 - val_mae: 3.7839
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.7844 - mse: 25.7081 - mae: 3.9766 - val_loss: 22.5577 - val_mse: 22.4797 - val_mae: 3.7918
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.8040 - mse: 25.7243 - mae: 3.9721 - val_loss: 21.9782 - val_mse: 21.8965 - val_mae: 3.7350
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.4788 - mse: 25.3951 - mae: 3.9446 - val_loss: 21.7152 - val_mse: 21.6298 - val_mae: 3.7173
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.1949 - mse: 25.1076 - mae: 3.9258 - val_loss: 21.7366 - val_mse: 21.6475 - val_mae: 3.7241
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.0109 - mse: 24.9201 - mae: 3.9038 - val_loss: 21.2567 - val_mse: 21.1642 - val_mae: 3.6817
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.8027 - mse: 24.7084 - mae: 3.8871 - val_loss: 21.0809 - val_mse: 20.9850 - val_mae: 3.6738
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.6245 - mse: 24.5268 - mae: 3.8769 - val_loss: 20.8582 - val_mse: 20.7590 - val_mae: 3.6486
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.4841 - mse: 24.3835 - mae: 3.8583 - val_loss: 20.6370 - val_mse: 20.5346 - val_mae: 3.6341
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.2975 - mse: 24.1939 - mae: 3.8463 - val_loss: 20.8287 - val_mse: 20.7239 - val_mae: 3.6579
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.2849 - mse: 24.1786 - mae: 3.8429 - val_loss: 20.7088 - val_mse: 20.6011 - val_mae: 3.6357
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.2325 - mse: 24.1238 - mae: 3.8405 - val_loss: 20.5437 - val_mse: 20.4341 - val_mae: 3.6329
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.1190 - mse: 24.0081 - mae: 3.8246 - val_loss: 20.4854 - val_mse: 20.3734 - val_mae: 3.6263
bias 0.0017646408
si 0.5005164
rmse 0.04513685
kgeprime [0.77645898]
rmse_95 0.066529706
rmse_99 0.080622524
pearson 0.8448408361312462
pearson_95 0.6083295664543934
pearson_99 0.42945134620345493
rscore 0.7115192093570184
rscore_95 -2.4250886906929083
rscore_99 -5.1615972379905175
nse [0.71151921]
nse_95 [-2.42508869]
nse_99 [-5.16159724]
kge [0.74301922]
ext_kge_95 [0.49807398]
ext_kge_99 [0.35024043]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -44.49 -44.18 -43.87 ... -38.25 -37.94
  * longitude       (longitude) float32 170.0 170.3 170.6 ... 175.9 176.2 176.6
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -3.16 -2.209 ... 0.2841
    vgrd10m         (time, latitude, longitude) float32 2.888 2.888 ... 2.363
    uw2             (time, latitude, longitude) float32 9.984 4.88 ... 0.08074
    vw2             (time, latitude, longitude) float32 8.343 8.343 ... 5.583
    wind_magnitude  (time, latitude, longitude) float32 4.281 3.636 ... 2.38
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([76223 76247 76271], shape=(3,), dtype=int64) Times out: tf.Tensor(76271, shape=(), dtype=int64)
Times in: tf.Tensor([2792 2816 2840], shape=(3,), dtype=int64) Times out: tf.Tensor(2840, shape=(), dtype=int64)
Times in: tf.Tensor([16084 16108 16132], shape=(3,), dtype=int64) Times out: tf.Tensor(16132, shape=(), dtype=int64)
Times in: tf.Tensor([79913 79937 79961], shape=(3,), dtype=int64) Times out: tf.Tensor(79961, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_303&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_304 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_606 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_607 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_303 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_606 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_303 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_607 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 25.2863 - mse: 25.2365 - mae: 3.9354 - val_loss: 20.4409 - val_mse: 20.3763 - val_mae: 3.5059
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8821 - mse: 20.8104 - mae: 3.5836 - val_loss: 19.2114 - val_mse: 19.1328 - val_mae: 3.4236
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.2427 - mse: 20.1596 - mae: 3.5299 - val_loss: 18.7415 - val_mse: 18.6543 - val_mae: 3.3843
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9194 - mse: 19.8296 - mae: 3.4963 - val_loss: 18.5030 - val_mse: 18.4106 - val_mae: 3.3762
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6883 - mse: 19.5942 - mae: 3.4733 - val_loss: 18.3506 - val_mse: 18.2543 - val_mae: 3.3612
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5453 - mse: 19.4473 - mae: 3.4579 - val_loss: 18.2275 - val_mse: 18.1278 - val_mae: 3.3429
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.4300 - mse: 19.3292 - mae: 3.4477 - val_loss: 18.2205 - val_mse: 18.1180 - val_mae: 3.3418
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3362 - mse: 19.2326 - mae: 3.4393 - val_loss: 18.1304 - val_mse: 18.0254 - val_mae: 3.3313
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.1635 - mse: 19.0576 - mae: 3.4261 - val_loss: 18.0738 - val_mse: 17.9666 - val_mae: 3.3288
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0504 - mse: 18.9423 - mae: 3.4157 - val_loss: 18.0958 - val_mse: 17.9864 - val_mae: 3.3314
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.9768 - mse: 18.8665 - mae: 3.4075 - val_loss: 17.9691 - val_mse: 17.8574 - val_mae: 3.3230
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8760 - mse: 18.7634 - mae: 3.3998 - val_loss: 17.9288 - val_mse: 17.8149 - val_mae: 3.3116
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.7796 - mse: 18.6647 - mae: 3.3898 - val_loss: 17.9664 - val_mse: 17.8504 - val_mae: 3.3262
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6676 - mse: 18.5506 - mae: 3.3817 - val_loss: 17.9270 - val_mse: 17.8088 - val_mae: 3.3189
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6298 - mse: 18.5108 - mae: 3.3752 - val_loss: 17.9183 - val_mse: 17.7982 - val_mae: 3.3263
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6129 - mse: 18.4921 - mae: 3.3750 - val_loss: 17.9705 - val_mse: 17.8485 - val_mae: 3.3223
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.4771 - mse: 18.3542 - mae: 3.3643 - val_loss: 17.8511 - val_mse: 17.7270 - val_mae: 3.3221
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.4415 - mse: 18.3164 - mae: 3.3589 - val_loss: 17.8519 - val_mse: 17.7256 - val_mae: 3.3226
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.3280 - mse: 18.2008 - mae: 3.3490 - val_loss: 17.7890 - val_mse: 17.6606 - val_mae: 3.3116
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.3044 - mse: 18.1752 - mae: 3.3457 - val_loss: 17.6783 - val_mse: 17.5478 - val_mae: 3.3043
bias -0.001322398
si 0.59217453
rmse 0.041890115
kgeprime [0.63491689]
rmse_95 0.07193254
rmse_99 0.106631674
pearson 0.7804659973727465
pearson_95 0.45279575410102085
pearson_99 0.0027850491363344333
rscore 0.6081115120789216
rscore_95 -1.9773515702583664
rscore_99 -13.579768597757049
nse [0.60811151]
nse_95 [-1.97735157]
nse_99 [-13.5797686]
kge [0.66631206]
ext_kge_95 [0.36353533]
ext_kge_99 [-0.09811253]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([111705 111729 111753], shape=(3,), dtype=int64) Times out: tf.Tensor(111753, shape=(), dtype=int64)
Times in: tf.Tensor([115662 115686 115710], shape=(3,), dtype=int64) Times out: tf.Tensor(115710, shape=(), dtype=int64)
Times in: tf.Tensor([77878 77902 77926], shape=(3,), dtype=int64) Times out: tf.Tensor(77926, shape=(), dtype=int64)
Times in: tf.Tensor([71041 71065 71089], shape=(3,), dtype=int64) Times out: tf.Tensor(71089, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_304&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_305 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_608 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_609 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_304 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_608 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_304 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_609 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.6898 - mse: 25.6500 - mae: 3.9482 - val_loss: 22.8161 - val_mse: 22.7659 - val_mae: 3.6969
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.0489 - mse: 20.9969 - mae: 3.5948 - val_loss: 21.5451 - val_mse: 21.4893 - val_mae: 3.6029
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.4106 - mse: 20.3533 - mae: 3.5419 - val_loss: 21.3223 - val_mse: 21.2618 - val_mae: 3.5846
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.1101 - mse: 20.0488 - mae: 3.5162 - val_loss: 21.1887 - val_mse: 21.1248 - val_mae: 3.5797
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.9229 - mse: 19.8586 - mae: 3.5032 - val_loss: 20.8171 - val_mse: 20.7507 - val_mae: 3.5487
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.7485 - mse: 19.6815 - mae: 3.4869 - val_loss: 21.1337 - val_mse: 21.0649 - val_mae: 3.5757
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6407 - mse: 19.5717 - mae: 3.4771 - val_loss: 20.4555 - val_mse: 20.3846 - val_mae: 3.5207
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.5300 - mse: 19.4590 - mae: 3.4632 - val_loss: 20.3904 - val_mse: 20.3177 - val_mae: 3.5128
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4963 - mse: 19.4236 - mae: 3.4627 - val_loss: 20.5632 - val_mse: 20.4889 - val_mae: 3.5281
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.4385 - mse: 19.3640 - mae: 3.4555 - val_loss: 20.2737 - val_mse: 20.1978 - val_mae: 3.5021
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.2919 - mse: 19.2158 - mae: 3.4422 - val_loss: 20.3614 - val_mse: 20.2838 - val_mae: 3.5102
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.2831 - mse: 19.2053 - mae: 3.4409 - val_loss: 20.2002 - val_mse: 20.1207 - val_mae: 3.4972
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.1515 - mse: 19.0718 - mae: 3.4306 - val_loss: 20.3982 - val_mse: 20.3169 - val_mae: 3.5115
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.1192 - mse: 19.0377 - mae: 3.4289 - val_loss: 20.1858 - val_mse: 20.1028 - val_mae: 3.4945
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.0653 - mse: 18.9821 - mae: 3.4203 - val_loss: 20.3559 - val_mse: 20.2713 - val_mae: 3.5071
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.9898 - mse: 18.9048 - mae: 3.4168 - val_loss: 19.9779 - val_mse: 19.8915 - val_mae: 3.4760
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.9694 - mse: 18.8827 - mae: 3.4114 - val_loss: 20.0115 - val_mse: 19.9233 - val_mae: 3.4786
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.8448 - mse: 18.7561 - mae: 3.4023 - val_loss: 20.2800 - val_mse: 20.1898 - val_mae: 3.4991
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.7693 - mse: 18.6785 - mae: 3.3960 - val_loss: 19.9510 - val_mse: 19.8585 - val_mae: 3.4692
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.6846 - mse: 18.5917 - mae: 3.3871 - val_loss: 20.2188 - val_mse: 20.1245 - val_mae: 3.4910
bias 0.004748993
si 0.57742697
rmse 0.0448603
kgeprime [0.70117838]
rmse_95 0.0802619
rmse_99 0.0984533
pearson 0.7968231262893422
pearson_95 0.30121928972428663
pearson_99 0.1276688391761331
rscore 0.6254812204145894
rscore_95 -6.684936771625355
rscore_99 -31.752220222270672
nse [0.62548122]
nse_95 [-6.68493677]
nse_99 [-31.75222022]
kge [0.60607769]
ext_kge_95 [0.02614558]
ext_kge_99 [-0.78889734]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([11544 11568 11592], shape=(3,), dtype=int64) Times out: tf.Tensor(11592, shape=(), dtype=int64)
Times in: tf.Tensor([26096 26120 26144], shape=(3,), dtype=int64) Times out: tf.Tensor(26144, shape=(), dtype=int64)
Times in: tf.Tensor([34558 34582 34606], shape=(3,), dtype=int64) Times out: tf.Tensor(34606, shape=(), dtype=int64)
Times in: tf.Tensor([39800 39824 39848], shape=(3,), dtype=int64) Times out: tf.Tensor(39848, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_305&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_306 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_610 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_611 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_305 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_610 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_305 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_611 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.5622 - mse: 26.5248 - mae: 4.0236 - val_loss: 19.4388 - val_mse: 19.3884 - val_mae: 3.3957
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.4509 - mse: 21.3964 - mae: 3.6390 - val_loss: 18.7013 - val_mse: 18.6406 - val_mae: 3.3316
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.7386 - mse: 20.6759 - mae: 3.5781 - val_loss: 18.4840 - val_mse: 18.4167 - val_mae: 3.3114
Epoch 4/20
4856/4856 [==============================] - 8s 2ms/step - loss: 20.4559 - mse: 20.3873 - mae: 3.5530 - val_loss: 18.2440 - val_mse: 18.1710 - val_mae: 3.2863
Epoch 5/20
4856/4856 [==============================] - 8s 2ms/step - loss: 20.1526 - mse: 20.0781 - mae: 3.5233 - val_loss: 18.1219 - val_mse: 18.0432 - val_mae: 3.2799
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 8s 2ms/step - loss: 19.9911 - mse: 19.9116 - mae: 3.5116 - val_loss: 18.2290 - val_mse: 18.1463 - val_mae: 3.2935
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.8800 - mse: 19.7968 - mae: 3.5000 - val_loss: 18.0844 - val_mse: 17.9983 - val_mae: 3.2787
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.7612 - mse: 19.6746 - mae: 3.4883 - val_loss: 18.1756 - val_mse: 18.0865 - val_mae: 3.2903
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.6410 - mse: 19.5514 - mae: 3.4783 - val_loss: 18.0042 - val_mse: 17.9123 - val_mae: 3.2739
Epoch 10/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.5053 - mse: 19.4130 - mae: 3.4677 - val_loss: 17.9944 - val_mse: 17.9002 - val_mae: 3.2724
Epoch 11/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.4174 - mse: 19.3229 - mae: 3.4607 - val_loss: 17.9712 - val_mse: 17.8750 - val_mae: 3.2724
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3852 - mse: 19.2886 - mae: 3.4553 - val_loss: 18.0290 - val_mse: 17.9307 - val_mae: 3.2836
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2850 - mse: 19.1864 - mae: 3.4454 - val_loss: 17.9635 - val_mse: 17.8633 - val_mae: 3.2724
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.1812 - mse: 19.0806 - mae: 3.4371 - val_loss: 17.8827 - val_mse: 17.7804 - val_mae: 3.2671
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.1832 - mse: 19.0804 - mae: 3.4364 - val_loss: 17.8517 - val_mse: 17.7474 - val_mae: 3.2646
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.0456 - mse: 18.9409 - mae: 3.4237 - val_loss: 17.7580 - val_mse: 17.6519 - val_mae: 3.2565
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.9536 - mse: 18.8470 - mae: 3.4167 - val_loss: 17.7485 - val_mse: 17.6405 - val_mae: 3.2560
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.9141 - mse: 18.8057 - mae: 3.4119 - val_loss: 17.7785 - val_mse: 17.6689 - val_mae: 3.2606
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.8543 - mse: 18.7441 - mae: 3.4107 - val_loss: 17.6877 - val_mse: 17.5762 - val_mae: 3.2515
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.7545 - mse: 18.6425 - mae: 3.3994 - val_loss: 17.7949 - val_mse: 17.6817 - val_mae: 3.2618
bias 0.003025545
si 0.57271314
rmse 0.042049576
kgeprime [0.73887832]
rmse_95 0.075886995
rmse_99 0.08582593
pearson 0.7990925094571308
pearson_95 0.4734465742136489
pearson_99 0.2773484378352886
rscore 0.6363133964036489
rscore_95 -5.566638245888676
rscore_99 -15.620565833609195
nse [0.6363134]
nse_95 [-5.56663825]
nse_99 [-15.62056583]
kge [0.67305212]
ext_kge_95 [0.22530128]
ext_kge_99 [-0.16354218]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([11996 12020 12044], shape=(3,), dtype=int64) Times out: tf.Tensor(12044, shape=(), dtype=int64)
Times in: tf.Tensor([15251 15275 15299], shape=(3,), dtype=int64) Times out: tf.Tensor(15299, shape=(), dtype=int64)
Times in: tf.Tensor([13869 13893 13917], shape=(3,), dtype=int64) Times out: tf.Tensor(13917, shape=(), dtype=int64)
Times in: tf.Tensor([3921 3945 3969], shape=(3,), dtype=int64) Times out: tf.Tensor(3969, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_306&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_307 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_612 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_613 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_306 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_612 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_306 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_613 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.1430 - mse: 26.0971 - mae: 3.9677 - val_loss: 23.1020 - val_mse: 23.0431 - val_mae: 3.8347
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.2046 - mse: 21.1421 - mae: 3.5820 - val_loss: 22.3664 - val_mse: 22.2996 - val_mae: 3.7674
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.5793 - mse: 20.5096 - mae: 3.5331 - val_loss: 21.9971 - val_mse: 21.9235 - val_mae: 3.7339
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0768 - mse: 20.0013 - mae: 3.4942 - val_loss: 21.8683 - val_mse: 21.7902 - val_mae: 3.7194
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.8944 - mse: 19.8149 - mae: 3.4773 - val_loss: 21.8373 - val_mse: 21.7558 - val_mae: 3.7161
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6804 - mse: 19.5978 - mae: 3.4624 - val_loss: 22.0115 - val_mse: 21.9273 - val_mae: 3.7253
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5402 - mse: 19.4551 - mae: 3.4486 - val_loss: 21.8376 - val_mse: 21.7514 - val_mae: 3.7067
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4537 - mse: 19.3667 - mae: 3.4441 - val_loss: 21.8624 - val_mse: 21.7744 - val_mae: 3.7095
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3380 - mse: 19.2492 - mae: 3.4330 - val_loss: 22.0612 - val_mse: 21.9715 - val_mae: 3.7254
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2933 - mse: 19.2028 - mae: 3.4264 - val_loss: 21.4832 - val_mse: 21.3918 - val_mae: 3.6749
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2113 - mse: 19.1193 - mae: 3.4235 - val_loss: 21.5898 - val_mse: 21.4970 - val_mae: 3.6850
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1776 - mse: 19.0841 - mae: 3.4173 - val_loss: 21.4725 - val_mse: 21.3782 - val_mae: 3.6753
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.0510 - mse: 18.9559 - mae: 3.4047 - val_loss: 21.4771 - val_mse: 21.3810 - val_mae: 3.6747
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9917 - mse: 18.8950 - mae: 3.4053 - val_loss: 21.3165 - val_mse: 21.2189 - val_mae: 3.6633
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9696 - mse: 18.8711 - mae: 3.4012 - val_loss: 21.3655 - val_mse: 21.2662 - val_mae: 3.6646
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8011 - mse: 18.7010 - mae: 3.3873 - val_loss: 21.2100 - val_mse: 21.1090 - val_mae: 3.6504
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8502 - mse: 18.7484 - mae: 3.3885 - val_loss: 21.3190 - val_mse: 21.2163 - val_mae: 3.6583
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7345 - mse: 18.6310 - mae: 3.3799 - val_loss: 21.2654 - val_mse: 21.1610 - val_mae: 3.6553
Epoch 19/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 18.6944 - mse: 18.5892 - mae: 3.3747 - val_loss: 21.1229 - val_mse: 21.0168 - val_mae: 3.6408
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6094 - mse: 18.5024 - mae: 3.3700 - val_loss: 21.4490 - val_mse: 21.3409 - val_mae: 3.6663
bias 0.002993172
si 0.6473929
rmse 0.046196245
kgeprime [0.70376115]
rmse_95 0.075287
rmse_99 0.09689305
pearson 0.7399017214866971
pearson_95 0.349218921927945
pearson_99 0.27542746096924103
rscore 0.536409241906618
rscore_95 -7.183276060406248
rscore_99 -22.066366441472063
nse [0.53640924]
nse_95 [-7.18327606]
nse_99 [-22.06636644]
kge [0.66241968]
ext_kge_95 [0.04410231]
ext_kge_99 [-0.61194463]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([158031 158055 158079], shape=(3,), dtype=int64) Times out: tf.Tensor(158079, shape=(), dtype=int64)
Times in: tf.Tensor([140264 140288 140312], shape=(3,), dtype=int64) Times out: tf.Tensor(140312, shape=(), dtype=int64)
Times in: tf.Tensor([74464 74488 74512], shape=(3,), dtype=int64) Times out: tf.Tensor(74512, shape=(), dtype=int64)
Times in: tf.Tensor([150620 150644 150668], shape=(3,), dtype=int64) Times out: tf.Tensor(150668, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_307&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_308 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_614 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_615 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_307 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_614 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_307 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_615 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 9s 2ms/step - loss: 24.5688 - mse: 24.5207 - mae: 3.8603 - val_loss: 17.9543 - val_mse: 17.8971 - val_mae: 3.3356
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8536 - mse: 20.7904 - mae: 3.5705 - val_loss: 17.5660 - val_mse: 17.4975 - val_mae: 3.3027
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.2677 - mse: 20.1952 - mae: 3.5223 - val_loss: 17.3870 - val_mse: 17.3109 - val_mae: 3.2868
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9732 - mse: 19.8941 - mae: 3.4959 - val_loss: 17.0289 - val_mse: 16.9471 - val_mae: 3.2427
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8109 - mse: 19.7268 - mae: 3.4799 - val_loss: 16.9647 - val_mse: 16.8785 - val_mae: 3.2386
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6197 - mse: 19.5315 - mae: 3.4651 - val_loss: 16.8402 - val_mse: 16.7502 - val_mae: 3.2197
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.4652 - mse: 19.3737 - mae: 3.4489 - val_loss: 17.1876 - val_mse: 17.0948 - val_mae: 3.2598
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.4232 - mse: 19.3288 - mae: 3.4441 - val_loss: 16.8403 - val_mse: 16.7445 - val_mae: 3.2258
Epoch 9/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.2991 - mse: 19.2019 - mae: 3.4354 - val_loss: 16.6623 - val_mse: 16.5637 - val_mae: 3.2048
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.1889 - mse: 19.0889 - mae: 3.4234 - val_loss: 16.6384 - val_mse: 16.5371 - val_mae: 3.2041
Epoch 11/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.1302 - mse: 19.0278 - mae: 3.4186 - val_loss: 16.6735 - val_mse: 16.5698 - val_mae: 3.2076
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0673 - mse: 18.9621 - mae: 3.4161 - val_loss: 16.6135 - val_mse: 16.5069 - val_mae: 3.2077
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.9345 - mse: 18.8266 - mae: 3.4024 - val_loss: 16.5353 - val_mse: 16.4259 - val_mae: 3.1927
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.9152 - mse: 18.8043 - mae: 3.3994 - val_loss: 16.6036 - val_mse: 16.4914 - val_mae: 3.2064
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8009 - mse: 18.6871 - mae: 3.3880 - val_loss: 16.5299 - val_mse: 16.4146 - val_mae: 3.1971
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.7149 - mse: 18.5982 - mae: 3.3803 - val_loss: 16.3858 - val_mse: 16.2675 - val_mae: 3.1795
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6741 - mse: 18.5542 - mae: 3.3795 - val_loss: 16.3885 - val_mse: 16.2670 - val_mae: 3.1757
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6432 - mse: 18.5201 - mae: 3.3730 - val_loss: 16.4787 - val_mse: 16.3543 - val_mae: 3.1973
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.4855 - mse: 18.3594 - mae: 3.3613 - val_loss: 16.2823 - val_mse: 16.1546 - val_mae: 3.1711
Epoch 20/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.4902 - mse: 18.3611 - mae: 3.3616 - val_loss: 16.3107 - val_mse: 16.1801 - val_mae: 3.1738
bias 0.00076764834
si 0.589218
rmse 0.040224478
kgeprime [0.7188673]
rmse_95 0.063755915
rmse_99 0.072285086
pearson 0.7795829535868929
pearson_95 0.544443553531209
pearson_99 0.44793707099449676
rscore 0.6071356495660376
rscore_95 -3.754142021438744
rscore_99 -3.6254242460303354
nse [0.60713565]
nse_95 [-3.75414202]
nse_99 [-3.62542425]
kge [0.70139777]
ext_kge_95 [0.37377182]
ext_kge_99 [0.34850483]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -44.49 -44.18 -43.87 ... -37.94 -37.62
  * longitude       (longitude) float32 169.7 170.0 170.3 ... 175.6 175.9 176.2
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -3.22 -3.16 ... 1.184
    vgrd10m         (time, latitude, longitude) float32 2.69 2.888 ... 1.839
    uw2             (time, latitude, longitude) float32 10.37 9.984 ... 1.401
    vw2             (time, latitude, longitude) float32 7.235 8.343 ... 3.384
    wind_magnitude  (time, latitude, longitude) float32 4.195 4.281 ... 2.187
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([83272 83296 83320], shape=(3,), dtype=int64) Times out: tf.Tensor(83320, shape=(), dtype=int64)
Times in: tf.Tensor([31830 31854 31878], shape=(3,), dtype=int64) Times out: tf.Tensor(31878, shape=(), dtype=int64)
Times in: tf.Tensor([142229 142253 142277], shape=(3,), dtype=int64) Times out: tf.Tensor(142277, shape=(), dtype=int64)
Times in: tf.Tensor([9746 9770 9794], shape=(3,), dtype=int64) Times out: tf.Tensor(9794, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_308&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_309 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_616 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_617 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_308 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_616 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_308 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_617 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.1416 - mse: 28.0975 - mae: 4.1538 - val_loss: 21.3430 - val_mse: 21.2900 - val_mae: 3.6302
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2992 - mse: 21.2403 - mae: 3.6241 - val_loss: 19.6744 - val_mse: 19.6106 - val_mae: 3.4648
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5022 - mse: 20.4337 - mae: 3.5501 - val_loss: 19.1744 - val_mse: 19.1024 - val_mae: 3.4484
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1676 - mse: 20.0926 - mae: 3.5205 - val_loss: 18.8261 - val_mse: 18.7484 - val_mae: 3.4135
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9850 - mse: 19.9053 - mae: 3.5002 - val_loss: 18.6505 - val_mse: 18.5693 - val_mae: 3.3998
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8333 - mse: 19.7503 - mae: 3.4872 - val_loss: 18.5018 - val_mse: 18.4173 - val_mae: 3.3868
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.7324 - mse: 19.6466 - mae: 3.4781 - val_loss: 18.4745 - val_mse: 18.3872 - val_mae: 3.3850
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6153 - mse: 19.5266 - mae: 3.4672 - val_loss: 18.4359 - val_mse: 18.3457 - val_mae: 3.3734
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.4966 - mse: 19.4051 - mae: 3.4557 - val_loss: 18.3672 - val_mse: 18.2742 - val_mae: 3.3771
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.4770 - mse: 19.3828 - mae: 3.4522 - val_loss: 18.2909 - val_mse: 18.1950 - val_mae: 3.3622
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3372 - mse: 19.2402 - mae: 3.4388 - val_loss: 18.3587 - val_mse: 18.2603 - val_mae: 3.3745
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2936 - mse: 19.1939 - mae: 3.4336 - val_loss: 18.1860 - val_mse: 18.0850 - val_mae: 3.3555
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2051 - mse: 19.1027 - mae: 3.4278 - val_loss: 18.2809 - val_mse: 18.1769 - val_mae: 3.3696
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.1093 - mse: 19.0043 - mae: 3.4187 - val_loss: 18.0734 - val_mse: 17.9668 - val_mae: 3.3401
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0710 - mse: 18.9634 - mae: 3.4124 - val_loss: 18.1067 - val_mse: 17.9977 - val_mae: 3.3408
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.9798 - mse: 18.8696 - mae: 3.4044 - val_loss: 17.9848 - val_mse: 17.8733 - val_mae: 3.3276
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8799 - mse: 18.7673 - mae: 3.3980 - val_loss: 18.1445 - val_mse: 18.0307 - val_mae: 3.3529
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8028 - mse: 18.6880 - mae: 3.3900 - val_loss: 17.9923 - val_mse: 17.8764 - val_mae: 3.3312
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.7660 - mse: 18.6489 - mae: 3.3878 - val_loss: 18.0273 - val_mse: 17.9093 - val_mae: 3.3328
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6614 - mse: 18.5425 - mae: 3.3780 - val_loss: 17.9362 - val_mse: 17.8163 - val_mae: 3.3246
bias -0.0026523548
si 0.5938934
rmse 0.042209312
kgeprime [0.58341773]
rmse_95 0.07413671
rmse_99 0.111090764
pearson 0.7800277091565816
pearson_95 0.4760659074792216
pearson_99 0.21067749606968303
rscore 0.6055917221856829
rscore_95 -1.7861907743828587
rscore_99 -16.88400107397214
nse [0.60559172]
nse_95 [-1.78619077]
nse_99 [-16.88400107]
kge [0.64281599]
ext_kge_95 [0.37959414]
ext_kge_99 [-0.00501345]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([90366 90390 90414], shape=(3,), dtype=int64) Times out: tf.Tensor(90414, shape=(), dtype=int64)
Times in: tf.Tensor([59691 59715 59739], shape=(3,), dtype=int64) Times out: tf.Tensor(59739, shape=(), dtype=int64)
Times in: tf.Tensor([68987 69011 69035], shape=(3,), dtype=int64) Times out: tf.Tensor(69035, shape=(), dtype=int64)
Times in: tf.Tensor([26559 26583 26607], shape=(3,), dtype=int64) Times out: tf.Tensor(26607, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_309&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_310 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_618 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_619 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_309 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_618 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_309 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_619 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.3234 - mse: 25.2828 - mae: 3.9413 - val_loss: 22.4528 - val_mse: 22.3985 - val_mae: 3.7049
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.9115 - mse: 20.8544 - mae: 3.5954 - val_loss: 21.6171 - val_mse: 21.5545 - val_mae: 3.6299
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.3357 - mse: 20.2715 - mae: 3.5406 - val_loss: 20.9413 - val_mse: 20.8726 - val_mae: 3.5704
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.8441 - mse: 19.7747 - mae: 3.4967 - val_loss: 20.4588 - val_mse: 20.3861 - val_mae: 3.5349
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5564 - mse: 19.4835 - mae: 3.4691 - val_loss: 20.5286 - val_mse: 20.4527 - val_mae: 3.5366
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 19.5023 - mse: 19.4265 - mae: 3.4639 - val_loss: 20.4912 - val_mse: 20.4125 - val_mae: 3.5336
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3206 - mse: 19.2420 - mae: 3.4480 - val_loss: 20.3354 - val_mse: 20.2540 - val_mae: 3.5183
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.2811 - mse: 19.1999 - mae: 3.4432 - val_loss: 20.1209 - val_mse: 20.0367 - val_mae: 3.5008
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1391 - mse: 19.0552 - mae: 3.4297 - val_loss: 20.1137 - val_mse: 20.0271 - val_mae: 3.4927
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1041 - mse: 19.0176 - mae: 3.4264 - val_loss: 20.0170 - val_mse: 19.9277 - val_mae: 3.4890
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.0149 - mse: 18.9261 - mae: 3.4189 - val_loss: 19.9934 - val_mse: 19.9018 - val_mae: 3.4894
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9316 - mse: 18.8403 - mae: 3.4101 - val_loss: 20.0259 - val_mse: 19.9318 - val_mae: 3.4826
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8897 - mse: 18.7958 - mae: 3.4019 - val_loss: 20.0603 - val_mse: 19.9639 - val_mae: 3.4847
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8369 - mse: 18.7405 - mae: 3.4018 - val_loss: 19.9171 - val_mse: 19.8181 - val_mae: 3.4722
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7516 - mse: 18.6525 - mae: 3.3961 - val_loss: 20.0131 - val_mse: 19.9117 - val_mae: 3.4787
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6233 - mse: 18.5219 - mae: 3.3805 - val_loss: 19.7622 - val_mse: 19.6584 - val_mae: 3.4632
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.5921 - mse: 18.4883 - mae: 3.3755 - val_loss: 19.7747 - val_mse: 19.6685 - val_mae: 3.4620
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.5277 - mse: 18.4215 - mae: 3.3679 - val_loss: 19.7428 - val_mse: 19.6343 - val_mae: 3.4571
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.4609 - mse: 18.3524 - mae: 3.3639 - val_loss: 19.7824 - val_mse: 19.6718 - val_mae: 3.4586
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.4028 - mse: 18.2919 - mae: 3.3573 - val_loss: 19.6874 - val_mse: 19.5741 - val_mae: 3.4514
bias 0.0012984738
si 0.5760003
rmse 0.044242673
kgeprime [0.69789399]
rmse_95 0.076681376
rmse_99 0.09484654
pearson 0.7970642279343729
pearson_95 0.39451067361663317
pearson_99 0.24740753415262778
rscore 0.6318006103992884
rscore_95 -5.740438130858998
rscore_99 -24.170719110367344
nse [0.63180061]
nse_95 [-5.74043813]
nse_99 [-24.17071911]
kge [0.66621795]
ext_kge_95 [-0.00902593]
ext_kge_99 [-0.89919624]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([77050 77074 77098], shape=(3,), dtype=int64) Times out: tf.Tensor(77098, shape=(), dtype=int64)
Times in: tf.Tensor([1513 1537 1561], shape=(3,), dtype=int64) Times out: tf.Tensor(1561, shape=(), dtype=int64)
Times in: tf.Tensor([26017 26041 26065], shape=(3,), dtype=int64) Times out: tf.Tensor(26065, shape=(), dtype=int64)
Times in: tf.Tensor([53097 53121 53145], shape=(3,), dtype=int64) Times out: tf.Tensor(53145, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_310&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_311 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_620 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_621 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_310 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_620 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_310 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_621 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 25.4534 - mse: 25.4048 - mae: 3.9442 - val_loss: 18.9572 - val_mse: 18.8901 - val_mae: 3.3621
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.5878 - mse: 20.5154 - mae: 3.5612 - val_loss: 18.2395 - val_mse: 18.1591 - val_mae: 3.3025
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.8633 - mse: 19.7800 - mae: 3.5024 - val_loss: 17.8607 - val_mse: 17.7717 - val_mae: 3.2606
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.5084 - mse: 19.4174 - mae: 3.4733 - val_loss: 17.7736 - val_mse: 17.6783 - val_mae: 3.2522
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3175 - mse: 19.2209 - mae: 3.4550 - val_loss: 17.6922 - val_mse: 17.5919 - val_mae: 3.2401
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2158 - mse: 19.1148 - mae: 3.4395 - val_loss: 17.6732 - val_mse: 17.5692 - val_mae: 3.2411
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.1096 - mse: 19.0052 - mae: 3.4313 - val_loss: 17.6581 - val_mse: 17.5509 - val_mae: 3.2414
Epoch 8/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.9875 - mse: 18.8802 - mae: 3.4199 - val_loss: 17.7002 - val_mse: 17.5904 - val_mae: 3.2479
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.8717 - mse: 18.7616 - mae: 3.4102 - val_loss: 17.6492 - val_mse: 17.5365 - val_mae: 3.2412
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.8395 - mse: 18.7267 - mae: 3.4051 - val_loss: 17.5403 - val_mse: 17.4250 - val_mae: 3.2313
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.6932 - mse: 18.5779 - mae: 3.3945 - val_loss: 17.4961 - val_mse: 17.3783 - val_mae: 3.2251
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.6654 - mse: 18.5473 - mae: 3.3913 - val_loss: 17.3965 - val_mse: 17.2761 - val_mae: 3.2165
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.6222 - mse: 18.5016 - mae: 3.3863 - val_loss: 17.3754 - val_mse: 17.2527 - val_mae: 3.2132
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.5017 - mse: 18.3788 - mae: 3.3773 - val_loss: 17.4377 - val_mse: 17.3128 - val_mae: 3.2263
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.4534 - mse: 18.3279 - mae: 3.3715 - val_loss: 17.3170 - val_mse: 17.1894 - val_mae: 3.2086
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.3767 - mse: 18.2486 - mae: 3.3639 - val_loss: 17.3666 - val_mse: 17.2364 - val_mae: 3.2150
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.3287 - mse: 18.1979 - mae: 3.3573 - val_loss: 17.4432 - val_mse: 17.3104 - val_mae: 3.2250
Epoch 18/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.2357 - mse: 18.1021 - mae: 3.3513 - val_loss: 17.3117 - val_mse: 17.1761 - val_mae: 3.2097
Epoch 19/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 8s 2ms/step - loss: 18.1386 - mse: 18.0023 - mae: 3.3411 - val_loss: 17.1965 - val_mse: 17.0584 - val_mae: 3.1973
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.0742 - mse: 17.9353 - mae: 3.3352 - val_loss: 17.3649 - val_mse: 17.2238 - val_mae: 3.2163
bias -0.0039604055
si 0.5719065
rmse 0.041501608
kgeprime [0.55918102]
rmse_95 0.06844434
rmse_99 0.07326542
pearson 0.8006499070395978
pearson_95 0.502901869336762
pearson_99 0.47421962130481343
rscore 0.6367538857172517
rscore_95 -4.575030700242177
rscore_99 -13.729861375819159
nse [0.63675389]
nse_95 [-4.5750307]
nse_99 [-13.72986138]
kge [0.64523881]
ext_kge_95 [0.20334363]
ext_kge_99 [-0.24310414]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([12479 12503 12527], shape=(3,), dtype=int64) Times out: tf.Tensor(12527, shape=(), dtype=int64)
Times in: tf.Tensor([8703 8727 8751], shape=(3,), dtype=int64) Times out: tf.Tensor(8751, shape=(), dtype=int64)
Times in: tf.Tensor([20209 20233 20257], shape=(3,), dtype=int64) Times out: tf.Tensor(20257, shape=(), dtype=int64)
Times in: tf.Tensor([18003 18027 18051], shape=(3,), dtype=int64) Times out: tf.Tensor(18051, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_311&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_312 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_622 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_623 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_311 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_622 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_311 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_623 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 23.4887 - mse: 23.4487 - mae: 3.7675 - val_loss: 21.8937 - val_mse: 21.8443 - val_mae: 3.7386
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.9003 - mse: 19.8459 - mae: 3.4803 - val_loss: 21.7522 - val_mse: 21.6923 - val_mae: 3.7158
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2431 - mse: 19.1798 - mae: 3.4267 - val_loss: 21.3113 - val_mse: 21.2444 - val_mae: 3.6705
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9598 - mse: 18.8906 - mae: 3.4052 - val_loss: 21.2462 - val_mse: 21.1741 - val_mae: 3.6607
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8280 - mse: 18.7540 - mae: 3.3927 - val_loss: 21.1577 - val_mse: 21.0812 - val_mae: 3.6503
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6642 - mse: 18.5862 - mae: 3.3734 - val_loss: 21.1098 - val_mse: 21.0293 - val_mae: 3.6464
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.5262 - mse: 18.4444 - mae: 3.3617 - val_loss: 21.1984 - val_mse: 21.1144 - val_mae: 3.6491
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.4270 - mse: 18.3418 - mae: 3.3543 - val_loss: 21.0105 - val_mse: 20.9232 - val_mae: 3.6394
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3459 - mse: 18.2575 - mae: 3.3470 - val_loss: 21.0086 - val_mse: 20.9183 - val_mae: 3.6359
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.2424 - mse: 18.1511 - mae: 3.3364 - val_loss: 20.9874 - val_mse: 20.8942 - val_mae: 3.6327
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.1739 - mse: 18.0795 - mae: 3.3311 - val_loss: 20.8547 - val_mse: 20.7585 - val_mae: 3.6177
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.0639 - mse: 17.9666 - mae: 3.3231 - val_loss: 20.7358 - val_mse: 20.6367 - val_mae: 3.6108
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.9823 - mse: 17.8821 - mae: 3.3109 - val_loss: 20.9578 - val_mse: 20.8560 - val_mae: 3.6243
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.9200 - mse: 17.8172 - mae: 3.3075 - val_loss: 20.9253 - val_mse: 20.8209 - val_mae: 3.6244
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.8629 - mse: 17.7572 - mae: 3.3010 - val_loss: 20.8286 - val_mse: 20.7213 - val_mae: 3.6169
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.7444 - mse: 17.6357 - mae: 3.2899 - val_loss: 20.6070 - val_mse: 20.4966 - val_mae: 3.5955
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.6648 - mse: 17.5532 - mae: 3.2830 - val_loss: 20.6309 - val_mse: 20.5178 - val_mae: 3.5974
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.5640 - mse: 17.4494 - mae: 3.2712 - val_loss: 20.4625 - val_mse: 20.3463 - val_mae: 3.5824
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.5096 - mse: 17.3921 - mae: 3.2676 - val_loss: 20.4113 - val_mse: 20.2923 - val_mae: 3.5768
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.5158 - mse: 17.3955 - mae: 3.2674 - val_loss: 20.6260 - val_mse: 20.5041 - val_mae: 3.5930
bias -0.0054858644
si 0.6374992
rmse 0.045281406
kgeprime [0.50018302]
rmse_95 0.06565462
rmse_99 0.08558362
pearson 0.7478527069993393
pearson_95 0.322496273439748
pearson_99 0.23044364176472593
rscore 0.5467103902788935
rscore_95 -5.314111922453956
rscore_99 -17.288781367812152
nse [0.54671039]
nse_95 [-5.31411192]
nse_99 [-17.28878137]
kge [0.59815047]
ext_kge_95 [0.11008784]
ext_kge_99 [-0.26412106]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([117830 117854 117878], shape=(3,), dtype=int64) Times out: tf.Tensor(117878, shape=(), dtype=int64)
Times in: tf.Tensor([126555 126579 126603], shape=(3,), dtype=int64) Times out: tf.Tensor(126603, shape=(), dtype=int64)
Times in: tf.Tensor([81793 81817 81841], shape=(3,), dtype=int64) Times out: tf.Tensor(81841, shape=(), dtype=int64)
Times in: tf.Tensor([148996 149020 149044], shape=(3,), dtype=int64) Times out: tf.Tensor(149044, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_312&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_313 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_624 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_625 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_312 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_624 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_312 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_625 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 27.5404 - mse: 27.4901 - mae: 4.0878 - val_loss: 18.4067 - val_mse: 18.3428 - val_mae: 3.3660
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.7283 - mse: 21.6597 - mae: 3.6449 - val_loss: 17.5632 - val_mse: 17.4904 - val_mae: 3.2995
Epoch 3/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.9567 - mse: 20.8802 - mae: 3.5814 - val_loss: 17.4823 - val_mse: 17.4029 - val_mae: 3.2970
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6366 - mse: 20.5550 - mae: 3.5548 - val_loss: 17.3200 - val_mse: 17.2370 - val_mae: 3.2847
Epoch 5/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.4281 - mse: 20.3435 - mae: 3.5348 - val_loss: 16.9580 - val_mse: 16.8722 - val_mae: 3.2489
Epoch 6/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.2665 - mse: 20.1797 - mae: 3.5211 - val_loss: 17.0087 - val_mse: 16.9212 - val_mae: 3.2539
Epoch 7/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.1691 - mse: 20.0806 - mae: 3.5137 - val_loss: 17.3034 - val_mse: 17.2142 - val_mae: 3.2869
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1097 - mse: 20.0196 - mae: 3.5025 - val_loss: 17.3060 - val_mse: 17.2154 - val_mae: 3.2866
Epoch 9/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.0014 - mse: 19.9102 - mae: 3.4995 - val_loss: 16.5947 - val_mse: 16.5029 - val_mae: 3.2102
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9394 - mse: 19.8470 - mae: 3.4910 - val_loss: 17.0179 - val_mse: 16.9248 - val_mae: 3.2559
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9174 - mse: 19.8236 - mae: 3.4896 - val_loss: 16.6844 - val_mse: 16.5900 - val_mae: 3.2210
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.7804 - mse: 19.6853 - mae: 3.4782 - val_loss: 16.9528 - val_mse: 16.8571 - val_mae: 3.2524
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.7478 - mse: 19.6515 - mae: 3.4733 - val_loss: 16.6900 - val_mse: 16.5932 - val_mae: 3.2206
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6440 - mse: 19.5464 - mae: 3.4641 - val_loss: 16.8155 - val_mse: 16.7173 - val_mae: 3.2338
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5724 - mse: 19.4734 - mae: 3.4588 - val_loss: 16.9335 - val_mse: 16.8339 - val_mae: 3.2489
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5146 - mse: 19.4141 - mae: 3.4540 - val_loss: 16.5594 - val_mse: 16.4584 - val_mae: 3.2039
Epoch 17/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.5010 - mse: 19.3991 - mae: 3.4545 - val_loss: 16.7027 - val_mse: 16.6002 - val_mae: 3.2242
Epoch 18/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.4041 - mse: 19.3008 - mae: 3.4455 - val_loss: 16.5228 - val_mse: 16.4187 - val_mae: 3.2059
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3635 - mse: 19.2586 - mae: 3.4390 - val_loss: 16.7465 - val_mse: 16.6411 - val_mae: 3.2327
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2325 - mse: 19.1261 - mae: 3.4283 - val_loss: 16.4846 - val_mse: 16.3776 - val_mae: 3.2016
bias 0.006186173
si 0.59635055
rmse 0.040469244
kgeprime [0.64477553]
rmse_95 0.070744894
rmse_99 0.07811413
pearson 0.7741125069107554
pearson_95 0.573267624501499
pearson_99 0.4010537321944767
rscore 0.5888753302635358
rscore_95 -5.159604792004375
rscore_99 -6.264302960819219
nse [0.58887533]
nse_95 [-5.15960479]
nse_99 [-6.26430296]
kge [0.56434147]
ext_kge_95 [0.33977825]
ext_kge_99 [0.26836632]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -44.18 -43.87 -43.56 ... -37.94 -37.62
  * longitude       (longitude) float32 172.8 173.1 173.4 ... 179.1 179.4 179.7
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 3.88 2.638 ... 1.874
    vgrd10m         (time, latitude, longitude) float32 4.668 4.499 ... 0.9593
    uw2             (time, latitude, longitude) float32 15.05 6.962 ... 3.511
    vw2             (time, latitude, longitude) float32 21.79 20.24 ... 0.9202
    wind_magnitude  (time, latitude, longitude) float32 6.07 5.216 ... 2.105
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([92374 92398 92422], shape=(3,), dtype=int64) Times out: tf.Tensor(92422, shape=(), dtype=int64)
Times in: tf.Tensor([128823 128847 128871], shape=(3,), dtype=int64) Times out: tf.Tensor(128871, shape=(), dtype=int64)
Times in: tf.Tensor([98351 98375 98399], shape=(3,), dtype=int64) Times out: tf.Tensor(98399, shape=(), dtype=int64)
Times in: tf.Tensor([118953 118977 119001], shape=(3,), dtype=int64) Times out: tf.Tensor(119001, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_313&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_314 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_626 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_627 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_313 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_626 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_313 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_627 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 25.7132 - mse: 25.6653 - mae: 3.9604 - val_loss: 19.6811 - val_mse: 19.6251 - val_mae: 3.4802
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0179 - mse: 21.9574 - mae: 3.6853 - val_loss: 19.2847 - val_mse: 19.2207 - val_mae: 3.4492
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.7054 - mse: 21.6391 - mae: 3.6538 - val_loss: 19.1773 - val_mse: 19.1089 - val_mae: 3.4424
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4537 - mse: 21.3835 - mae: 3.6321 - val_loss: 18.9660 - val_mse: 18.8939 - val_mae: 3.4275
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1307 - mse: 21.0572 - mae: 3.6049 - val_loss: 18.3921 - val_mse: 18.3168 - val_mae: 3.3663
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8619 - mse: 20.7853 - mae: 3.5798 - val_loss: 18.6774 - val_mse: 18.5992 - val_mae: 3.4071
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5680 - mse: 20.4888 - mae: 3.5558 - val_loss: 18.0513 - val_mse: 17.9707 - val_mae: 3.3452
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4152 - mse: 20.3338 - mae: 3.5397 - val_loss: 17.9492 - val_mse: 17.8665 - val_mae: 3.3361
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.2929 - mse: 20.2093 - mae: 3.5300 - val_loss: 17.4273 - val_mse: 17.3425 - val_mae: 3.2800
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1621 - mse: 20.0764 - mae: 3.5177 - val_loss: 17.4844 - val_mse: 17.3977 - val_mae: 3.2917
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.0518 - mse: 19.9643 - mae: 3.5092 - val_loss: 17.3554 - val_mse: 17.2668 - val_mae: 3.2737
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9994 - mse: 19.9102 - mae: 3.5032 - val_loss: 17.4009 - val_mse: 17.3109 - val_mae: 3.2806
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9363 - mse: 19.8455 - mae: 3.4948 - val_loss: 17.2979 - val_mse: 17.2061 - val_mae: 3.2725
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8475 - mse: 19.7550 - mae: 3.4908 - val_loss: 17.3643 - val_mse: 17.2707 - val_mae: 3.2764
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8280 - mse: 19.7338 - mae: 3.4842 - val_loss: 17.3625 - val_mse: 17.2673 - val_mae: 3.2807
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8338 - mse: 19.7378 - mae: 3.4872 - val_loss: 17.3964 - val_mse: 17.2992 - val_mae: 3.2846
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.7037 - mse: 19.6058 - mae: 3.4763 - val_loss: 17.2562 - val_mse: 17.1574 - val_mae: 3.2632
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.7233 - mse: 19.6239 - mae: 3.4768 - val_loss: 17.3119 - val_mse: 17.2116 - val_mae: 3.2750
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6703 - mse: 19.5691 - mae: 3.4741 - val_loss: 17.1781 - val_mse: 17.0759 - val_mae: 3.2617
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6190 - mse: 19.5160 - mae: 3.4688 - val_loss: 17.2444 - val_mse: 17.1405 - val_mae: 3.2694
bias -0.004475283
si 0.5047161
rmse 0.041401073
kgeprime [0.62340516]
rmse_95 0.06728452
rmse_99 0.08622443
pearson 0.8450077110789012
pearson_95 0.4196908784249358
pearson_99 0.6911871508291456
rscore 0.7095937671456036
rscore_95 -3.534844523061108
rscore_99 -15.28647900030829
nse [0.70959377]
nse_95 [-3.53484452]
nse_99 [-15.286479]
kge [0.70686471]
ext_kge_95 [0.31557273]
ext_kge_99 [0.27655398]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([111649 111673 111697], shape=(3,), dtype=int64) Times out: tf.Tensor(111697, shape=(), dtype=int64)
Times in: tf.Tensor([39579 39603 39627], shape=(3,), dtype=int64) Times out: tf.Tensor(39627, shape=(), dtype=int64)
Times in: tf.Tensor([93286 93310 93334], shape=(3,), dtype=int64) Times out: tf.Tensor(93334, shape=(), dtype=int64)
Times in: tf.Tensor([97290 97314 97338], shape=(3,), dtype=int64) Times out: tf.Tensor(97338, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_314&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_315 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_628 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_629 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_314 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_628 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_314 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_629 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.6360 - mse: 26.5862 - mae: 4.0353 - val_loss: 22.9645 - val_mse: 22.9040 - val_mae: 3.7192
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8936 - mse: 21.8278 - mae: 3.6806 - val_loss: 21.8744 - val_mse: 21.8037 - val_mae: 3.6256
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.1702 - mse: 21.0954 - mae: 3.6143 - val_loss: 21.2163 - val_mse: 21.1384 - val_mae: 3.5735
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.6102 - mse: 20.5300 - mae: 3.5672 - val_loss: 20.4362 - val_mse: 20.3547 - val_mae: 3.5048
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.2992 - mse: 20.2164 - mae: 3.5387 - val_loss: 19.9451 - val_mse: 19.8614 - val_mae: 3.4519
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0270 - mse: 19.9423 - mae: 3.5160 - val_loss: 20.2563 - val_mse: 20.1705 - val_mae: 3.4969
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.9094 - mse: 19.8229 - mae: 3.5051 - val_loss: 19.7934 - val_mse: 19.7060 - val_mae: 3.4508
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.7573 - mse: 19.6691 - mae: 3.4876 - val_loss: 19.7283 - val_mse: 19.6392 - val_mae: 3.4449
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5933 - mse: 19.5030 - mae: 3.4768 - val_loss: 19.6203 - val_mse: 19.5289 - val_mae: 3.4358
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4648 - mse: 19.3725 - mae: 3.4641 - val_loss: 19.4161 - val_mse: 19.3229 - val_mae: 3.4117
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3834 - mse: 19.2889 - mae: 3.4560 - val_loss: 19.0544 - val_mse: 18.9591 - val_mae: 3.3799
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2661 - mse: 19.1694 - mae: 3.4485 - val_loss: 19.0573 - val_mse: 18.9598 - val_mae: 3.3736
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1941 - mse: 19.0953 - mae: 3.4397 - val_loss: 18.9214 - val_mse: 18.8220 - val_mae: 3.3641
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1164 - mse: 19.0157 - mae: 3.4320 - val_loss: 19.4064 - val_mse: 19.3049 - val_mae: 3.4229
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.0163 - mse: 18.9137 - mae: 3.4218 - val_loss: 18.9797 - val_mse: 18.8766 - val_mae: 3.3705
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9627 - mse: 18.8584 - mae: 3.4193 - val_loss: 18.9145 - val_mse: 18.8097 - val_mae: 3.3702
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9411 - mse: 18.8350 - mae: 3.4165 - val_loss: 18.9450 - val_mse: 18.8384 - val_mae: 3.3744
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9267 - mse: 18.8187 - mae: 3.4119 - val_loss: 19.1550 - val_mse: 19.0466 - val_mae: 3.3948
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8589 - mse: 18.7491 - mae: 3.4088 - val_loss: 18.8930 - val_mse: 18.7827 - val_mae: 3.3667
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8364 - mse: 18.7248 - mae: 3.4052 - val_loss: 18.8802 - val_mse: 18.7680 - val_mae: 3.3647
bias 0.0033048284
si 0.48955387
rmse 0.043322094
kgeprime [0.78022877]
rmse_95 0.0667297
rmse_99 0.08667074
pearson 0.8610275005690117
pearson_95 0.33447230525716243
pearson_99 -0.3687423938174912
rscore 0.7321908691761883
rscore_95 -5.729526471205747
rscore_99 -33.175756229360715
nse [0.73219087]
nse_95 [-5.72952647]
nse_99 [-33.17575623]
kge [0.70921653]
ext_kge_95 [0.15172294]
ext_kge_99 [-1.23995895]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([51194 51218 51242], shape=(3,), dtype=int64) Times out: tf.Tensor(51242, shape=(), dtype=int64)
Times in: tf.Tensor([42451 42475 42499], shape=(3,), dtype=int64) Times out: tf.Tensor(42499, shape=(), dtype=int64)
Times in: tf.Tensor([20325 20349 20373], shape=(3,), dtype=int64) Times out: tf.Tensor(20373, shape=(), dtype=int64)
Times in: tf.Tensor([60328 60352 60376], shape=(3,), dtype=int64) Times out: tf.Tensor(60376, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_315&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_316 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_630 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_631 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_315 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_630 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_315 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_631 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 25.8859 - mse: 25.8378 - mae: 3.9887 - val_loss: 20.0912 - val_mse: 20.0347 - val_mae: 3.4895
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.8377 - mse: 21.7770 - mae: 3.6822 - val_loss: 19.8332 - val_mse: 19.7680 - val_mae: 3.4653
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.5086 - mse: 21.4417 - mae: 3.6526 - val_loss: 19.5646 - val_mse: 19.4944 - val_mae: 3.4385
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.1081 - mse: 21.0366 - mae: 3.6164 - val_loss: 19.1882 - val_mse: 19.1135 - val_mae: 3.3963
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.9225 - mse: 20.8465 - mae: 3.5994 - val_loss: 18.9115 - val_mse: 18.8322 - val_mae: 3.3726
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.5351 - mse: 20.4544 - mae: 3.5666 - val_loss: 18.5671 - val_mse: 18.4832 - val_mae: 3.3456
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2091 - mse: 20.1240 - mae: 3.5385 - val_loss: 18.3209 - val_mse: 18.2330 - val_mae: 3.3241
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.0148 - mse: 19.9260 - mae: 3.5204 - val_loss: 18.1325 - val_mse: 18.0414 - val_mae: 3.3030
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.8598 - mse: 19.7678 - mae: 3.5086 - val_loss: 17.9166 - val_mse: 17.8223 - val_mae: 3.2799
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.7609 - mse: 19.6656 - mae: 3.4997 - val_loss: 17.8265 - val_mse: 17.7292 - val_mae: 3.2709
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.6372 - mse: 19.5390 - mae: 3.4876 - val_loss: 17.7289 - val_mse: 17.6287 - val_mae: 3.2627
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.5613 - mse: 19.4606 - mae: 3.4809 - val_loss: 17.8252 - val_mse: 17.7229 - val_mae: 3.2862
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.4882 - mse: 19.3851 - mae: 3.4728 - val_loss: 17.6354 - val_mse: 17.5306 - val_mae: 3.2538
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3849 - mse: 19.2796 - mae: 3.4629 - val_loss: 17.6136 - val_mse: 17.5067 - val_mae: 3.2465
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3774 - mse: 19.2701 - mae: 3.4605 - val_loss: 17.6025 - val_mse: 17.4936 - val_mae: 3.2540
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3949 - mse: 19.2857 - mae: 3.4630 - val_loss: 17.5578 - val_mse: 17.4472 - val_mae: 3.2488
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2444 - mse: 19.1338 - mae: 3.4505 - val_loss: 17.5762 - val_mse: 17.4643 - val_mae: 3.2507
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2787 - mse: 19.1665 - mae: 3.4542 - val_loss: 17.5738 - val_mse: 17.4604 - val_mae: 3.2515
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.1421 - mse: 19.0284 - mae: 3.4414 - val_loss: 17.5855 - val_mse: 17.4705 - val_mae: 3.2558
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.1728 - mse: 19.0577 - mae: 3.4435 - val_loss: 17.6327 - val_mse: 17.5166 - val_mae: 3.2448
bias -0.0027976795
si 0.5051759
rmse 0.041852783
kgeprime [0.69192264]
rmse_95 0.06611535
rmse_99 0.1116413
pearson 0.8456179705885126
pearson_95 0.20194570243293092
pearson_99 -0.11876618552020587
rscore 0.7136473920531181
rscore_95 -2.778428638436495
rscore_99 -21.305866732058266
nse [0.71364739]
nse_95 [-2.77842864]
nse_99 [-21.30586673]
kge [0.75013512]
ext_kge_95 [0.15211705]
ext_kge_99 [-0.8927248]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([8129 8153 8177], shape=(3,), dtype=int64) Times out: tf.Tensor(8177, shape=(), dtype=int64)
Times in: tf.Tensor([19495 19519 19543], shape=(3,), dtype=int64) Times out: tf.Tensor(19543, shape=(), dtype=int64)
Times in: tf.Tensor([11950 11974 11998], shape=(3,), dtype=int64) Times out: tf.Tensor(11998, shape=(), dtype=int64)
Times in: tf.Tensor([29507 29531 29555], shape=(3,), dtype=int64) Times out: tf.Tensor(29555, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_316&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_317 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_632 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_633 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_316 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_632 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_316 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_633 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.6293 - mse: 25.5847 - mae: 3.9464 - val_loss: 22.6494 - val_mse: 22.5977 - val_mae: 3.7796
Epoch 2/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 21.4147 - mse: 21.3593 - mae: 3.6250 - val_loss: 22.1691 - val_mse: 22.1103 - val_mae: 3.7224
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.8633 - mse: 20.8018 - mae: 3.5814 - val_loss: 22.2490 - val_mse: 22.1848 - val_mae: 3.7343
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.5915 - mse: 20.5251 - mae: 3.5562 - val_loss: 21.8982 - val_mse: 21.8294 - val_mae: 3.6993
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.2746 - mse: 20.2038 - mae: 3.5298 - val_loss: 21.4857 - val_mse: 21.4126 - val_mae: 3.6589
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0108 - mse: 19.9358 - mae: 3.5046 - val_loss: 21.0495 - val_mse: 20.9725 - val_mae: 3.6166
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.8347 - mse: 19.7559 - mae: 3.4896 - val_loss: 21.3306 - val_mse: 21.2499 - val_mae: 3.6562
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6056 - mse: 19.5233 - mae: 3.4685 - val_loss: 20.8346 - val_mse: 20.7507 - val_mae: 3.6062
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4361 - mse: 19.3508 - mae: 3.4531 - val_loss: 20.6935 - val_mse: 20.6068 - val_mae: 3.5929
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3226 - mse: 19.2346 - mae: 3.4429 - val_loss: 20.6955 - val_mse: 20.6062 - val_mae: 3.5951
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2942 - mse: 19.2038 - mae: 3.4383 - val_loss: 20.8448 - val_mse: 20.7535 - val_mae: 3.6160
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1948 - mse: 19.1021 - mae: 3.4313 - val_loss: 20.5930 - val_mse: 20.4994 - val_mae: 3.5896
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1276 - mse: 19.0328 - mae: 3.4247 - val_loss: 20.7854 - val_mse: 20.6897 - val_mae: 3.6118
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.0464 - mse: 18.9497 - mae: 3.4133 - val_loss: 20.3905 - val_mse: 20.2928 - val_mae: 3.5657
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9912 - mse: 18.8924 - mae: 3.4064 - val_loss: 20.4246 - val_mse: 20.3249 - val_mae: 3.5733
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9719 - mse: 18.8714 - mae: 3.4067 - val_loss: 20.2241 - val_mse: 20.1228 - val_mae: 3.5506
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8976 - mse: 18.7952 - mae: 3.4016 - val_loss: 20.2042 - val_mse: 20.1010 - val_mae: 3.5501
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9275 - mse: 18.8232 - mae: 3.4026 - val_loss: 20.0121 - val_mse: 19.9069 - val_mae: 3.5271
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7739 - mse: 18.6678 - mae: 3.3877 - val_loss: 20.3813 - val_mse: 20.2744 - val_mae: 3.5700
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7568 - mse: 18.6490 - mae: 3.3901 - val_loss: 20.2540 - val_mse: 20.1453 - val_mae: 3.5555
bias 0.004626032
si 0.54996836
rmse 0.044883538
kgeprime [0.73832718]
rmse_95 0.06689509
rmse_99 0.082110636
pearson 0.8159006024622617
pearson_95 0.3878244244316979
pearson_99 0.015213489971459422
rscore 0.6606150873186636
rscore_95 -6.678837462880258
rscore_99 -34.77149342047079
nse [0.66061509]
nse_95 [-6.67883746]
nse_99 [-34.77149342]
kge [0.70359222]
ext_kge_95 [0.13120179]
ext_kge_99 [-0.80328812]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([139366 139390 139414], shape=(3,), dtype=int64) Times out: tf.Tensor(139414, shape=(), dtype=int64)
Times in: tf.Tensor([65432 65456 65480], shape=(3,), dtype=int64) Times out: tf.Tensor(65480, shape=(), dtype=int64)
Times in: tf.Tensor([193251 193275 193299], shape=(3,), dtype=int64) Times out: tf.Tensor(193299, shape=(), dtype=int64)
Times in: tf.Tensor([55145 55169 55193], shape=(3,), dtype=int64) Times out: tf.Tensor(55193, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_317&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_318 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_634 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_635 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_317 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_634 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_317 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_635 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 27.3012 - mse: 27.2499 - mae: 4.0669 - val_loss: 20.2488 - val_mse: 20.1886 - val_mae: 3.6106
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6004 - mse: 22.5341 - mae: 3.7164 - val_loss: 19.2171 - val_mse: 19.1459 - val_mae: 3.5144
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0747 - mse: 22.0003 - mae: 3.6713 - val_loss: 19.2834 - val_mse: 19.2063 - val_mae: 3.5274
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.8496 - mse: 21.7708 - mae: 3.6512 - val_loss: 18.7099 - val_mse: 18.6294 - val_mae: 3.4657
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5991 - mse: 21.5175 - mae: 3.6265 - val_loss: 18.7834 - val_mse: 18.7005 - val_mae: 3.4831
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.3585 - mse: 21.2746 - mae: 3.6048 - val_loss: 18.5738 - val_mse: 18.4884 - val_mae: 3.4703
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.0045 - mse: 20.9180 - mae: 3.5709 - val_loss: 17.9766 - val_mse: 17.8887 - val_mae: 3.4163
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7721 - mse: 20.6834 - mae: 3.5510 - val_loss: 17.6498 - val_mse: 17.5601 - val_mae: 3.3907
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5334 - mse: 20.4429 - mae: 3.5337 - val_loss: 17.5543 - val_mse: 17.4629 - val_mae: 3.3847
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6151 - mse: 20.5231 - mae: 3.5379 - val_loss: 17.5370 - val_mse: 17.4440 - val_mae: 3.3880
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4051 - mse: 20.3114 - mae: 3.5201 - val_loss: 17.3556 - val_mse: 17.2611 - val_mae: 3.3663
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3850 - mse: 20.2898 - mae: 3.5178 - val_loss: 17.1871 - val_mse: 17.0913 - val_mae: 3.3515
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3058 - mse: 20.2096 - mae: 3.5051 - val_loss: 17.1800 - val_mse: 17.0830 - val_mae: 3.3515
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.2749 - mse: 20.1773 - mae: 3.5090 - val_loss: 17.2023 - val_mse: 17.1039 - val_mae: 3.3519
Epoch 15/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.1342 - mse: 20.0353 - mae: 3.4993 - val_loss: 17.2786 - val_mse: 17.1790 - val_mae: 3.3658
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1865 - mse: 20.0863 - mae: 3.4986 - val_loss: 17.5543 - val_mse: 17.4533 - val_mae: 3.3925
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.0545 - mse: 19.9528 - mae: 3.4926 - val_loss: 16.9431 - val_mse: 16.8407 - val_mae: 3.3242
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.0825 - mse: 19.9794 - mae: 3.4945 - val_loss: 17.2402 - val_mse: 17.1362 - val_mae: 3.3526
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9744 - mse: 19.8701 - mae: 3.4812 - val_loss: 17.3223 - val_mse: 17.2174 - val_mae: 3.3644
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9491 - mse: 19.8434 - mae: 3.4821 - val_loss: 16.8913 - val_mse: 16.7853 - val_mae: 3.3178
bias 0.0035849388
si 0.50675744
rmse 0.040969875
kgeprime [0.78365573]
rmse_95 0.059003063
rmse_99 0.072660446
pearson 0.8426908671645472
pearson_95 0.7008328707990804
pearson_99 0.6184389464009005
rscore 0.7066279528902732
rscore_95 -2.1786077255120024
rscore_99 -7.352283706375291
nse [0.70662795]
nse_95 [-2.17860773]
nse_99 [-7.35228371]
kge [0.71947851]
ext_kge_95 [0.57621343]
ext_kge_99 [0.43608611]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -44.18 -43.87 -43.56 ... -37.62 -37.31
  * longitude       (longitude) float32 169.7 170.0 170.3 ... 175.6 175.9 176.2
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -1.818 -2.589 ... 0.6046
    vgrd10m         (time, latitude, longitude) float32 1.889 2.329 ... 2.408
    uw2             (time, latitude, longitude) float32 3.305 6.705 ... 0.3656
    vw2             (time, latitude, longitude) float32 3.567 5.423 ... 5.797
    wind_magnitude  (time, latitude, longitude) float32 2.622 3.482 ... 2.483
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([66035 66059 66083], shape=(3,), dtype=int64) Times out: tf.Tensor(66083, shape=(), dtype=int64)
Times in: tf.Tensor([51678 51702 51726], shape=(3,), dtype=int64) Times out: tf.Tensor(51726, shape=(), dtype=int64)
Times in: tf.Tensor([25460 25484 25508], shape=(3,), dtype=int64) Times out: tf.Tensor(25508, shape=(), dtype=int64)
Times in: tf.Tensor([147113 147137 147161], shape=(3,), dtype=int64) Times out: tf.Tensor(147161, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_318&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_319 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_636 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_637 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_318 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_636 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_318 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_637 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 25.4611 - mse: 25.4147 - mae: 3.9213 - val_loss: 21.8922 - val_mse: 21.8330 - val_mae: 3.6409
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6858 - mse: 20.6207 - mae: 3.5560 - val_loss: 20.6216 - val_mse: 20.5513 - val_mae: 3.5222
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1743 - mse: 20.1004 - mae: 3.5086 - val_loss: 20.1077 - val_mse: 20.0299 - val_mae: 3.4692
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9246 - mse: 19.8437 - mae: 3.4868 - val_loss: 19.9427 - val_mse: 19.8584 - val_mae: 3.4717
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6655 - mse: 19.5787 - mae: 3.4647 - val_loss: 19.4321 - val_mse: 19.3425 - val_mae: 3.4191
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5754 - mse: 19.4838 - mae: 3.4538 - val_loss: 19.3359 - val_mse: 19.2419 - val_mae: 3.4197
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.4006 - mse: 19.3048 - mae: 3.4384 - val_loss: 19.1151 - val_mse: 19.0172 - val_mae: 3.3993
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2252 - mse: 19.1261 - mae: 3.4237 - val_loss: 19.0915 - val_mse: 18.9906 - val_mae: 3.4082
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0954 - mse: 18.9929 - mae: 3.4129 - val_loss: 18.7231 - val_mse: 18.6188 - val_mae: 3.3674
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0067 - mse: 18.9013 - mae: 3.4044 - val_loss: 18.8051 - val_mse: 18.6981 - val_mae: 3.3862
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8659 - mse: 18.7580 - mae: 3.3868 - val_loss: 18.5053 - val_mse: 18.3960 - val_mae: 3.3499
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8560 - mse: 18.7460 - mae: 3.3929 - val_loss: 18.4593 - val_mse: 18.3480 - val_mae: 3.3542
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8245 - mse: 18.7124 - mae: 3.3876 - val_loss: 18.3038 - val_mse: 18.1908 - val_mae: 3.3397
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6671 - mse: 18.5533 - mae: 3.3753 - val_loss: 18.3585 - val_mse: 18.2435 - val_mae: 3.3511
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6779 - mse: 18.5623 - mae: 3.3736 - val_loss: 18.1854 - val_mse: 18.0687 - val_mae: 3.3260
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6329 - mse: 18.5155 - mae: 3.3696 - val_loss: 18.4161 - val_mse: 18.2975 - val_mae: 3.3672
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.5193 - mse: 18.4001 - mae: 3.3601 - val_loss: 18.0815 - val_mse: 17.9611 - val_mae: 3.3304
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.4522 - mse: 18.3311 - mae: 3.3519 - val_loss: 18.0791 - val_mse: 17.9569 - val_mae: 3.3335
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.3245 - mse: 18.2014 - mae: 3.3426 - val_loss: 17.8748 - val_mse: 17.7505 - val_mae: 3.3030
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.2811 - mse: 18.1559 - mae: 3.3394 - val_loss: 17.9189 - val_mse: 17.7927 - val_mae: 3.3186
bias -0.004320797
si 0.5720063
rmse 0.04218135
kgeprime [0.54081316]
rmse_95 0.07524415
rmse_99 0.112693325
pearson 0.7983177452025776
pearson_95 0.6346251838368049
pearson_99 0.533126243190544
rscore 0.6304815709496727
rscore_95 -1.0089326422141394
rscore_99 -8.264665986788104
nse [0.63048157]
nse_95 [-1.00893264]
nse_99 [-8.26466599]
kge [0.62688528]
ext_kge_95 [0.50416717]
ext_kge_99 [0.26286933]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([107927 107951 107975], shape=(3,), dtype=int64) Times out: tf.Tensor(107975, shape=(), dtype=int64)
Times in: tf.Tensor([83451 83475 83499], shape=(3,), dtype=int64) Times out: tf.Tensor(83499, shape=(), dtype=int64)
Times in: tf.Tensor([83770 83794 83818], shape=(3,), dtype=int64) Times out: tf.Tensor(83818, shape=(), dtype=int64)
Times in: tf.Tensor([15573 15597 15621], shape=(3,), dtype=int64) Times out: tf.Tensor(15621, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_319&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_320 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_638 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_639 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_319 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_638 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_319 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_639 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 24.3680 - mse: 24.3226 - mae: 3.8556 - val_loss: 22.3556 - val_mse: 22.2934 - val_mae: 3.6996
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.2694 - mse: 20.2044 - mae: 3.5294 - val_loss: 21.5557 - val_mse: 21.4819 - val_mae: 3.6267
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5521 - mse: 19.4762 - mae: 3.4700 - val_loss: 20.7660 - val_mse: 20.6823 - val_mae: 3.5572
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1044 - mse: 19.0196 - mae: 3.4292 - val_loss: 20.7064 - val_mse: 20.6150 - val_mae: 3.5547
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.8931 - mse: 18.8016 - mae: 3.4095 - val_loss: 20.5230 - val_mse: 20.4261 - val_mae: 3.5365
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7622 - mse: 18.6653 - mae: 3.3958 - val_loss: 20.3341 - val_mse: 20.2320 - val_mae: 3.5200
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6047 - mse: 18.5030 - mae: 3.3796 - val_loss: 20.3437 - val_mse: 20.2371 - val_mae: 3.5218
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3981 - mse: 18.2920 - mae: 3.3602 - val_loss: 20.1266 - val_mse: 20.0157 - val_mae: 3.4994
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3607 - mse: 18.2505 - mae: 3.3560 - val_loss: 20.2245 - val_mse: 20.1095 - val_mae: 3.5120
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.2521 - mse: 18.1378 - mae: 3.3470 - val_loss: 20.1193 - val_mse: 20.0005 - val_mae: 3.4951
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.1583 - mse: 18.0401 - mae: 3.3369 - val_loss: 19.8583 - val_mse: 19.7356 - val_mae: 3.4767
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.0744 - mse: 17.9524 - mae: 3.3258 - val_loss: 19.8506 - val_mse: 19.7243 - val_mae: 3.4739
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.9299 - mse: 17.8045 - mae: 3.3115 - val_loss: 19.7856 - val_mse: 19.6562 - val_mae: 3.4691
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.8910 - mse: 17.7624 - mae: 3.3079 - val_loss: 19.7882 - val_mse: 19.6557 - val_mae: 3.4742
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.7586 - mse: 17.6264 - mae: 3.2935 - val_loss: 19.7294 - val_mse: 19.5934 - val_mae: 3.4646
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.6699 - mse: 17.5342 - mae: 3.2857 - val_loss: 19.5697 - val_mse: 19.4301 - val_mae: 3.4513
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.6485 - mse: 17.5096 - mae: 3.2825 - val_loss: 19.4789 - val_mse: 19.3365 - val_mae: 3.4370
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.5388 - mse: 17.3967 - mae: 3.2757 - val_loss: 19.5411 - val_mse: 19.3955 - val_mae: 3.4448
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.4610 - mse: 17.3159 - mae: 3.2654 - val_loss: 19.4344 - val_mse: 19.2862 - val_mae: 3.4379
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.4157 - mse: 17.2676 - mae: 3.2597 - val_loss: 19.3453 - val_mse: 19.1942 - val_mae: 3.4293
bias -0.0026718623
si 0.55984336
rmse 0.04381116
kgeprime [0.62955619]
rmse_95 0.0730344
rmse_99 0.080221154
pearson 0.8079844376873689
pearson_95 0.5886358527732256
pearson_99 0.44617328424421915
rscore 0.6506733062933097
rscore_95 -3.14180415986094
rscore_99 -7.916391590780133
nse [0.65067331]
nse_95 [-3.14180416]
nse_99 [-7.91639159]
kge [0.68746574]
ext_kge_95 [0.11637938]
ext_kge_99 [-0.66071648]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([62991 63015 63039], shape=(3,), dtype=int64) Times out: tf.Tensor(63039, shape=(), dtype=int64)
Times in: tf.Tensor([20445 20469 20493], shape=(3,), dtype=int64) Times out: tf.Tensor(20493, shape=(), dtype=int64)
Times in: tf.Tensor([74820 74844 74868], shape=(3,), dtype=int64) Times out: tf.Tensor(74868, shape=(), dtype=int64)
Times in: tf.Tensor([43768 43792 43816], shape=(3,), dtype=int64) Times out: tf.Tensor(43816, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_320&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_321 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_640 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_641 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_320 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_640 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_320 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_641 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.0560 - mse: 26.0049 - mae: 3.9792 - val_loss: 18.9835 - val_mse: 18.9142 - val_mae: 3.3775
Epoch 2/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 21.3041 - mse: 21.2306 - mae: 3.6114 - val_loss: 18.3494 - val_mse: 18.2672 - val_mae: 3.3195
Epoch 3/20
4856/4856 [==============================] - 8s 2ms/step - loss: 20.6168 - mse: 20.5323 - mae: 3.5492 - val_loss: 18.1712 - val_mse: 18.0793 - val_mae: 3.3164
Epoch 4/20
4856/4856 [==============================] - 8s 2ms/step - loss: 20.2845 - mse: 20.1910 - mae: 3.5226 - val_loss: 17.9968 - val_mse: 17.8969 - val_mae: 3.3062
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.0693 - mse: 19.9687 - mae: 3.5064 - val_loss: 17.9153 - val_mse: 17.8097 - val_mae: 3.3026
Epoch 6/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.8141 - mse: 19.7081 - mae: 3.4831 - val_loss: 17.8951 - val_mse: 17.7852 - val_mae: 3.3039
Epoch 7/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.5755 - mse: 19.4656 - mae: 3.4622 - val_loss: 17.5857 - val_mse: 17.4723 - val_mae: 3.2659
Epoch 8/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.4158 - mse: 19.3028 - mae: 3.4461 - val_loss: 17.5783 - val_mse: 17.4625 - val_mae: 3.2654
Epoch 9/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.3201 - mse: 19.2048 - mae: 3.4374 - val_loss: 17.5870 - val_mse: 17.4698 - val_mae: 3.2697
Epoch 10/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.1837 - mse: 19.0667 - mae: 3.4287 - val_loss: 17.4792 - val_mse: 17.3598 - val_mae: 3.2517
Epoch 11/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.0912 - mse: 18.9723 - mae: 3.4192 - val_loss: 17.5462 - val_mse: 17.4256 - val_mae: 3.2608
Epoch 12/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.0292 - mse: 18.9092 - mae: 3.4140 - val_loss: 17.3979 - val_mse: 17.2762 - val_mae: 3.2325
Epoch 13/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.9503 - mse: 18.8288 - mae: 3.4042 - val_loss: 17.4321 - val_mse: 17.3089 - val_mae: 3.2482
Epoch 14/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.9032 - mse: 18.7803 - mae: 3.4024 - val_loss: 17.3487 - val_mse: 17.2245 - val_mae: 3.2287
Epoch 15/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.8551 - mse: 18.7310 - mae: 3.3945 - val_loss: 17.4162 - val_mse: 17.2907 - val_mae: 3.2425
Epoch 16/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.7176 - mse: 18.5922 - mae: 3.3872 - val_loss: 17.2875 - val_mse: 17.1606 - val_mae: 3.2238
Epoch 17/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.6236 - mse: 18.4968 - mae: 3.3780 - val_loss: 17.3561 - val_mse: 17.2278 - val_mae: 3.2394
Epoch 18/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.5476 - mse: 18.4192 - mae: 3.3699 - val_loss: 17.3021 - val_mse: 17.1720 - val_mae: 3.2313
Epoch 19/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.5030 - mse: 18.3729 - mae: 3.3665 - val_loss: 17.2036 - val_mse: 17.0722 - val_mae: 3.2207
Epoch 20/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.4195 - mse: 18.2879 - mae: 3.3540 - val_loss: 17.2444 - val_mse: 17.1114 - val_mae: 3.2261
bias -0.0044467305
si 0.572123
rmse 0.041365955
kgeprime [0.54901102]
rmse_95 0.066171534
rmse_99 0.06775732
pearson 0.7998347956771991
pearson_95 0.5033518899937302
pearson_99 0.37669832319774155
rscore 0.6352627459448581
rscore_95 -4.447649078112635
rscore_99 -10.27068013306636
nse [0.63526275]
nse_95 [-4.44764908]
nse_99 [-10.27068013]
kge [0.64169866]
ext_kge_95 [0.22792632]
ext_kge_99 [-0.13087076]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([31709 31733 31757], shape=(3,), dtype=int64) Times out: tf.Tensor(31757, shape=(), dtype=int64)
Times in: tf.Tensor([19632 19656 19680], shape=(3,), dtype=int64) Times out: tf.Tensor(19680, shape=(), dtype=int64)
Times in: tf.Tensor([36539 36563 36587], shape=(3,), dtype=int64) Times out: tf.Tensor(36587, shape=(), dtype=int64)
Times in: tf.Tensor([28174 28198 28222], shape=(3,), dtype=int64) Times out: tf.Tensor(28222, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_321&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_322 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_642 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_643 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_321 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_642 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_321 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_643 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 25.4082 - mse: 25.3690 - mae: 3.8994 - val_loss: 21.4722 - val_mse: 21.4210 - val_mae: 3.6997
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.7765 - mse: 20.7210 - mae: 3.5335 - val_loss: 21.1152 - val_mse: 21.0550 - val_mae: 3.6618
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.1584 - mse: 20.0948 - mae: 3.4864 - val_loss: 20.9098 - val_mse: 20.8418 - val_mae: 3.6421
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7567 - mse: 19.6857 - mae: 3.4523 - val_loss: 20.7011 - val_mse: 20.6262 - val_mae: 3.6247
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4778 - mse: 19.3999 - mae: 3.4318 - val_loss: 20.7835 - val_mse: 20.7023 - val_mae: 3.6255
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.2805 - mse: 19.1972 - mae: 3.4175 - val_loss: 20.5329 - val_mse: 20.4471 - val_mae: 3.6052
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1703 - mse: 19.0828 - mae: 3.4056 - val_loss: 20.7270 - val_mse: 20.6373 - val_mae: 3.6242
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.9959 - mse: 18.9049 - mae: 3.3918 - val_loss: 20.3547 - val_mse: 20.2619 - val_mae: 3.5864
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.9253 - mse: 18.8315 - mae: 3.3861 - val_loss: 20.2647 - val_mse: 20.1695 - val_mae: 3.5798
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.8476 - mse: 18.7515 - mae: 3.3755 - val_loss: 20.4287 - val_mse: 20.3311 - val_mae: 3.5897
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.7314 - mse: 18.6331 - mae: 3.3676 - val_loss: 20.3123 - val_mse: 20.2128 - val_mae: 3.5777
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6374 - mse: 18.5371 - mae: 3.3613 - val_loss: 20.1537 - val_mse: 20.0520 - val_mae: 3.5645
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.5799 - mse: 18.4773 - mae: 3.3573 - val_loss: 20.2540 - val_mse: 20.1502 - val_mae: 3.5758
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.5277 - mse: 18.4230 - mae: 3.3526 - val_loss: 20.2201 - val_mse: 20.1141 - val_mae: 3.5731
Epoch 15/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 18.4185 - mse: 18.3116 - mae: 3.3427 - val_loss: 20.1691 - val_mse: 20.0609 - val_mae: 3.5647
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3608 - mse: 18.2519 - mae: 3.3347 - val_loss: 20.0804 - val_mse: 19.9704 - val_mae: 3.5603
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3305 - mse: 18.2196 - mae: 3.3324 - val_loss: 20.1033 - val_mse: 19.9913 - val_mae: 3.5671
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.2310 - mse: 18.1181 - mae: 3.3215 - val_loss: 20.0610 - val_mse: 19.9470 - val_mae: 3.5561
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.1488 - mse: 18.0339 - mae: 3.3156 - val_loss: 19.9711 - val_mse: 19.8552 - val_mae: 3.5515
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.1348 - mse: 18.0179 - mae: 3.3150 - val_loss: 19.9767 - val_mse: 19.8587 - val_mae: 3.5489
bias -0.0015391001
si 0.63024765
rmse 0.044563048
kgeprime [0.64599429]
rmse_95 0.06697208
rmse_99 0.07606281
pearson 0.7527313912192709
pearson_95 0.38573838532381693
pearson_99 0.2317790819247154
rscore 0.5625805303694373
rscore_95 -5.666876600710375
rscore_99 -12.338906871454562
nse [0.56258053]
nse_95 [-5.6668766]
nse_99 [-12.33890687]
kge [0.68103045]
ext_kge_95 [0.05918477]
ext_kge_99 [-0.26229972]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([86963 86987 87011], shape=(3,), dtype=int64) Times out: tf.Tensor(87011, shape=(), dtype=int64)
Times in: tf.Tensor([107114 107138 107162], shape=(3,), dtype=int64) Times out: tf.Tensor(107162, shape=(), dtype=int64)
Times in: tf.Tensor([56233 56257 56281], shape=(3,), dtype=int64) Times out: tf.Tensor(56281, shape=(), dtype=int64)
Times in: tf.Tensor([61611 61635 61659], shape=(3,), dtype=int64) Times out: tf.Tensor(61659, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_322&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_323 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_644 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_645 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_322 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_644 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_322 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_645 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 25.8150 - mse: 25.7720 - mae: 3.9553 - val_loss: 17.4897 - val_mse: 17.4334 - val_mae: 3.2835
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2310 - mse: 21.1672 - mae: 3.5986 - val_loss: 17.1451 - val_mse: 17.0747 - val_mae: 3.2496
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6299 - mse: 20.5548 - mae: 3.5463 - val_loss: 16.6055 - val_mse: 16.5260 - val_mae: 3.1911
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1833 - mse: 20.1004 - mae: 3.5120 - val_loss: 16.5456 - val_mse: 16.4591 - val_mae: 3.1794
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9728 - mse: 19.8835 - mae: 3.4917 - val_loss: 16.3209 - val_mse: 16.2286 - val_mae: 3.1553
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8155 - mse: 19.7209 - mae: 3.4781 - val_loss: 16.1769 - val_mse: 16.0799 - val_mae: 3.1399
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.6414 - mse: 19.5427 - mae: 3.4656 - val_loss: 16.1375 - val_mse: 16.0368 - val_mae: 3.1339
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5485 - mse: 19.4463 - mae: 3.4532 - val_loss: 16.0776 - val_mse: 15.9737 - val_mae: 3.1379
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3694 - mse: 19.2643 - mae: 3.4392 - val_loss: 15.9954 - val_mse: 15.8888 - val_mae: 3.1193
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3120 - mse: 19.2041 - mae: 3.4328 - val_loss: 15.9026 - val_mse: 15.7934 - val_mae: 3.1165
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.1681 - mse: 19.0575 - mae: 3.4187 - val_loss: 15.8641 - val_mse: 15.7522 - val_mae: 3.1146
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0567 - mse: 18.9435 - mae: 3.4125 - val_loss: 15.7958 - val_mse: 15.6812 - val_mae: 3.0976
Epoch 13/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.9637 - mse: 18.8481 - mae: 3.3997 - val_loss: 15.7766 - val_mse: 15.6597 - val_mae: 3.1089
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8390 - mse: 18.7208 - mae: 3.3921 - val_loss: 15.6594 - val_mse: 15.5399 - val_mae: 3.0859
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.7803 - mse: 18.6596 - mae: 3.3837 - val_loss: 15.6128 - val_mse: 15.4908 - val_mae: 3.0863
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6114 - mse: 18.4882 - mae: 3.3684 - val_loss: 15.5540 - val_mse: 15.4295 - val_mae: 3.0853
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.5645 - mse: 18.4387 - mae: 3.3655 - val_loss: 15.4691 - val_mse: 15.3424 - val_mae: 3.0746
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.4749 - mse: 18.3469 - mae: 3.3592 - val_loss: 15.4711 - val_mse: 15.3420 - val_mae: 3.0705
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.4273 - mse: 18.2969 - mae: 3.3506 - val_loss: 15.5221 - val_mse: 15.3905 - val_mae: 3.0829
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.2732 - mse: 18.1405 - mae: 3.3414 - val_loss: 15.3524 - val_mse: 15.2185 - val_mae: 3.0627
bias -0.00022766792
si 0.5866017
rmse 0.039010935
kgeprime [0.6961948]
rmse_95 0.05694109
rmse_99 0.06943671
pearson 0.7815754705592852
pearson_95 0.5208580290962616
pearson_99 0.4321457529935458
rscore 0.6106089901672731
rscore_95 -2.7516727203689095
rscore_99 -7.845994201972653
nse [0.61060899]
nse_95 [-2.75167272]
nse_99 [-7.8459942]
kge [0.70164029]
ext_kge_95 [0.36248647]
ext_kge_99 [0.12775404]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -42.93 -42.62 -42.31 ... -36.37 -36.06
  * longitude       (longitude) float32 173.4 173.8 174.1 ... 179.7 180.0 180.3
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 4.298 3.888 ... 3.017
    vgrd10m         (time, latitude, longitude) float32 -0.05982 ... -0.1495
    uw2             (time, latitude, longitude) float32 18.48 15.12 ... 12.1 9.1
    vw2             (time, latitude, longitude) float32 0.003578 ... 0.02237
    wind_magnitude  (time, latitude, longitude) float32 4.299 3.91 ... 3.02
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([140216 140240 140264], shape=(3,), dtype=int64) Times out: tf.Tensor(140264, shape=(), dtype=int64)
Times in: tf.Tensor([48976 49000 49024], shape=(3,), dtype=int64) Times out: tf.Tensor(49024, shape=(), dtype=int64)
Times in: tf.Tensor([98445 98469 98493], shape=(3,), dtype=int64) Times out: tf.Tensor(98493, shape=(), dtype=int64)
Times in: tf.Tensor([59373 59397 59421], shape=(3,), dtype=int64) Times out: tf.Tensor(59421, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_323&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_324 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_646 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_647 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_323 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_646 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_323 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_647 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 26.4783 - mse: 26.4446 - mae: 4.0049 - val_loss: 18.5718 - val_mse: 18.5355 - val_mae: 3.3931
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0296 - mse: 21.9925 - mae: 3.6751 - val_loss: 17.8064 - val_mse: 17.7688 - val_mae: 3.3139
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5141 - mse: 21.4758 - mae: 3.6306 - val_loss: 17.6733 - val_mse: 17.6343 - val_mae: 3.3018
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4044 - mse: 21.3646 - mae: 3.6195 - val_loss: 17.5218 - val_mse: 17.4813 - val_mae: 3.2864
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2928 - mse: 21.2516 - mae: 3.6093 - val_loss: 17.2839 - val_mse: 17.2420 - val_mae: 3.2625
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2364 - mse: 21.1939 - mae: 3.6032 - val_loss: 17.5215 - val_mse: 17.4783 - val_mae: 3.2717
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1916 - mse: 21.1477 - mae: 3.6018 - val_loss: 17.1268 - val_mse: 17.0823 - val_mae: 3.2419
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1219 - mse: 21.0767 - mae: 3.5942 - val_loss: 17.0720 - val_mse: 17.0262 - val_mae: 3.2378
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.0282 - mse: 20.9817 - mae: 3.5859 - val_loss: 17.0292 - val_mse: 16.9819 - val_mae: 3.2358
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.9463 - mse: 20.8983 - mae: 3.5784 - val_loss: 16.8732 - val_mse: 16.8245 - val_mae: 3.2147
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8316 - mse: 20.7820 - mae: 3.5717 - val_loss: 16.7585 - val_mse: 16.7080 - val_mae: 3.2144
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7281 - mse: 20.6768 - mae: 3.5626 - val_loss: 16.7897 - val_mse: 16.7375 - val_mae: 3.2060
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8083 - mse: 20.7552 - mae: 3.5644 - val_loss: 16.7160 - val_mse: 16.6621 - val_mae: 3.2090
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6019 - mse: 20.5471 - mae: 3.5515 - val_loss: 16.5581 - val_mse: 16.5024 - val_mae: 3.1997
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6075 - mse: 20.5508 - mae: 3.5494 - val_loss: 16.6322 - val_mse: 16.5746 - val_mae: 3.2059
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5517 - mse: 20.4932 - mae: 3.5420 - val_loss: 16.4601 - val_mse: 16.4008 - val_mae: 3.1909
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4440 - mse: 20.3839 - mae: 3.5327 - val_loss: 16.4195 - val_mse: 16.3585 - val_mae: 3.1808
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4338 - mse: 20.3720 - mae: 3.5276 - val_loss: 16.4638 - val_mse: 16.4012 - val_mae: 3.1875
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4980 - mse: 20.4345 - mae: 3.5366 - val_loss: 16.3816 - val_mse: 16.3174 - val_mae: 3.1858
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4065 - mse: 20.3416 - mae: 3.5261 - val_loss: 16.3837 - val_mse: 16.3180 - val_mae: 3.1923
bias -0.00064376206
si 0.54399323
rmse 0.04039555
kgeprime [0.6651687]
rmse_95 0.06799899
rmse_99 0.08545295
pearson 0.8234257230681833
pearson_95 0.4425307440617832
pearson_99 0.3422447138926236
rscore 0.6703488038405843
rscore_95 -4.462311439775919
rscore_99 -17.444072505254418
nse [0.6703488]
nse_95 [-4.46231144]
nse_99 [-17.44407251]
kge [0.68145262]
ext_kge_95 [0.30906185]
ext_kge_99 [0.14273548]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([107140 107164 107188], shape=(3,), dtype=int64) Times out: tf.Tensor(107188, shape=(), dtype=int64)
Times in: tf.Tensor([11650 11674 11698], shape=(3,), dtype=int64) Times out: tf.Tensor(11698, shape=(), dtype=int64)
Times in: tf.Tensor([6994 7018 7042], shape=(3,), dtype=int64) Times out: tf.Tensor(7042, shape=(), dtype=int64)
Times in: tf.Tensor([77350 77374 77398], shape=(3,), dtype=int64) Times out: tf.Tensor(77398, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_324&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_325 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_648 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_649 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_324 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_648 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_324 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_649 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 8s 2ms/step - loss: 25.5932 - mse: 25.5536 - mae: 3.9394 - val_loss: 20.5528 - val_mse: 20.5082 - val_mae: 3.5410
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.7947 - mse: 20.7497 - mae: 3.5775 - val_loss: 20.1179 - val_mse: 20.0718 - val_mae: 3.4973
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.3600 - mse: 20.3130 - mae: 3.5376 - val_loss: 19.8278 - val_mse: 19.7790 - val_mae: 3.4734
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0929 - mse: 20.0433 - mae: 3.5119 - val_loss: 19.7020 - val_mse: 19.6510 - val_mae: 3.4625
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.1361 - mse: 20.0848 - mae: 3.5132 - val_loss: 19.8921 - val_mse: 19.8395 - val_mae: 3.4798
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0150 - mse: 19.9621 - mae: 3.5038 - val_loss: 19.4595 - val_mse: 19.4053 - val_mae: 3.4396
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.9055 - mse: 19.8512 - mae: 3.4956 - val_loss: 19.6417 - val_mse: 19.5862 - val_mae: 3.4575
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.7504 - mse: 19.6946 - mae: 3.4832 - val_loss: 19.2634 - val_mse: 19.2066 - val_mae: 3.4219
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.7482 - mse: 19.6911 - mae: 3.4804 - val_loss: 19.2075 - val_mse: 19.1493 - val_mae: 3.4154
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6496 - mse: 19.5911 - mae: 3.4743 - val_loss: 18.8489 - val_mse: 18.7893 - val_mae: 3.3815
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5985 - mse: 19.5386 - mae: 3.4696 - val_loss: 18.8445 - val_mse: 18.7836 - val_mae: 3.3836
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.6108 - mse: 19.5495 - mae: 3.4687 - val_loss: 19.1363 - val_mse: 19.0738 - val_mae: 3.4139
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3831 - mse: 19.3203 - mae: 3.4515 - val_loss: 18.6789 - val_mse: 18.6150 - val_mae: 3.3648
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4561 - mse: 19.3919 - mae: 3.4568 - val_loss: 18.6910 - val_mse: 18.6259 - val_mae: 3.3660
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2260 - mse: 19.1605 - mae: 3.4376 - val_loss: 18.7374 - val_mse: 18.6706 - val_mae: 3.3718
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2818 - mse: 19.2148 - mae: 3.4390 - val_loss: 18.8905 - val_mse: 18.8226 - val_mae: 3.3886
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3217 - mse: 19.2533 - mae: 3.4452 - val_loss: 18.6568 - val_mse: 18.5873 - val_mae: 3.3634
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1991 - mse: 19.1291 - mae: 3.4315 - val_loss: 18.5287 - val_mse: 18.4576 - val_mae: 3.3502
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1764 - mse: 19.1050 - mae: 3.4331 - val_loss: 18.7096 - val_mse: 18.6370 - val_mae: 3.3685
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1684 - mse: 19.0955 - mae: 3.4300 - val_loss: 18.4899 - val_mse: 18.4159 - val_mae: 3.3446
bias -0.0013924148
si 0.526922
rmse 0.042913783
kgeprime [0.66082597]
rmse_95 0.0684028
rmse_99 0.091552794
pearson 0.8377544296140497
pearson_95 0.44792152773298766
pearson_99 0.7203655845582195
rscore 0.6932582964209233
rscore_95 -4.063924878306032
rscore_99 -13.33766853080777
nse [0.6932583]
nse_95 [-4.06392488]
nse_99 [-13.33766853]
kge [0.69423793]
ext_kge_95 [0.3102893]
ext_kge_99 [0.13203864]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([52449 52473 52497], shape=(3,), dtype=int64) Times out: tf.Tensor(52497, shape=(), dtype=int64)
Times in: tf.Tensor([77290 77314 77338], shape=(3,), dtype=int64) Times out: tf.Tensor(77338, shape=(), dtype=int64)
Times in: tf.Tensor([66761 66785 66809], shape=(3,), dtype=int64) Times out: tf.Tensor(66809, shape=(), dtype=int64)
Times in: tf.Tensor([58456 58480 58504], shape=(3,), dtype=int64) Times out: tf.Tensor(58504, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_325&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_326 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_650 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_651 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_325 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_650 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_325 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_651 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 24.4773 - mse: 24.4254 - mae: 3.8681 - val_loss: 19.1217 - val_mse: 19.0560 - val_mae: 3.3552
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.7313 - mse: 19.6590 - mae: 3.4997 - val_loss: 18.6480 - val_mse: 18.5682 - val_mae: 3.3126
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2621 - mse: 19.1792 - mae: 3.4564 - val_loss: 18.1981 - val_mse: 18.1102 - val_mae: 3.2736
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.9932 - mse: 18.9041 - mae: 3.4339 - val_loss: 18.0103 - val_mse: 17.9174 - val_mae: 3.2586
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.7844 - mse: 18.6905 - mae: 3.4132 - val_loss: 17.7886 - val_mse: 17.6919 - val_mae: 3.2357
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.5123 - mse: 18.4153 - mae: 3.3879 - val_loss: 17.6958 - val_mse: 17.5968 - val_mae: 3.2245
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.4294 - mse: 18.3299 - mae: 3.3795 - val_loss: 17.6211 - val_mse: 17.5195 - val_mae: 3.2197
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.2494 - mse: 18.1476 - mae: 3.3627 - val_loss: 17.7082 - val_mse: 17.6047 - val_mae: 3.2253
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.1411 - mse: 18.0370 - mae: 3.3575 - val_loss: 17.5086 - val_mse: 17.4029 - val_mae: 3.2101
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 18.0889 - mse: 17.9830 - mae: 3.3512 - val_loss: 17.4119 - val_mse: 17.3045 - val_mae: 3.1942
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.9697 - mse: 17.8617 - mae: 3.3402 - val_loss: 17.5201 - val_mse: 17.4104 - val_mae: 3.2125
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.9355 - mse: 17.8251 - mae: 3.3349 - val_loss: 17.4550 - val_mse: 17.3433 - val_mae: 3.1979
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.8420 - mse: 17.7295 - mae: 3.3283 - val_loss: 17.3135 - val_mse: 17.1997 - val_mae: 3.1892
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 17.7745 - mse: 17.6596 - mae: 3.3203 - val_loss: 17.3084 - val_mse: 17.1925 - val_mae: 3.1866
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.6689 - mse: 17.5522 - mae: 3.3120 - val_loss: 17.2132 - val_mse: 17.0954 - val_mae: 3.1784
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.7061 - mse: 17.5871 - mae: 3.3124 - val_loss: 17.2406 - val_mse: 17.1205 - val_mae: 3.1841
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.6352 - mse: 17.5142 - mae: 3.3062 - val_loss: 17.3056 - val_mse: 17.1835 - val_mae: 3.1901
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.5463 - mse: 17.4230 - mae: 3.2964 - val_loss: 17.1931 - val_mse: 17.0686 - val_mae: 3.1747
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.5605 - mse: 17.4348 - mae: 3.3007 - val_loss: 17.1798 - val_mse: 17.0530 - val_mae: 3.1747
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 17.4693 - mse: 17.3413 - mae: 3.2873 - val_loss: 17.1186 - val_mse: 16.9892 - val_mae: 3.1684
bias 0.0002681627
si 0.535381
rmse 0.04121796
kgeprime [0.75355932]
rmse_95 0.07149434
rmse_99 0.11216918
pearson 0.827012491000633
pearson_95 0.27247664870707866
pearson_99 -0.15594112189983336
rscore 0.6838025677846375
rscore_95 -3.4251217229031514
rscore_99 -33.705195161389675
nse [0.68380257]
nse_95 [-3.42512172]
nse_99 [-33.70519516]
kge [0.74679351]
ext_kge_95 [0.17826246]
ext_kge_99 [-1.35207754]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([10960 10984 11008], shape=(3,), dtype=int64) Times out: tf.Tensor(11008, shape=(), dtype=int64)
Times in: tf.Tensor([20969 20993 21017], shape=(3,), dtype=int64) Times out: tf.Tensor(21017, shape=(), dtype=int64)
Times in: tf.Tensor([3999 4023 4047], shape=(3,), dtype=int64) Times out: tf.Tensor(4047, shape=(), dtype=int64)
Times in: tf.Tensor([12263 12287 12311], shape=(3,), dtype=int64) Times out: tf.Tensor(12311, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_326&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_327 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_652 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_653 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_326 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_652 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_326 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_653 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 24.3683 - mse: 24.3237 - mae: 3.8328 - val_loss: 20.3506 - val_mse: 20.2959 - val_mae: 3.5978
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0128 - mse: 19.9545 - mae: 3.4938 - val_loss: 20.2828 - val_mse: 20.2208 - val_mae: 3.5919
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6366 - mse: 19.5728 - mae: 3.4557 - val_loss: 19.9767 - val_mse: 19.9105 - val_mae: 3.5665
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4310 - mse: 19.3636 - mae: 3.4370 - val_loss: 19.8304 - val_mse: 19.7610 - val_mae: 3.5531
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3640 - mse: 19.2936 - mae: 3.4305 - val_loss: 19.4776 - val_mse: 19.4058 - val_mae: 3.5187
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2407 - mse: 19.1683 - mae: 3.4205 - val_loss: 19.4892 - val_mse: 19.4154 - val_mae: 3.5214
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1398 - mse: 19.0652 - mae: 3.4120 - val_loss: 19.2244 - val_mse: 19.1484 - val_mae: 3.4980
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9430 - mse: 18.8659 - mae: 3.3947 - val_loss: 19.3208 - val_mse: 19.2426 - val_mae: 3.5071
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7826 - mse: 18.7033 - mae: 3.3827 - val_loss: 18.9898 - val_mse: 18.9095 - val_mae: 3.4754
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7062 - mse: 18.6247 - mae: 3.3746 - val_loss: 19.0863 - val_mse: 19.0040 - val_mae: 3.4845
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.6246 - mse: 18.5411 - mae: 3.3668 - val_loss: 19.1000 - val_mse: 19.0155 - val_mae: 3.4853
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.6427 - mse: 18.5571 - mae: 3.3690 - val_loss: 18.8212 - val_mse: 18.7350 - val_mae: 3.4559
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.5037 - mse: 18.4163 - mae: 3.3539 - val_loss: 18.8691 - val_mse: 18.7811 - val_mae: 3.4587
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3767 - mse: 18.2876 - mae: 3.3444 - val_loss: 18.8252 - val_mse: 18.7356 - val_mae: 3.4545
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.3915 - mse: 18.3010 - mae: 3.3463 - val_loss: 18.8507 - val_mse: 18.7595 - val_mae: 3.4538
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.3794 - mse: 18.2874 - mae: 3.3407 - val_loss: 18.7924 - val_mse: 18.6997 - val_mae: 3.4490
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.2581 - mse: 18.1645 - mae: 3.3311 - val_loss: 18.7046 - val_mse: 18.6103 - val_mae: 3.4400
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.2711 - mse: 18.1756 - mae: 3.3314 - val_loss: 18.8303 - val_mse: 18.7340 - val_mae: 3.4529
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.1157 - mse: 18.0183 - mae: 3.3188 - val_loss: 18.6851 - val_mse: 18.5871 - val_mae: 3.4389
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.1676 - mse: 18.0686 - mae: 3.3250 - val_loss: 18.8255 - val_mse: 18.7256 - val_mae: 3.4490
bias 0.00042821694
si 0.57778716
rmse 0.043273136
kgeprime [0.77016563]
rmse_95 0.05621768
rmse_99 0.06962581
pearson 0.7988621370641332
pearson_95 0.3677894575881875
pearson_99 0.2576915320791006
rscore 0.6325638961039715
rscore_95 -5.233736944320206
rscore_99 -14.560808023663618
nse [0.6325639]
nse_95 [-5.23373694]
nse_99 [-14.56080802]
kge [0.76167327]
ext_kge_95 [0.19752883]
ext_kge_99 [0.13508913]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([67499 67523 67547], shape=(3,), dtype=int64) Times out: tf.Tensor(67547, shape=(), dtype=int64)
Times in: tf.Tensor([159352 159376 159400], shape=(3,), dtype=int64) Times out: tf.Tensor(159400, shape=(), dtype=int64)
Times in: tf.Tensor([102562 102586 102610], shape=(3,), dtype=int64) Times out: tf.Tensor(102610, shape=(), dtype=int64)
Times in: tf.Tensor([188989 189013 189037], shape=(3,), dtype=int64) Times out: tf.Tensor(189037, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_327&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_328 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_654 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_655 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_327 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_654 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_327 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_655 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 26.4895 - mse: 26.4550 - mae: 4.0024 - val_loss: 18.3751 - val_mse: 18.3347 - val_mae: 3.4039
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4722 - mse: 21.4289 - mae: 3.6257 - val_loss: 17.9415 - val_mse: 17.8948 - val_mae: 3.3828
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6232 - mse: 20.5729 - mae: 3.5471 - val_loss: 17.3002 - val_mse: 17.2467 - val_mae: 3.3071
Epoch 4/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.3101 - mse: 20.2538 - mae: 3.5218 - val_loss: 17.2267 - val_mse: 17.1680 - val_mae: 3.3018
Epoch 5/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.1810 - mse: 20.1202 - mae: 3.5069 - val_loss: 17.2180 - val_mse: 17.1552 - val_mae: 3.3002
Epoch 6/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.0391 - mse: 19.9744 - mae: 3.4947 - val_loss: 17.1582 - val_mse: 17.0920 - val_mae: 3.2961
Epoch 7/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.0255 - mse: 19.9575 - mae: 3.4956 - val_loss: 17.0106 - val_mse: 16.9409 - val_mae: 3.2808
Epoch 8/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.8880 - mse: 19.8166 - mae: 3.4829 - val_loss: 16.9890 - val_mse: 16.9160 - val_mae: 3.2807
Epoch 9/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.7063 - mse: 19.6316 - mae: 3.4658 - val_loss: 16.8697 - val_mse: 16.7936 - val_mae: 3.2679
Epoch 10/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.6656 - mse: 19.5878 - mae: 3.4625 - val_loss: 16.6285 - val_mse: 16.5491 - val_mae: 3.2418
Epoch 11/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.6007 - mse: 19.5198 - mae: 3.4521 - val_loss: 16.5037 - val_mse: 16.4213 - val_mae: 3.2279
Epoch 12/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.4526 - mse: 19.3689 - mae: 3.4425 - val_loss: 16.4133 - val_mse: 16.3284 - val_mae: 3.2222
Epoch 13/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.4012 - mse: 19.3151 - mae: 3.4339 - val_loss: 16.2782 - val_mse: 16.1909 - val_mae: 3.2056
Epoch 14/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.3595 - mse: 19.2711 - mae: 3.4343 - val_loss: 16.5600 - val_mse: 16.4706 - val_mae: 3.2429
Epoch 15/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.3305 - mse: 19.2401 - mae: 3.4281 - val_loss: 16.1178 - val_mse: 16.0262 - val_mae: 3.1934
Epoch 16/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.2228 - mse: 19.1303 - mae: 3.4206 - val_loss: 16.5822 - val_mse: 16.4888 - val_mae: 3.2511
Epoch 17/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.2321 - mse: 19.1378 - mae: 3.4199 - val_loss: 16.1894 - val_mse: 16.0941 - val_mae: 3.2038
Epoch 18/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.1737 - mse: 19.0776 - mae: 3.4190 - val_loss: 16.5312 - val_mse: 16.4343 - val_mae: 3.2483
Epoch 19/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.1316 - mse: 19.0337 - mae: 3.4113 - val_loss: 16.1042 - val_mse: 16.0054 - val_mae: 3.1919
Epoch 20/20
4857/4857 [==============================] - 8s 2ms/step - loss: 19.0448 - mse: 18.9451 - mae: 3.4053 - val_loss: 16.1938 - val_mse: 16.0932 - val_mae: 3.2066
bias 0.004992828
si 0.5398664
rmse 0.04011631
kgeprime [0.72766305]
rmse_95 0.06102256
rmse_99 0.0806678
pearson 0.8204275799348212
pearson_95 0.4876927329275815
pearson_99 0.33656577163241064
rscore 0.6675563198879437
rscore_95 -3.4611274294216194
rscore_99 -9.938729265202065
nse [0.66755632]
nse_95 [-3.46112743]
nse_99 [-9.93872927]
kge [0.66249518]
ext_kge_95 [0.39685271]
ext_kge_99 [0.22374722]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -42.31 -41.99 -41.68 ... -36.06 -35.75
  * longitude       (longitude) float32 170.6 170.9 171.2 ... 176.9 177.2 177.5
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 0.72 0.5491 ... 4.895
    vgrd10m         (time, latitude, longitude) float32 -0.03846 ... -0.2307
    uw2             (time, latitude, longitude) float32 0.5184 0.3015 ... 23.96
    vw2             (time, latitude, longitude) float32 0.001479 ... 0.05324
    wind_magnitude  (time, latitude, longitude) float32 0.721 0.5638 ... 4.9
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([42372 42396 42420], shape=(3,), dtype=int64) Times out: tf.Tensor(42420, shape=(), dtype=int64)
Times in: tf.Tensor([33907 33931 33955], shape=(3,), dtype=int64) Times out: tf.Tensor(33955, shape=(), dtype=int64)
Times in: tf.Tensor([34272 34296 34320], shape=(3,), dtype=int64) Times out: tf.Tensor(34320, shape=(), dtype=int64)
Times in: tf.Tensor([154627 154651 154675], shape=(3,), dtype=int64) Times out: tf.Tensor(154675, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_328&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_329 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_656 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_657 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_328 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_656 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_328 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_657 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 37.2698 - mse: 37.2281 - mae: 4.6479 - val_loss: 23.5417 - val_mse: 23.4904 - val_mae: 3.7353
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 27.3906 - mse: 27.3351 - mae: 4.0267 - val_loss: 22.5854 - val_mse: 22.5262 - val_mae: 3.6773
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.4906 - mse: 26.4283 - mae: 3.9567 - val_loss: 22.0925 - val_mse: 22.0272 - val_mae: 3.6343
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.0310 - mse: 25.9635 - mae: 3.9243 - val_loss: 21.7711 - val_mse: 21.7014 - val_mae: 3.6085
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.7618 - mse: 25.6901 - mae: 3.9075 - val_loss: 21.7960 - val_mse: 21.7223 - val_mae: 3.6217
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.5826 - mse: 25.5072 - mae: 3.8938 - val_loss: 21.5248 - val_mse: 21.4477 - val_mae: 3.5914
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.3974 - mse: 25.3184 - mae: 3.8762 - val_loss: 21.4047 - val_mse: 21.3244 - val_mae: 3.5797
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.3436 - mse: 25.2614 - mae: 3.8738 - val_loss: 21.6612 - val_mse: 21.5773 - val_mae: 3.6069
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.1767 - mse: 25.0913 - mae: 3.8604 - val_loss: 21.5090 - val_mse: 21.4220 - val_mae: 3.5951
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.9882 - mse: 24.8998 - mae: 3.8472 - val_loss: 21.1753 - val_mse: 21.0852 - val_mae: 3.5580
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.9192 - mse: 24.8276 - mae: 3.8454 - val_loss: 21.1771 - val_mse: 21.0840 - val_mae: 3.5673
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.6875 - mse: 24.5927 - mae: 3.8265 - val_loss: 20.8893 - val_mse: 20.7930 - val_mae: 3.5380
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.6117 - mse: 24.5137 - mae: 3.8206 - val_loss: 20.7716 - val_mse: 20.6719 - val_mae: 3.5307
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.3660 - mse: 24.2648 - mae: 3.7997 - val_loss: 20.4512 - val_mse: 20.3484 - val_mae: 3.5047
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.2893 - mse: 24.1849 - mae: 3.7980 - val_loss: 20.2207 - val_mse: 20.1150 - val_mae: 3.4814
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.2313 - mse: 24.1240 - mae: 3.7860 - val_loss: 20.2413 - val_mse: 20.1325 - val_mae: 3.4868
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.9799 - mse: 23.8696 - mae: 3.7712 - val_loss: 19.9246 - val_mse: 19.8126 - val_mae: 3.4580
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.8726 - mse: 23.7591 - mae: 3.7605 - val_loss: 19.8819 - val_mse: 19.7669 - val_mae: 3.4548
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.8783 - mse: 23.7619 - mae: 3.7584 - val_loss: 19.6677 - val_mse: 19.5499 - val_mae: 3.4343
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.7561 - mse: 23.6370 - mae: 3.7487 - val_loss: 19.6235 - val_mse: 19.5030 - val_mae: 3.4309
bias -0.005227003
si 0.46627975
rmse 0.04416225
kgeprime [0.62968723]
rmse_95 0.06999903
rmse_99 0.114811294
pearson 0.8721193147239816
pearson_95 0.48017218627837305
pearson_99 0.6573987970302221
rscore 0.7545874741300977
rscore_95 -0.8073328114967293
rscore_99 -3.5940363744204413
nse [0.75458747]
nse_95 [-0.80733281]
nse_99 [-3.59403637]
kge [0.71952148]
ext_kge_95 [0.4447167]
ext_kge_99 [0.51578461]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([8345 8369 8393], shape=(3,), dtype=int64) Times out: tf.Tensor(8393, shape=(), dtype=int64)
Times in: tf.Tensor([57034 57058 57082], shape=(3,), dtype=int64) Times out: tf.Tensor(57082, shape=(), dtype=int64)
Times in: tf.Tensor([98029 98053 98077], shape=(3,), dtype=int64) Times out: tf.Tensor(98077, shape=(), dtype=int64)
Times in: tf.Tensor([99410 99434 99458], shape=(3,), dtype=int64) Times out: tf.Tensor(99458, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_329&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_330 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_658 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_659 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_329 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_658 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_329 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_659 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 33.5676 - mse: 33.5216 - mae: 4.4039 - val_loss: 26.1762 - val_mse: 26.1163 - val_mae: 3.9464
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.0532 - mse: 24.9878 - mae: 3.8547 - val_loss: 25.5335 - val_mse: 25.4629 - val_mae: 3.9166
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.3734 - mse: 24.2993 - mae: 3.7998 - val_loss: 24.8880 - val_mse: 24.8102 - val_mae: 3.8557
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.0936 - mse: 24.0126 - mae: 3.7771 - val_loss: 24.6274 - val_mse: 24.5425 - val_mae: 3.8438
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.7412 - mse: 23.6538 - mae: 3.7509 - val_loss: 24.1225 - val_mse: 24.0315 - val_mae: 3.8023
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.5218 - mse: 23.4285 - mae: 3.7333 - val_loss: 23.9162 - val_mse: 23.8196 - val_mae: 3.7893
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.2792 - mse: 23.1806 - mae: 3.7180 - val_loss: 23.5085 - val_mse: 23.4068 - val_mae: 3.7637
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.0998 - mse: 22.9966 - mae: 3.6996 - val_loss: 23.1800 - val_mse: 23.0738 - val_mae: 3.7364
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.9157 - mse: 22.8080 - mae: 3.6823 - val_loss: 23.0226 - val_mse: 22.9122 - val_mae: 3.7217
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.7402 - mse: 22.6284 - mae: 3.6713 - val_loss: 22.9039 - val_mse: 22.7897 - val_mae: 3.7076
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5415 - mse: 22.4261 - mae: 3.6537 - val_loss: 22.6295 - val_mse: 22.5117 - val_mae: 3.6910
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.4270 - mse: 22.3079 - mae: 3.6480 - val_loss: 22.4281 - val_mse: 22.3065 - val_mae: 3.6725
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.2809 - mse: 22.1581 - mae: 3.6340 - val_loss: 22.4503 - val_mse: 22.3251 - val_mae: 3.6680
Epoch 14/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 22.2037 - mse: 22.0775 - mae: 3.6270 - val_loss: 22.0883 - val_mse: 21.9599 - val_mae: 3.6390
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0620 - mse: 21.9326 - mae: 3.6138 - val_loss: 21.9532 - val_mse: 21.8216 - val_mae: 3.6332
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.9238 - mse: 21.7912 - mae: 3.6081 - val_loss: 21.7422 - val_mse: 21.6075 - val_mae: 3.6165
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8265 - mse: 21.6908 - mae: 3.5977 - val_loss: 21.7848 - val_mse: 21.6468 - val_mae: 3.6180
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7765 - mse: 21.6379 - mae: 3.5921 - val_loss: 21.7849 - val_mse: 21.6441 - val_mae: 3.6156
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.6860 - mse: 21.5445 - mae: 3.5835 - val_loss: 21.6581 - val_mse: 21.5145 - val_mae: 3.6031
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5491 - mse: 21.4049 - mae: 3.5751 - val_loss: 21.6492 - val_mse: 21.5030 - val_mae: 3.6008
bias 0.001948968
si 0.45875007
rmse 0.046371333
kgeprime [0.8262559]
rmse_95 0.07254428
rmse_99 0.07742237
pearson 0.877752929504148
pearson_95 0.6697246765711021
pearson_99 0.6125634941624808
rscore 0.7683840141072786
rscore_95 -2.533146456661588
rscore_99 -11.489819452313847
nse [0.76838401]
nse_95 [-2.53314646]
nse_99 [-11.48981945]
kge [0.78634854]
ext_kge_95 [0.26636924]
ext_kge_99 [-0.76593208]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([25641 25665 25689], shape=(3,), dtype=int64) Times out: tf.Tensor(25689, shape=(), dtype=int64)
Times in: tf.Tensor([56742 56766 56790], shape=(3,), dtype=int64) Times out: tf.Tensor(56790, shape=(), dtype=int64)
Times in: tf.Tensor([15748 15772 15796], shape=(3,), dtype=int64) Times out: tf.Tensor(15796, shape=(), dtype=int64)
Times in: tf.Tensor([27606 27630 27654], shape=(3,), dtype=int64) Times out: tf.Tensor(27654, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_330&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_331 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_660 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_661 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_330 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_660 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_330 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_661 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 33.1991 - mse: 33.1538 - mae: 4.4142 - val_loss: 23.9724 - val_mse: 23.9154 - val_mae: 3.6716
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 25.4078 - mse: 25.3461 - mae: 3.9141 - val_loss: 23.6077 - val_mse: 23.5411 - val_mae: 3.6410
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.8384 - mse: 24.7694 - mae: 3.8733 - val_loss: 23.5792 - val_mse: 23.5064 - val_mae: 3.6379
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.6704 - mse: 24.5954 - mae: 3.8612 - val_loss: 23.1863 - val_mse: 23.1079 - val_mae: 3.6014
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.3736 - mse: 24.2933 - mae: 3.8394 - val_loss: 23.0979 - val_mse: 23.0143 - val_mae: 3.5925
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.1293 - mse: 24.0439 - mae: 3.8208 - val_loss: 22.9824 - val_mse: 22.8938 - val_mae: 3.5872
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.0428 - mse: 23.9523 - mae: 3.8144 - val_loss: 22.9006 - val_mse: 22.8068 - val_mae: 3.5835
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.8151 - mse: 23.7191 - mae: 3.7944 - val_loss: 22.7708 - val_mse: 22.6711 - val_mae: 3.5737
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.6358 - mse: 23.5340 - mae: 3.7808 - val_loss: 22.6134 - val_mse: 22.5076 - val_mae: 3.5622
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.3274 - mse: 23.2195 - mae: 3.7576 - val_loss: 22.4878 - val_mse: 22.3761 - val_mae: 3.5539
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.2501 - mse: 23.1364 - mae: 3.7474 - val_loss: 22.3103 - val_mse: 22.1930 - val_mae: 3.5427
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.9555 - mse: 22.8364 - mae: 3.7213 - val_loss: 22.2819 - val_mse: 22.1595 - val_mae: 3.5391
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.7764 - mse: 22.6524 - mae: 3.7054 - val_loss: 22.0179 - val_mse: 21.8906 - val_mae: 3.5181
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.6241 - mse: 22.4956 - mae: 3.6945 - val_loss: 21.9193 - val_mse: 21.7880 - val_mae: 3.5144
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.4580 - mse: 22.3258 - mae: 3.6812 - val_loss: 21.7206 - val_mse: 21.5856 - val_mae: 3.4944
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.3490 - mse: 22.2132 - mae: 3.6731 - val_loss: 21.7678 - val_mse: 21.6293 - val_mae: 3.4987
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.2375 - mse: 22.0985 - mae: 3.6615 - val_loss: 21.8256 - val_mse: 21.6844 - val_mae: 3.4989
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.0586 - mse: 21.9169 - mae: 3.6518 - val_loss: 21.7371 - val_mse: 21.5930 - val_mae: 3.4937
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.9388 - mse: 21.7943 - mae: 3.6389 - val_loss: 21.7296 - val_mse: 21.5831 - val_mae: 3.4908
Epoch 20/20
4856/4856 [==============================] - 8s 2ms/step - loss: 21.8997 - mse: 21.7528 - mae: 3.6334 - val_loss: 21.6688 - val_mse: 21.5197 - val_mae: 3.4958
bias 0.0031961242
si 0.49868274
rmse 0.04638939
kgeprime [0.80714847]
rmse_95 0.09120711
rmse_99 0.16282015
pearson 0.8527908163414746
pearson_95 0.17004364239870243
pearson_99 -0.16818535882923746
rscore 0.7258649230315942
rscore_95 -2.406125595050696
rscore_99 -9.403027068186134
nse [0.72586492]
nse_95 [-2.4061256]
nse_99 [-9.40302707]
kge [0.75595655]
ext_kge_95 [0.12671953]
ext_kge_99 [-0.3084448]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([34978 35002 35026], shape=(3,), dtype=int64) Times out: tf.Tensor(35026, shape=(), dtype=int64)
Times in: tf.Tensor([25328 25352 25376], shape=(3,), dtype=int64) Times out: tf.Tensor(25376, shape=(), dtype=int64)
Times in: tf.Tensor([9553 9577 9601], shape=(3,), dtype=int64) Times out: tf.Tensor(9601, shape=(), dtype=int64)
Times in: tf.Tensor([25752 25776 25800], shape=(3,), dtype=int64) Times out: tf.Tensor(25800, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_331&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_332 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_662 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_663 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_331 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_662 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_331 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_663 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 31.6616 - mse: 31.6086 - mae: 4.2903 - val_loss: 23.0335 - val_mse: 22.9737 - val_mae: 3.7304
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 25.5824 - mse: 25.5196 - mae: 3.9003 - val_loss: 23.1565 - val_mse: 23.0904 - val_mae: 3.7296
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.8955 - mse: 24.8258 - mae: 3.8470 - val_loss: 22.3157 - val_mse: 22.2426 - val_mae: 3.6692
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.4524 - mse: 24.3756 - mae: 3.8145 - val_loss: 21.8300 - val_mse: 21.7499 - val_mae: 3.6326
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.0442 - mse: 23.9611 - mae: 3.7854 - val_loss: 21.5317 - val_mse: 21.4456 - val_mae: 3.6112
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.8414 - mse: 23.7526 - mae: 3.7725 - val_loss: 21.4342 - val_mse: 21.3427 - val_mae: 3.5937
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.4790 - mse: 23.3844 - mae: 3.7401 - val_loss: 21.0106 - val_mse: 20.9133 - val_mae: 3.5721
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.2056 - mse: 23.1052 - mae: 3.7205 - val_loss: 21.0786 - val_mse: 20.9756 - val_mae: 3.5586
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 22.9642 - mse: 22.8583 - mae: 3.6988 - val_loss: 20.5970 - val_mse: 20.4885 - val_mae: 3.5335
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.7628 - mse: 22.6515 - mae: 3.6858 - val_loss: 20.4317 - val_mse: 20.3181 - val_mae: 3.5190
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5987 - mse: 22.4824 - mae: 3.6710 - val_loss: 20.2690 - val_mse: 20.1505 - val_mae: 3.4974
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3907 - mse: 22.2698 - mae: 3.6526 - val_loss: 20.2131 - val_mse: 20.0900 - val_mae: 3.4930
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.2106 - mse: 22.0851 - mae: 3.6380 - val_loss: 20.1115 - val_mse: 19.9841 - val_mae: 3.4835
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0890 - mse: 21.9593 - mae: 3.6294 - val_loss: 19.9285 - val_mse: 19.7969 - val_mae: 3.4692
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8817 - mse: 21.7482 - mae: 3.6085 - val_loss: 19.8688 - val_mse: 19.7335 - val_mae: 3.4664
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7776 - mse: 21.6403 - mae: 3.6000 - val_loss: 19.7849 - val_mse: 19.6458 - val_mae: 3.4653
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7481 - mse: 21.6073 - mae: 3.6003 - val_loss: 19.7216 - val_mse: 19.5790 - val_mae: 3.4556
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5684 - mse: 21.4243 - mae: 3.5797 - val_loss: 19.5715 - val_mse: 19.4255 - val_mae: 3.4494
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.6074 - mse: 21.4600 - mae: 3.5826 - val_loss: 19.7572 - val_mse: 19.6081 - val_mae: 3.4469
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.4838 - mse: 21.3334 - mae: 3.5768 - val_loss: 19.6902 - val_mse: 19.5382 - val_mae: 3.4426
bias -0.0038539346
si 0.49249685
rmse 0.0442021
kgeprime [0.70852525]
rmse_95 0.055205
rmse_99 0.06869499
pearson 0.8566128110476148
pearson_95 0.5229578952095031
pearson_99 0.5993517003055105
rscore 0.7299683188262439
rscore_95 -1.4307642841387045
rscore_99 -5.324230839731495
nse [0.72996832]
nse_95 [-1.43076428]
nse_99 [-5.32423084]
kge [0.77545423]
ext_kge_95 [0.32751933]
ext_kge_99 [0.05698903]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([117795 117819 117843], shape=(3,), dtype=int64) Times out: tf.Tensor(117843, shape=(), dtype=int64)
Times in: tf.Tensor([160262 160286 160310], shape=(3,), dtype=int64) Times out: tf.Tensor(160310, shape=(), dtype=int64)
Times in: tf.Tensor([48305 48329 48353], shape=(3,), dtype=int64) Times out: tf.Tensor(48353, shape=(), dtype=int64)
Times in: tf.Tensor([178421 178445 178469], shape=(3,), dtype=int64) Times out: tf.Tensor(178469, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_332&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_333 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_664 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_665 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_332 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_664 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_332 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_665 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 34.6529 - mse: 34.6055 - mae: 4.4724 - val_loss: 22.5862 - val_mse: 22.5333 - val_mae: 3.7329
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.9736 - mse: 26.9193 - mae: 3.9880 - val_loss: 21.7071 - val_mse: 21.6517 - val_mae: 3.6681
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.3068 - mse: 26.2500 - mae: 3.9401 - val_loss: 21.3229 - val_mse: 21.2648 - val_mae: 3.6311
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.1782 - mse: 26.1185 - mae: 3.9299 - val_loss: 21.2636 - val_mse: 21.2028 - val_mae: 3.6286
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.8698 - mse: 25.8077 - mae: 3.9065 - val_loss: 21.2180 - val_mse: 21.1547 - val_mae: 3.6390
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.8217 - mse: 25.7571 - mae: 3.9019 - val_loss: 20.7843 - val_mse: 20.7185 - val_mae: 3.5871
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.5956 - mse: 25.5282 - mae: 3.8873 - val_loss: 20.5970 - val_mse: 20.5280 - val_mae: 3.5755
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.4236 - mse: 25.3526 - mae: 3.8759 - val_loss: 20.3991 - val_mse: 20.3260 - val_mae: 3.5558
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.2020 - mse: 25.1269 - mae: 3.8572 - val_loss: 20.2276 - val_mse: 20.1503 - val_mae: 3.5385
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.8795 - mse: 24.8001 - mae: 3.8340 - val_loss: 20.5782 - val_mse: 20.4964 - val_mae: 3.5915
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.8470 - mse: 24.7632 - mae: 3.8298 - val_loss: 19.7266 - val_mse: 19.6406 - val_mae: 3.5058
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.4576 - mse: 24.3695 - mae: 3.7997 - val_loss: 19.6538 - val_mse: 19.5638 - val_mae: 3.5005
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.2984 - mse: 24.2064 - mae: 3.7870 - val_loss: 19.3693 - val_mse: 19.2754 - val_mae: 3.4687
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.1444 - mse: 24.0484 - mae: 3.7733 - val_loss: 19.2549 - val_mse: 19.1570 - val_mae: 3.4611
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.9938 - mse: 23.8940 - mae: 3.7599 - val_loss: 19.1945 - val_mse: 19.0931 - val_mae: 3.4492
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.9828 - mse: 23.8794 - mae: 3.7586 - val_loss: 19.0881 - val_mse: 18.9829 - val_mae: 3.4413
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.8411 - mse: 23.7343 - mae: 3.7486 - val_loss: 19.1821 - val_mse: 19.0737 - val_mae: 3.4531
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.6972 - mse: 23.5874 - mae: 3.7417 - val_loss: 18.9943 - val_mse: 18.8832 - val_mae: 3.4301
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.6594 - mse: 23.5468 - mae: 3.7353 - val_loss: 18.9067 - val_mse: 18.7928 - val_mae: 3.4273
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.5982 - mse: 23.4832 - mae: 3.7283 - val_loss: 18.9548 - val_mse: 18.8385 - val_mae: 3.4274
bias -0.0016620957
si 0.46576667
rmse 0.043403395
kgeprime [0.77160819]
rmse_95 0.0663501
rmse_99 0.09381489
pearson 0.8715963182869136
pearson_95 0.6305993710798252
pearson_99 0.6165778024918583
rscore 0.7593050606670002
rscore_95 -0.8206044941951391
rscore_99 -2.875611441337254
nse [0.75930506]
nse_95 [-0.82060449]
nse_99 [-2.87561144]
kge [0.80641119]
ext_kge_95 [0.57215581]
ext_kge_99 [0.53227258]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -41.99 -41.68 -41.37 ... -35.75 -35.44
  * longitude       (longitude) float32 174.7 175.0 175.3 ... 180.6 180.9 181.2
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 3.69 2.918 ... 5.655
    vgrd10m         (time, latitude, longitude) float32 -5.589 -5.999 ... -1.613
    uw2             (time, latitude, longitude) float32 13.61 8.517 ... 31.98
    vw2             (time, latitude, longitude) float32 31.24 35.99 ... 2.602
    wind_magnitude  (time, latitude, longitude) float32 6.697 6.671 ... 5.881
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([80324 80348 80372], shape=(3,), dtype=int64) Times out: tf.Tensor(80372, shape=(), dtype=int64)
Times in: tf.Tensor([29593 29617 29641], shape=(3,), dtype=int64) Times out: tf.Tensor(29641, shape=(), dtype=int64)
Times in: tf.Tensor([11879 11903 11927], shape=(3,), dtype=int64) Times out: tf.Tensor(11927, shape=(), dtype=int64)
Times in: tf.Tensor([75546 75570 75594], shape=(3,), dtype=int64) Times out: tf.Tensor(75594, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_333&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_334 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_666 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_667 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_333 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_666 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_333 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_667 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 29.2012 - mse: 29.1609 - mae: 4.2004 - val_loss: 20.0588 - val_mse: 20.0091 - val_mae: 3.5515
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6620 - mse: 22.6074 - mae: 3.7363 - val_loss: 19.2367 - val_mse: 19.1784 - val_mae: 3.4681
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.1401 - mse: 22.0787 - mae: 3.6909 - val_loss: 19.0041 - val_mse: 18.9401 - val_mae: 3.4383
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.8497 - mse: 21.7834 - mae: 3.6696 - val_loss: 18.7791 - val_mse: 18.7108 - val_mae: 3.4220
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.6657 - mse: 21.5954 - mae: 3.6561 - val_loss: 18.5320 - val_mse: 18.4600 - val_mae: 3.4029
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5910 - mse: 21.5175 - mae: 3.6455 - val_loss: 18.2377 - val_mse: 18.1625 - val_mae: 3.3827
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4199 - mse: 21.3429 - mae: 3.6331 - val_loss: 18.1232 - val_mse: 18.0444 - val_mae: 3.3646
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.2853 - mse: 21.2047 - mae: 3.6197 - val_loss: 18.0311 - val_mse: 17.9488 - val_mae: 3.3569
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1341 - mse: 21.0499 - mae: 3.6043 - val_loss: 17.7622 - val_mse: 17.6762 - val_mae: 3.3436
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.9484 - mse: 20.8605 - mae: 3.5921 - val_loss: 17.8040 - val_mse: 17.7145 - val_mae: 3.3428
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8777 - mse: 20.7864 - mae: 3.5836 - val_loss: 17.4988 - val_mse: 17.4056 - val_mae: 3.3129
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7294 - mse: 20.6346 - mae: 3.5741 - val_loss: 17.4987 - val_mse: 17.4021 - val_mae: 3.3166
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6432 - mse: 20.5453 - mae: 3.5666 - val_loss: 17.4496 - val_mse: 17.3501 - val_mae: 3.3058
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5697 - mse: 20.4691 - mae: 3.5593 - val_loss: 17.3636 - val_mse: 17.2615 - val_mae: 3.3008
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4737 - mse: 20.3703 - mae: 3.5521 - val_loss: 17.3805 - val_mse: 17.2758 - val_mae: 3.3064
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4139 - mse: 20.3078 - mae: 3.5470 - val_loss: 17.3195 - val_mse: 17.2121 - val_mae: 3.2923
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3529 - mse: 20.2443 - mae: 3.5390 - val_loss: 17.3002 - val_mse: 17.1903 - val_mae: 3.2928
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3926 - mse: 20.2818 - mae: 3.5408 - val_loss: 17.4536 - val_mse: 17.3415 - val_mae: 3.3002
Epoch 19/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.2474 - mse: 20.1342 - mae: 3.5295 - val_loss: 17.3875 - val_mse: 17.2733 - val_mae: 3.2975
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1558 - mse: 20.0403 - mae: 3.5223 - val_loss: 17.3739 - val_mse: 17.2571 - val_mae: 3.3069
bias -0.0015179592
si 0.5417008
rmse 0.0415417
kgeprime [0.63430573]
rmse_95 0.067060925
rmse_99 0.07858047
pearson 0.8282820222368035
pearson_95 0.5944791611195127
pearson_99 0.5406507205294606
rscore 0.675493964527025
rscore_95 -3.190933659661983
rscore_99 -9.141450646294917
nse [0.67549396]
nse_95 [-3.19093366]
nse_99 [-9.14145065]
kge [0.6716646]
ext_kge_95 [0.44961764]
ext_kge_99 [0.31032504]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([112457 112481 112505], shape=(3,), dtype=int64) Times out: tf.Tensor(112505, shape=(), dtype=int64)
Times in: tf.Tensor([69313 69337 69361], shape=(3,), dtype=int64) Times out: tf.Tensor(69361, shape=(), dtype=int64)
Times in: tf.Tensor([492 516 540], shape=(3,), dtype=int64) Times out: tf.Tensor(540, shape=(), dtype=int64)
Times in: tf.Tensor([34424 34448 34472], shape=(3,), dtype=int64) Times out: tf.Tensor(34472, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_334&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_335 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_668 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_669 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_334 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_668 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_334 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_669 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 27.8050 - mse: 27.7550 - mae: 4.1171 - val_loss: 24.1804 - val_mse: 24.1234 - val_mae: 3.8563
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.8152 - mse: 22.7526 - mae: 3.7574 - val_loss: 22.5248 - val_mse: 22.4565 - val_mae: 3.7334
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.8482 - mse: 21.7762 - mae: 3.6749 - val_loss: 21.7939 - val_mse: 21.7183 - val_mae: 3.6700
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.5530 - mse: 21.4749 - mae: 3.6449 - val_loss: 21.6105 - val_mse: 21.5298 - val_mae: 3.6492
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.2109 - mse: 21.1286 - mae: 3.6170 - val_loss: 21.0480 - val_mse: 20.9634 - val_mae: 3.6001
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.9290 - mse: 20.8431 - mae: 3.5936 - val_loss: 21.0410 - val_mse: 20.9531 - val_mae: 3.5994
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.6356 - mse: 20.5469 - mae: 3.5736 - val_loss: 20.4424 - val_mse: 20.3518 - val_mae: 3.5463
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.4655 - mse: 20.3742 - mae: 3.5577 - val_loss: 20.5459 - val_mse: 20.4531 - val_mae: 3.5581
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.3970 - mse: 20.3036 - mae: 3.5499 - val_loss: 20.1632 - val_mse: 20.0684 - val_mae: 3.5222
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.2318 - mse: 20.1363 - mae: 3.5340 - val_loss: 19.8837 - val_mse: 19.7868 - val_mae: 3.4949
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.2032 - mse: 20.1056 - mae: 3.5304 - val_loss: 19.9868 - val_mse: 19.8876 - val_mae: 3.5061
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.1022 - mse: 20.0025 - mae: 3.5243 - val_loss: 19.6859 - val_mse: 19.5851 - val_mae: 3.4768
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.9962 - mse: 19.8947 - mae: 3.5111 - val_loss: 19.8553 - val_mse: 19.7526 - val_mae: 3.4923
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.9247 - mse: 19.8214 - mae: 3.5050 - val_loss: 19.7054 - val_mse: 19.6010 - val_mae: 3.4758
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.8380 - mse: 19.7330 - mae: 3.4976 - val_loss: 19.7268 - val_mse: 19.6207 - val_mae: 3.4780
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7990 - mse: 19.6923 - mae: 3.4964 - val_loss: 19.7248 - val_mse: 19.6169 - val_mae: 3.4806
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7335 - mse: 19.6252 - mae: 3.4917 - val_loss: 19.3734 - val_mse: 19.2640 - val_mae: 3.4382
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7173 - mse: 19.6074 - mae: 3.4881 - val_loss: 19.6327 - val_mse: 19.5216 - val_mae: 3.4708
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.7136 - mse: 19.6021 - mae: 3.4872 - val_loss: 19.6532 - val_mse: 19.5406 - val_mae: 3.4728
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5913 - mse: 19.4782 - mae: 3.4751 - val_loss: 19.6152 - val_mse: 19.5010 - val_mae: 3.4677
bias 0.001162563
si 0.5370256
rmse 0.044159904
kgeprime [0.72652447]
rmse_95 0.06996534
rmse_99 0.075603835
pearson 0.8302311117421705
pearson_95 0.5969566115844329
pearson_99 0.49948951646465206
rscore 0.6831659368526297
rscore_95 -3.046625499443282
rscore_99 -3.5305754917337007
nse [0.68316594]
nse_95 [-3.0466255]
nse_99 [-3.53057549]
kge [0.69691301]
ext_kge_95 [0.38238959]
ext_kge_99 [0.4172481]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([63318 63342 63366], shape=(3,), dtype=int64) Times out: tf.Tensor(63366, shape=(), dtype=int64)
Times in: tf.Tensor([12435 12459 12483], shape=(3,), dtype=int64) Times out: tf.Tensor(12483, shape=(), dtype=int64)
Times in: tf.Tensor([50433 50457 50481], shape=(3,), dtype=int64) Times out: tf.Tensor(50481, shape=(), dtype=int64)
Times in: tf.Tensor([52894 52918 52942], shape=(3,), dtype=int64) Times out: tf.Tensor(52942, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_335&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_336 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_670 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_671 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_335 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_670 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_335 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_671 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 26.5132 - mse: 26.4660 - mae: 4.0324 - val_loss: 21.4162 - val_mse: 21.3593 - val_mae: 3.5513
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.7878 - mse: 21.7244 - mae: 3.6835 - val_loss: 20.6126 - val_mse: 20.5427 - val_mae: 3.4871
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.2968 - mse: 21.2226 - mae: 3.6407 - val_loss: 20.1674 - val_mse: 20.0885 - val_mae: 3.4530
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.9040 - mse: 20.8218 - mae: 3.6108 - val_loss: 20.5296 - val_mse: 20.4434 - val_mae: 3.4880
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.7140 - mse: 20.6252 - mae: 3.5965 - val_loss: 19.8511 - val_mse: 19.7589 - val_mae: 3.4242
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.5916 - mse: 20.4978 - mae: 3.5820 - val_loss: 20.3366 - val_mse: 20.2399 - val_mae: 3.4779
Epoch 7/20
4856/4856 [==============================] - 8s 2ms/step - loss: 20.3813 - mse: 20.2833 - mae: 3.5652 - val_loss: 19.2317 - val_mse: 19.1306 - val_mae: 3.3705
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2548 - mse: 20.1527 - mae: 3.5557 - val_loss: 19.1974 - val_mse: 19.0928 - val_mae: 3.3668
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.0987 - mse: 19.9932 - mae: 3.5416 - val_loss: 19.5136 - val_mse: 19.4057 - val_mae: 3.3966
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.8922 - mse: 19.7837 - mae: 3.5232 - val_loss: 19.4966 - val_mse: 19.3859 - val_mae: 3.3939
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.8526 - mse: 19.7412 - mae: 3.5156 - val_loss: 19.3752 - val_mse: 19.2620 - val_mae: 3.3845
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.7666 - mse: 19.6525 - mae: 3.5140 - val_loss: 19.1483 - val_mse: 19.0323 - val_mae: 3.3670
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.6907 - mse: 19.5740 - mae: 3.5063 - val_loss: 19.0823 - val_mse: 18.9637 - val_mae: 3.3579
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.5774 - mse: 19.4583 - mae: 3.4952 - val_loss: 19.1818 - val_mse: 19.0608 - val_mae: 3.3653
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.5176 - mse: 19.3961 - mae: 3.4889 - val_loss: 18.8813 - val_mse: 18.7580 - val_mae: 3.3404
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.4793 - mse: 19.3556 - mae: 3.4866 - val_loss: 19.0381 - val_mse: 18.9129 - val_mae: 3.3520
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.4604 - mse: 19.3348 - mae: 3.4827 - val_loss: 19.0161 - val_mse: 18.8888 - val_mae: 3.3490
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2955 - mse: 19.1679 - mae: 3.4718 - val_loss: 18.6932 - val_mse: 18.5640 - val_mae: 3.3235
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.3049 - mse: 19.1752 - mae: 3.4707 - val_loss: 18.8229 - val_mse: 18.6915 - val_mae: 3.3351
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 19.2690 - mse: 19.1370 - mae: 3.4686 - val_loss: 19.4109 - val_mse: 19.2773 - val_mae: 3.3838
bias -0.010365747
si 0.5380543
rmse 0.043905836
kgeprime [0.35287558]
rmse_95 0.06759157
rmse_99 0.103181176
pearson 0.8268308261983219
pearson_95 0.5403595936547074
pearson_99 0.5326383017727951
rscore 0.6649393626961926
rscore_95 -1.4385995893909538
rscore_99 -8.621383512519474
nse [0.66493936]
nse_95 [-1.43859959]
nse_99 [-8.62138351]
kge [0.48701537]
ext_kge_95 [0.42872118]
ext_kge_99 [-0.47989752]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([26574 26598 26622], shape=(3,), dtype=int64) Times out: tf.Tensor(26622, shape=(), dtype=int64)
Times in: tf.Tensor([29414 29438 29462], shape=(3,), dtype=int64) Times out: tf.Tensor(29462, shape=(), dtype=int64)
Times in: tf.Tensor([579 603 627], shape=(3,), dtype=int64) Times out: tf.Tensor(627, shape=(), dtype=int64)
Times in: tf.Tensor([19897 19921 19945], shape=(3,), dtype=int64) Times out: tf.Tensor(19945, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_336&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_337 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_672 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_673 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_336 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_672 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_336 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_673 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 26.3525 - mse: 26.3026 - mae: 3.9774 - val_loss: 21.4085 - val_mse: 21.3459 - val_mae: 3.6960
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.6720 - mse: 21.6022 - mae: 3.6346 - val_loss: 20.7827 - val_mse: 20.7063 - val_mae: 3.6457
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.0798 - mse: 20.9983 - mae: 3.5865 - val_loss: 20.6574 - val_mse: 20.5701 - val_mae: 3.6428
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.7144 - mse: 20.6232 - mae: 3.5575 - val_loss: 20.3309 - val_mse: 20.2352 - val_mae: 3.6140
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.4712 - mse: 20.3726 - mae: 3.5355 - val_loss: 20.4918 - val_mse: 20.3898 - val_mae: 3.6350
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 20.2431 - mse: 20.1388 - mae: 3.5160 - val_loss: 20.1084 - val_mse: 20.0012 - val_mae: 3.6005
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.1557 - mse: 20.0470 - mae: 3.5085 - val_loss: 20.0341 - val_mse: 19.9235 - val_mae: 3.5936
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.0214 - mse: 19.9091 - mae: 3.4937 - val_loss: 19.7328 - val_mse: 19.6191 - val_mae: 3.5653
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.8838 - mse: 19.7684 - mae: 3.4852 - val_loss: 19.8974 - val_mse: 19.7806 - val_mae: 3.5819
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.7864 - mse: 19.6681 - mae: 3.4744 - val_loss: 20.0150 - val_mse: 19.8954 - val_mae: 3.5940
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.7354 - mse: 19.6146 - mae: 3.4701 - val_loss: 19.9185 - val_mse: 19.7969 - val_mae: 3.5828
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6862 - mse: 19.5632 - mae: 3.4631 - val_loss: 19.5073 - val_mse: 19.3835 - val_mae: 3.5434
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.6130 - mse: 19.4883 - mae: 3.4616 - val_loss: 19.4531 - val_mse: 19.3275 - val_mae: 3.5387
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.5431 - mse: 19.4164 - mae: 3.4506 - val_loss: 19.3856 - val_mse: 19.2581 - val_mae: 3.5313
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4492 - mse: 19.3203 - mae: 3.4440 - val_loss: 19.5509 - val_mse: 19.4212 - val_mae: 3.5476
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4028 - mse: 19.2720 - mae: 3.4428 - val_loss: 19.4282 - val_mse: 19.2965 - val_mae: 3.5364
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3430 - mse: 19.2101 - mae: 3.4371 - val_loss: 19.4679 - val_mse: 19.3344 - val_mae: 3.5404
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.3287 - mse: 19.1939 - mae: 3.4379 - val_loss: 19.5178 - val_mse: 19.3822 - val_mae: 3.5453
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2488 - mse: 19.1120 - mae: 3.4284 - val_loss: 19.4452 - val_mse: 19.3076 - val_mae: 3.5356
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.1771 - mse: 19.0384 - mae: 3.4244 - val_loss: 19.3231 - val_mse: 19.1837 - val_mae: 3.5241
bias -0.0028049115
si 0.56980604
rmse 0.043799218
kgeprime [0.66726135]
rmse_95 0.05619214
rmse_99 0.07166305
pearson 0.8048226881220657
pearson_95 0.3334831882326077
pearson_99 0.4691966132407743
rscore 0.643586587232616
rscore_95 -4.7131678943575706
rscore_99 -20.309973910648154
nse [0.64358659]
nse_95 [-4.71316789]
nse_99 [-20.30997391]
kge [0.72740878]
ext_kge_95 [0.06349291]
ext_kge_99 [-0.30855423]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([155287 155311 155335], shape=(3,), dtype=int64) Times out: tf.Tensor(155335, shape=(), dtype=int64)
Times in: tf.Tensor([187217 187241 187265], shape=(3,), dtype=int64) Times out: tf.Tensor(187265, shape=(), dtype=int64)
Times in: tf.Tensor([106174 106198 106222], shape=(3,), dtype=int64) Times out: tf.Tensor(106222, shape=(), dtype=int64)
Times in: tf.Tensor([90979 91003 91027], shape=(3,), dtype=int64) Times out: tf.Tensor(91027, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_337&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_338 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_674 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_675 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_337 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_674 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_337 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_675 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 27.5387 - mse: 27.4954 - mae: 4.0900 - val_loss: 21.0350 - val_mse: 20.9837 - val_mae: 3.6356
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.5763 - mse: 22.5199 - mae: 3.7307 - val_loss: 19.7452 - val_mse: 19.6841 - val_mae: 3.5155
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.9999 - mse: 21.9345 - mae: 3.6829 - val_loss: 19.2998 - val_mse: 19.2304 - val_mae: 3.4750
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.5934 - mse: 21.5203 - mae: 3.6485 - val_loss: 19.3948 - val_mse: 19.3186 - val_mae: 3.4966
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4113 - mse: 21.3319 - mae: 3.6362 - val_loss: 18.9395 - val_mse: 18.8576 - val_mae: 3.4489
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1794 - mse: 21.0950 - mae: 3.6160 - val_loss: 18.6692 - val_mse: 18.5826 - val_mae: 3.4221
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.0272 - mse: 20.9387 - mae: 3.6009 - val_loss: 18.5817 - val_mse: 18.4916 - val_mae: 3.4170
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8764 - mse: 20.7845 - mae: 3.5878 - val_loss: 18.5504 - val_mse: 18.4572 - val_mae: 3.4165
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.7905 - mse: 20.6956 - mae: 3.5759 - val_loss: 18.5098 - val_mse: 18.4136 - val_mae: 3.4186
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6231 - mse: 20.5254 - mae: 3.5661 - val_loss: 18.2002 - val_mse: 18.1014 - val_mae: 3.3868
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5409 - mse: 20.4406 - mae: 3.5553 - val_loss: 17.8859 - val_mse: 17.7845 - val_mae: 3.3481
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4810 - mse: 20.3785 - mae: 3.5531 - val_loss: 17.8316 - val_mse: 17.7282 - val_mae: 3.3414
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.4278 - mse: 20.3234 - mae: 3.5456 - val_loss: 18.0577 - val_mse: 17.9523 - val_mae: 3.3780
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3389 - mse: 20.2323 - mae: 3.5379 - val_loss: 17.9317 - val_mse: 17.8243 - val_mae: 3.3615
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.2491 - mse: 20.1407 - mae: 3.5333 - val_loss: 18.1768 - val_mse: 18.0678 - val_mae: 3.3959
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1371 - mse: 20.0271 - mae: 3.5228 - val_loss: 17.9330 - val_mse: 17.8225 - val_mae: 3.3661
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.1320 - mse: 20.0205 - mae: 3.5211 - val_loss: 18.2403 - val_mse: 18.1281 - val_mae: 3.4056
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.0355 - mse: 19.9223 - mae: 3.5123 - val_loss: 17.6184 - val_mse: 17.5043 - val_mae: 3.3276
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.0473 - mse: 19.9323 - mae: 3.5136 - val_loss: 18.1594 - val_mse: 18.0438 - val_mae: 3.3913
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.9476 - mse: 19.8309 - mae: 3.5061 - val_loss: 17.8286 - val_mse: 17.7115 - val_mae: 3.3570
bias 0.0068888194
si 0.5489242
rmse 0.042085037
kgeprime [0.63777692]
rmse_95 0.0663169
rmse_99 0.08381747
pearson 0.8147284585229495
pearson_95 0.5644249040332745
pearson_99 0.49626809273344225
rscore 0.6545105300471239
rscore_95 -3.216668426097942
rscore_99 -9.569928024302413
nse [0.65451053]
nse_95 [-3.21666843]
nse_99 [-9.56992802]
kge [0.61271751]
ext_kge_95 [0.41111621]
ext_kge_99 [0.22891037]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -41.37 -41.06 -40.75 ... -35.13 -34.81
  * longitude       (longitude) float32 171.6 171.9 172.2 ... 177.5 177.8 178.1
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 1.169 0.9293 ... 7.251
    vgrd10m         (time, latitude, longitude) float32 0.109 -0.2692 ... -2.079
    uw2             (time, latitude, longitude) float32 1.366 0.8637 ... 52.58
    vw2             (time, latitude, longitude) float32 0.01187 ... 4.321
    wind_magnitude  (time, latitude, longitude) float32 1.174 0.9675 ... 7.543
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([106535 106559 106583], shape=(3,), dtype=int64) Times out: tf.Tensor(106583, shape=(), dtype=int64)
Times in: tf.Tensor([89146 89170 89194], shape=(3,), dtype=int64) Times out: tf.Tensor(89194, shape=(), dtype=int64)
Times in: tf.Tensor([73799 73823 73847], shape=(3,), dtype=int64) Times out: tf.Tensor(73847, shape=(), dtype=int64)
Times in: tf.Tensor([36629 36653 36677], shape=(3,), dtype=int64) Times out: tf.Tensor(36677, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_338&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_339 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_676 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_677 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_338 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_676 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_338 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_677 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 32.6914 - mse: 32.6429 - mae: 4.3353 - val_loss: 22.2638 - val_mse: 22.2021 - val_mae: 3.6357
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.9983 - mse: 23.9306 - mae: 3.7652 - val_loss: 21.4252 - val_mse: 21.3530 - val_mae: 3.5658
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.3658 - mse: 23.2900 - mae: 3.7150 - val_loss: 20.8939 - val_mse: 20.8146 - val_mae: 3.5296
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0464 - mse: 22.9642 - mae: 3.6892 - val_loss: 20.5827 - val_mse: 20.4976 - val_mae: 3.5093
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.8529 - mse: 22.7653 - mae: 3.6772 - val_loss: 20.1530 - val_mse: 20.0626 - val_mae: 3.4672
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.6116 - mse: 22.5185 - mae: 3.6567 - val_loss: 20.0863 - val_mse: 19.9905 - val_mae: 3.4618
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.3139 - mse: 22.2152 - mae: 3.6334 - val_loss: 19.7020 - val_mse: 19.6002 - val_mae: 3.4370
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0365 - mse: 21.9322 - mae: 3.6125 - val_loss: 19.3744 - val_mse: 19.2672 - val_mae: 3.4094
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.6828 - mse: 21.5731 - mae: 3.5837 - val_loss: 18.9747 - val_mse: 18.8624 - val_mae: 3.3735
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.4692 - mse: 21.3543 - mae: 3.5658 - val_loss: 18.7780 - val_mse: 18.6606 - val_mae: 3.3631
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.3160 - mse: 21.1963 - mae: 3.5521 - val_loss: 18.5447 - val_mse: 18.4227 - val_mae: 3.3422
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.1162 - mse: 20.9921 - mae: 3.5362 - val_loss: 18.5063 - val_mse: 18.3802 - val_mae: 3.3373
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.9329 - mse: 20.8045 - mae: 3.5201 - val_loss: 18.3079 - val_mse: 18.1776 - val_mae: 3.3215
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.8471 - mse: 20.7148 - mae: 3.5123 - val_loss: 17.9174 - val_mse: 17.7834 - val_mae: 3.2844
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6705 - mse: 20.5347 - mae: 3.4988 - val_loss: 18.3565 - val_mse: 18.2192 - val_mae: 3.3288
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.6563 - mse: 20.5173 - mae: 3.4945 - val_loss: 17.8996 - val_mse: 17.7592 - val_mae: 3.2894
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5404 - mse: 20.3986 - mae: 3.4846 - val_loss: 17.8563 - val_mse: 17.7131 - val_mae: 3.2876
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5145 - mse: 20.3699 - mae: 3.4816 - val_loss: 17.6509 - val_mse: 17.5046 - val_mae: 3.2645
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3394 - mse: 20.1917 - mae: 3.4705 - val_loss: 17.6567 - val_mse: 17.5078 - val_mae: 3.2660
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3019 - mse: 20.1517 - mae: 3.4638 - val_loss: 17.6936 - val_mse: 17.5421 - val_mae: 3.2713
bias 0.0022219582
si 0.44818902
rmse 0.041883256
kgeprime [0.80864676]
rmse_95 0.07006141
rmse_99 0.10156646
pearson 0.8844028247403463
pearson_95 0.6804109401190271
pearson_99 0.6768117628097271
rscore 0.7748673232669212
rscore_95 -0.46212904614877615
rscore_99 -1.1088432338288778
nse [0.77486732]
nse_95 [-0.46212905]
nse_99 [-1.10884323]
kge [0.75987904]
ext_kge_95 [0.58798672]
ext_kge_99 [0.52023783]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([89000 89024 89048], shape=(3,), dtype=int64) Times out: tf.Tensor(89048, shape=(), dtype=int64)
Times in: tf.Tensor([38500 38524 38548], shape=(3,), dtype=int64) Times out: tf.Tensor(38548, shape=(), dtype=int64)
Times in: tf.Tensor([23144 23168 23192], shape=(3,), dtype=int64) Times out: tf.Tensor(23192, shape=(), dtype=int64)
Times in: tf.Tensor([19683 19707 19731], shape=(3,), dtype=int64) Times out: tf.Tensor(19731, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_339&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_340 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_678 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_679 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_339 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_678 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_339 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_679 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 32.9339 - mse: 32.8871 - mae: 4.3431 - val_loss: 24.6165 - val_mse: 24.5567 - val_mae: 3.8012
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.0476 - mse: 23.9838 - mae: 3.7584 - val_loss: 23.5530 - val_mse: 23.4845 - val_mae: 3.7229
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.4138 - mse: 23.3440 - mae: 3.7075 - val_loss: 22.7677 - val_mse: 22.6948 - val_mae: 3.6784
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.0465 - mse: 22.9729 - mae: 3.6840 - val_loss: 22.7274 - val_mse: 22.6514 - val_mae: 3.6707
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.0457 - mse: 22.9697 - mae: 3.6834 - val_loss: 22.7968 - val_mse: 22.7189 - val_mae: 3.6734
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.8604 - mse: 22.7822 - mae: 3.6725 - val_loss: 22.6647 - val_mse: 22.5846 - val_mae: 3.6654
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.6875 - mse: 22.6069 - mae: 3.6553 - val_loss: 22.4669 - val_mse: 22.3840 - val_mae: 3.6527
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.5017 - mse: 22.4185 - mae: 3.6494 - val_loss: 22.1179 - val_mse: 22.0324 - val_mae: 3.6323
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.4043 - mse: 22.3187 - mae: 3.6359 - val_loss: 21.9402 - val_mse: 21.8520 - val_mae: 3.6285
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.2652 - mse: 22.1767 - mae: 3.6250 - val_loss: 22.0039 - val_mse: 21.9131 - val_mae: 3.6226
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0122 - mse: 21.9210 - mae: 3.6058 - val_loss: 21.3539 - val_mse: 21.2603 - val_mae: 3.5696
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.8593 - mse: 21.7653 - mae: 3.5932 - val_loss: 21.2700 - val_mse: 21.1736 - val_mae: 3.5712
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7379 - mse: 21.6410 - mae: 3.5830 - val_loss: 21.2825 - val_mse: 21.1830 - val_mae: 3.5595
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5236 - mse: 21.4237 - mae: 3.5693 - val_loss: 21.0513 - val_mse: 20.9491 - val_mae: 3.5338
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3608 - mse: 21.2585 - mae: 3.5519 - val_loss: 20.7934 - val_mse: 20.6887 - val_mae: 3.5252
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.3451 - mse: 21.2404 - mae: 3.5476 - val_loss: 20.9350 - val_mse: 20.8280 - val_mae: 3.5271
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.2623 - mse: 21.1553 - mae: 3.5435 - val_loss: 20.6505 - val_mse: 20.5413 - val_mae: 3.5038
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.1516 - mse: 21.0424 - mae: 3.5332 - val_loss: 20.7818 - val_mse: 20.6704 - val_mae: 3.5065
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.1071 - mse: 20.9956 - mae: 3.5273 - val_loss: 20.3565 - val_mse: 20.2429 - val_mae: 3.4855
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.9959 - mse: 20.8823 - mae: 3.5238 - val_loss: 20.2976 - val_mse: 20.1820 - val_mae: 3.4771
bias -0.000110474524
si 0.45770308
rmse 0.0449244
kgeprime [0.80563916]
rmse_95 0.06574231
rmse_99 0.054644868
pearson 0.8769004639024308
pearson_95 0.7021959607500032
pearson_99 0.6527669984282984
rscore 0.7683687729935399
rscore_95 -2.092410978108777
rscore_99 -7.377043322612886
nse [0.76836877]
nse_95 [-2.09241098]
nse_99 [-7.37704332]
kge [0.80802229]
ext_kge_95 [0.20306879]
ext_kge_99 [-0.99751637]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([62560 62584 62608], shape=(3,), dtype=int64) Times out: tf.Tensor(62608, shape=(), dtype=int64)
Times in: tf.Tensor([17707 17731 17755], shape=(3,), dtype=int64) Times out: tf.Tensor(17755, shape=(), dtype=int64)
Times in: tf.Tensor([6487 6511 6535], shape=(3,), dtype=int64) Times out: tf.Tensor(6535, shape=(), dtype=int64)
Times in: tf.Tensor([28147 28171 28195], shape=(3,), dtype=int64) Times out: tf.Tensor(28195, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_340&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_341 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_680 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_681 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_340 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_680 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_340 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_681 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 33.5300 - mse: 33.4860 - mae: 4.4377 - val_loss: 23.0093 - val_mse: 22.9551 - val_mae: 3.5885
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 24.2922 - mse: 24.2338 - mae: 3.8214 - val_loss: 22.1083 - val_mse: 22.0446 - val_mae: 3.5071
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.4257 - mse: 23.3593 - mae: 3.7463 - val_loss: 22.0185 - val_mse: 21.9481 - val_mae: 3.5036
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 23.0309 - mse: 22.9592 - mae: 3.7203 - val_loss: 21.8945 - val_mse: 21.8193 - val_mae: 3.5041
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.7274 - mse: 22.6514 - mae: 3.6981 - val_loss: 21.4169 - val_mse: 21.3378 - val_mae: 3.4634
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 22.4552 - mse: 22.3754 - mae: 3.6772 - val_loss: 21.1524 - val_mse: 21.0692 - val_mae: 3.4396
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 22.2979 - mse: 22.2141 - mae: 3.6644 - val_loss: 20.9471 - val_mse: 20.8600 - val_mae: 3.4242
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.9540 - mse: 21.8656 - mae: 3.6359 - val_loss: 20.6598 - val_mse: 20.5677 - val_mae: 3.4021
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.6498 - mse: 21.5567 - mae: 3.6152 - val_loss: 20.4918 - val_mse: 20.3948 - val_mae: 3.3910
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.5597 - mse: 21.4615 - mae: 3.6037 - val_loss: 20.4410 - val_mse: 20.3389 - val_mae: 3.3920
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.1817 - mse: 21.0786 - mae: 3.5697 - val_loss: 20.2741 - val_mse: 20.1679 - val_mae: 3.3755
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.0278 - mse: 20.9203 - mae: 3.5617 - val_loss: 20.0312 - val_mse: 19.9202 - val_mae: 3.3474
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.8482 - mse: 20.7362 - mae: 3.5427 - val_loss: 19.9533 - val_mse: 19.8384 - val_mae: 3.3430
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.7187 - mse: 20.6029 - mae: 3.5320 - val_loss: 20.0757 - val_mse: 19.9570 - val_mae: 3.3532
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.6295 - mse: 20.5097 - mae: 3.5228 - val_loss: 19.8244 - val_mse: 19.7017 - val_mae: 3.3297
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.5675 - mse: 20.4437 - mae: 3.5146 - val_loss: 19.8019 - val_mse: 19.6752 - val_mae: 3.3281
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.3515 - mse: 20.2238 - mae: 3.5019 - val_loss: 19.7636 - val_mse: 19.6334 - val_mae: 3.3268
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2054 - mse: 20.0743 - mae: 3.4881 - val_loss: 19.7445 - val_mse: 19.6108 - val_mae: 3.3235
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.1681 - mse: 20.0338 - mae: 3.4838 - val_loss: 19.8112 - val_mse: 19.6743 - val_mae: 3.3264
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.1050 - mse: 19.9674 - mae: 3.4797 - val_loss: 19.7642 - val_mse: 19.6242 - val_mae: 3.3235
bias 0.001198578
si 0.49045464
rmse 0.04429922
kgeprime [0.80789237]
rmse_95 0.08786454
rmse_99 0.15751642
pearson 0.8572476648592526
pearson_95 0.2742192449172148
pearson_99 -0.13013700146969165
rscore 0.7344102677045805
rscore_95 -1.7854131910948632
rscore_99 -9.75014651677285
nse [0.73441027]
nse_95 [-1.78541319]
nse_99 [-9.75014652]
kge [0.7818514]
ext_kge_95 [0.22874938]
ext_kge_99 [-0.39928454]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([21925 21949 21973], shape=(3,), dtype=int64) Times out: tf.Tensor(21973, shape=(), dtype=int64)
Times in: tf.Tensor([28061 28085 28109], shape=(3,), dtype=int64) Times out: tf.Tensor(28109, shape=(), dtype=int64)
Times in: tf.Tensor([37104 37128 37152], shape=(3,), dtype=int64) Times out: tf.Tensor(37152, shape=(), dtype=int64)
Times in: tf.Tensor([8334 8358 8382], shape=(3,), dtype=int64) Times out: tf.Tensor(8382, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_341&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_342 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_682 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_683 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_341 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_682 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_341 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_683 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 34.0151 - mse: 33.9629 - mae: 4.4020 - val_loss: 21.8358 - val_mse: 21.7718 - val_mae: 3.6505
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 24.3773 - mse: 24.3082 - mae: 3.7888 - val_loss: 21.2332 - val_mse: 21.1591 - val_mae: 3.5941
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 23.6151 - mse: 23.5372 - mae: 3.7280 - val_loss: 21.0693 - val_mse: 20.9880 - val_mae: 3.5816
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 23.2752 - mse: 23.1912 - mae: 3.6985 - val_loss: 20.9324 - val_mse: 20.8457 - val_mae: 3.5713
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 23.0370 - mse: 22.9483 - mae: 3.6840 - val_loss: 20.5818 - val_mse: 20.4908 - val_mae: 3.5375
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.7466 - mse: 22.6535 - mae: 3.6624 - val_loss: 20.4403 - val_mse: 20.3447 - val_mae: 3.5234
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 22.5001 - mse: 22.4023 - mae: 3.6470 - val_loss: 20.1691 - val_mse: 20.0686 - val_mae: 3.5008
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.3993 - mse: 22.2962 - mae: 3.6407 - val_loss: 20.0651 - val_mse: 19.9586 - val_mae: 3.4956
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 22.0775 - mse: 21.9682 - mae: 3.6127 - val_loss: 19.9254 - val_mse: 19.8127 - val_mae: 3.4872
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.7905 - mse: 21.6752 - mae: 3.5914 - val_loss: 19.6888 - val_mse: 19.5702 - val_mae: 3.4593
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.5160 - mse: 21.3953 - mae: 3.5689 - val_loss: 19.5307 - val_mse: 19.4073 - val_mae: 3.4474
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.3801 - mse: 21.2551 - mae: 3.5584 - val_loss: 19.4344 - val_mse: 19.3067 - val_mae: 3.4420
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 21.3179 - mse: 21.1885 - mae: 3.5517 - val_loss: 19.1977 - val_mse: 19.0659 - val_mae: 3.4222
Epoch 14/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.1203 - mse: 20.9872 - mae: 3.5337 - val_loss: 19.2494 - val_mse: 19.1142 - val_mae: 3.4207
Epoch 15/20
4855/4855 [==============================] - 8s 2ms/step - loss: 21.0408 - mse: 20.9042 - mae: 3.5276 - val_loss: 19.1543 - val_mse: 19.0154 - val_mae: 3.4172
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.9279 - mse: 20.7882 - mae: 3.5170 - val_loss: 19.1847 - val_mse: 19.0433 - val_mae: 3.4240
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.8861 - mse: 20.7436 - mae: 3.5130 - val_loss: 18.9187 - val_mse: 18.7747 - val_mae: 3.3936
Epoch 18/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.8848 - mse: 20.7396 - mae: 3.5142 - val_loss: 18.7865 - val_mse: 18.6396 - val_mae: 3.3814
Epoch 19/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 8s 2ms/step - loss: 20.7048 - mse: 20.5570 - mae: 3.5007 - val_loss: 18.9165 - val_mse: 18.7672 - val_mae: 3.4018
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.6142 - mse: 20.4640 - mae: 3.4914 - val_loss: 18.6357 - val_mse: 18.4842 - val_mae: 3.3693
bias 8.0049555e-05
si 0.49596778
rmse 0.04299327
kgeprime [0.81381054]
rmse_95 0.06294636
rmse_99 0.07554923
pearson 0.8531862216894962
pearson_95 0.547637830365553
pearson_99 0.4763236933211074
rscore 0.7270412256116352
rscore_95 -2.5929144456350293
rscore_99 -6.676718686686516
nse [0.72704123]
nse_95 [-2.59291445]
nse_99 [-6.67671869]
kge [0.81220001]
ext_kge_95 [0.30185622]
ext_kge_99 [0.01705334]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([179960 179984 180008], shape=(3,), dtype=int64) Times out: tf.Tensor(180008, shape=(), dtype=int64)
Times in: tf.Tensor([180751 180775 180799], shape=(3,), dtype=int64) Times out: tf.Tensor(180799, shape=(), dtype=int64)
Times in: tf.Tensor([183021 183045 183069], shape=(3,), dtype=int64) Times out: tf.Tensor(183069, shape=(), dtype=int64)
Times in: tf.Tensor([50480 50504 50528], shape=(3,), dtype=int64) Times out: tf.Tensor(50528, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_342&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_343 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_684 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_685 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_342 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_684 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_342 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_685 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 38.3216 - mse: 38.2732 - mae: 4.7302 - val_loss: 23.2723 - val_mse: 23.2134 - val_mae: 3.7609
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 28.0101 - mse: 27.9453 - mae: 4.0735 - val_loss: 22.1781 - val_mse: 22.1064 - val_mae: 3.6877
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 26.4057 - mse: 26.3265 - mae: 3.9489 - val_loss: 21.1013 - val_mse: 21.0152 - val_mae: 3.5974
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 25.2381 - mse: 25.1460 - mae: 3.8540 - val_loss: 20.6535 - val_mse: 20.5562 - val_mae: 3.5546
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.7527 - mse: 24.6518 - mae: 3.8133 - val_loss: 19.7359 - val_mse: 19.6318 - val_mae: 3.4850
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.4204 - mse: 24.3140 - mae: 3.7849 - val_loss: 19.4845 - val_mse: 19.3758 - val_mae: 3.4633
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 24.1620 - mse: 24.0513 - mae: 3.7587 - val_loss: 19.3402 - val_mse: 19.2274 - val_mae: 3.4466
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.9485 - mse: 23.8338 - mae: 3.7450 - val_loss: 18.8828 - val_mse: 18.7661 - val_mae: 3.4153
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.7104 - mse: 23.5920 - mae: 3.7260 - val_loss: 18.6891 - val_mse: 18.5691 - val_mae: 3.3949
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.5397 - mse: 23.4181 - mae: 3.7122 - val_loss: 18.5206 - val_mse: 18.3974 - val_mae: 3.3785
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.2350 - mse: 23.1104 - mae: 3.6855 - val_loss: 18.3659 - val_mse: 18.2401 - val_mae: 3.3655
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0686 - mse: 22.9414 - mae: 3.6724 - val_loss: 18.3138 - val_mse: 18.1853 - val_mae: 3.3588
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 23.0434 - mse: 22.9136 - mae: 3.6701 - val_loss: 17.9593 - val_mse: 17.8279 - val_mae: 3.3336
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.9104 - mse: 22.7776 - mae: 3.6543 - val_loss: 18.1006 - val_mse: 17.9662 - val_mae: 3.3453
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.7197 - mse: 22.5839 - mae: 3.6417 - val_loss: 17.6065 - val_mse: 17.4693 - val_mae: 3.3016
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.5797 - mse: 22.4405 - mae: 3.6342 - val_loss: 17.5670 - val_mse: 17.4262 - val_mae: 3.3035
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4902 - mse: 22.3478 - mae: 3.6175 - val_loss: 17.5929 - val_mse: 17.4488 - val_mae: 3.3012
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.4239 - mse: 22.2783 - mae: 3.6165 - val_loss: 17.4803 - val_mse: 17.3331 - val_mae: 3.2949
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2409 - mse: 22.0922 - mae: 3.6044 - val_loss: 17.5117 - val_mse: 17.3616 - val_mae: 3.3018
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.2209 - mse: 22.0697 - mae: 3.6034 - val_loss: 17.2606 - val_mse: 17.1082 - val_mae: 3.2774
bias 0.0013308497
si 0.461202
rmse 0.04136209
kgeprime [0.81004983]
rmse_95 0.06318931
rmse_99 0.08208435
pearson 0.874573736672551
pearson_95 0.6650029334162094
pearson_99 0.7051769071801495
rscore 0.7621983167305608
rscore_95 -0.5335826759113393
rscore_99 -1.4979913223638635
nse [0.76219832]
nse_95 [-0.53358268]
nse_99 [-1.49799132]
kge [0.78003767]
ext_kge_95 [0.60695827]
ext_kge_99 [0.60864197]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 23, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -41.06 -40.75 -40.43 ... -34.5 -34.19
  * longitude       (longitude) float32 172.8 173.1 173.4 ... 179.1 179.4 179.7
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -2.318 -1.799 ... 7.456
    vgrd10m         (time, latitude, longitude) float32 -0.5683 ... -3.384
    uw2             (time, latitude, longitude) float32 5.373 3.236 ... 55.59
    vw2             (time, latitude, longitude) float32 0.323 0.1154 ... 11.45
    wind_magnitude  (time, latitude, longitude) float32 2.387 1.831 ... 8.188
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([149173 149197 149221], shape=(3,), dtype=int64) Times out: tf.Tensor(149221, shape=(), dtype=int64)
Times in: tf.Tensor([55312 55336 55360], shape=(3,), dtype=int64) Times out: tf.Tensor(55360, shape=(), dtype=int64)
Times in: tf.Tensor([82612 82636 82660], shape=(3,), dtype=int64) Times out: tf.Tensor(82660, shape=(), dtype=int64)
Times in: tf.Tensor([30592 30616 30640], shape=(3,), dtype=int64) Times out: tf.Tensor(30640, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_343&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_344 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_686 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_687 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_343 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_686 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_343 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_687 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.3643 - mse: 20.3128 - mae: 3.4870 - val_loss: 12.5418 - val_mse: 12.4744 - val_mae: 2.7571
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.7725 - mse: 15.6965 - mae: 3.1048 - val_loss: 12.1080 - val_mse: 12.0249 - val_mae: 2.7105
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.2839 - mse: 15.1950 - mae: 3.0573 - val_loss: 11.8747 - val_mse: 11.7806 - val_mae: 2.6670
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.0011 - mse: 14.9024 - mae: 3.0276 - val_loss: 11.7673 - val_mse: 11.6645 - val_mae: 2.6614
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.7926 - mse: 14.6861 - mae: 3.0050 - val_loss: 11.7233 - val_mse: 11.6131 - val_mae: 2.6598
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.6393 - mse: 14.5256 - mae: 2.9872 - val_loss: 11.7124 - val_mse: 11.5955 - val_mae: 2.6589
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.4876 - mse: 14.3678 - mae: 2.9727 - val_loss: 11.6834 - val_mse: 11.5608 - val_mae: 2.6570
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.3466 - mse: 14.2211 - mae: 2.9551 - val_loss: 11.5650 - val_mse: 11.4374 - val_mae: 2.6398
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.2757 - mse: 14.1458 - mae: 2.9474 - val_loss: 11.4807 - val_mse: 11.3487 - val_mae: 2.6329
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1415 - mse: 14.0073 - mae: 2.9327 - val_loss: 11.3998 - val_mse: 11.2640 - val_mae: 2.6192
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.0007 - mse: 13.8632 - mae: 2.9181 - val_loss: 11.2936 - val_mse: 11.1549 - val_mae: 2.6042
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.9744 - mse: 13.8343 - mae: 2.9162 - val_loss: 11.2192 - val_mse: 11.0781 - val_mae: 2.5965
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.8484 - mse: 13.7060 - mae: 2.9057 - val_loss: 11.1259 - val_mse: 10.9825 - val_mae: 2.5882
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7665 - mse: 13.6218 - mae: 2.8933 - val_loss: 11.1172 - val_mse: 10.9716 - val_mae: 2.5848
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6839 - mse: 13.5372 - mae: 2.8845 - val_loss: 11.2222 - val_mse: 11.0745 - val_mae: 2.6080
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5723 - mse: 13.4234 - mae: 2.8749 - val_loss: 10.9819 - val_mse: 10.8322 - val_mae: 2.5738
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5202 - mse: 13.3693 - mae: 2.8689 - val_loss: 10.9164 - val_mse: 10.7647 - val_mae: 2.5667
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4626 - mse: 13.3098 - mae: 2.8593 - val_loss: 10.8410 - val_mse: 10.6873 - val_mae: 2.5574
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.3646 - mse: 13.2099 - mae: 2.8485 - val_loss: 10.8451 - val_mse: 10.6897 - val_mae: 2.5576
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.2943 - mse: 13.1379 - mae: 2.8426 - val_loss: 10.8088 - val_mse: 10.6516 - val_mae: 2.5588
bias -0.0023082315
si 0.5286662
rmse 0.03263679
kgeprime [0.65700406]
rmse_95 0.045443956
rmse_99 0.04504461
pearson 0.8258075846544233
pearson_95 0.7368989397817953
pearson_99 0.6432131467689488
rscore 0.6797938169029721
rscore_95 -1.5507736600081041
rscore_99 -2.9592864962415995
nse [0.67979382]
nse_95 [-1.55077366]
nse_99 [-2.9592865]
kge [0.71547125]
ext_kge_95 [0.28268118]
ext_kge_99 [-0.34365251]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([79302 79326 79350], shape=(3,), dtype=int64) Times out: tf.Tensor(79350, shape=(), dtype=int64)
Times in: tf.Tensor([77142 77166 77190], shape=(3,), dtype=int64) Times out: tf.Tensor(77190, shape=(), dtype=int64)
Times in: tf.Tensor([60462 60486 60510], shape=(3,), dtype=int64) Times out: tf.Tensor(60510, shape=(), dtype=int64)
Times in: tf.Tensor([113435 113459 113483], shape=(3,), dtype=int64) Times out: tf.Tensor(113483, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_344&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_345 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_688 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_689 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_344 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_688 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_344 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_689 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.4287 - mse: 19.3836 - mae: 3.4130 - val_loss: 15.5976 - val_mse: 15.5428 - val_mae: 3.0499
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 15.2113 - mse: 15.1496 - mae: 3.0498 - val_loss: 14.7723 - val_mse: 14.7055 - val_mae: 2.9732
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.5235 - mse: 14.4498 - mae: 2.9818 - val_loss: 14.5518 - val_mse: 14.4737 - val_mae: 2.9501
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.2122 - mse: 14.1280 - mae: 2.9452 - val_loss: 14.3874 - val_mse: 14.2998 - val_mae: 2.9316
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.9783 - mse: 13.8853 - mae: 2.9224 - val_loss: 14.4310 - val_mse: 14.3354 - val_mae: 2.9337
Epoch 6/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 13.8454 - mse: 13.7450 - mae: 2.9094 - val_loss: 14.2585 - val_mse: 14.1560 - val_mae: 2.9193
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7214 - mse: 13.6148 - mae: 2.8936 - val_loss: 14.3672 - val_mse: 14.2590 - val_mae: 2.9298
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6376 - mse: 13.5255 - mae: 2.8808 - val_loss: 14.2545 - val_mse: 14.1413 - val_mae: 2.9167
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.5090 - mse: 13.3920 - mae: 2.8683 - val_loss: 14.2887 - val_mse: 14.1709 - val_mae: 2.9222
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4580 - mse: 13.3368 - mae: 2.8620 - val_loss: 14.4163 - val_mse: 14.2946 - val_mae: 2.9325
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4181 - mse: 13.2932 - mae: 2.8583 - val_loss: 14.3611 - val_mse: 14.2360 - val_mae: 2.9231
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.3294 - mse: 13.2008 - mae: 2.8485 - val_loss: 14.1812 - val_mse: 14.0521 - val_mae: 2.9087
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.2502 - mse: 13.1181 - mae: 2.8406 - val_loss: 14.2371 - val_mse: 14.1048 - val_mae: 2.9115
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.1984 - mse: 13.0632 - mae: 2.8311 - val_loss: 14.1487 - val_mse: 14.0135 - val_mae: 2.8992
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.1429 - mse: 13.0047 - mae: 2.8262 - val_loss: 14.0935 - val_mse: 13.9551 - val_mae: 2.8961
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.0142 - mse: 12.8731 - mae: 2.8122 - val_loss: 14.2225 - val_mse: 14.0814 - val_mae: 2.9094
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.9900 - mse: 12.8462 - mae: 2.8098 - val_loss: 14.0724 - val_mse: 13.9287 - val_mae: 2.8930
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.8936 - mse: 12.7474 - mae: 2.7993 - val_loss: 13.9502 - val_mse: 13.8042 - val_mae: 2.8846
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.8718 - mse: 12.7232 - mae: 2.7937 - val_loss: 13.8361 - val_mse: 13.6876 - val_mae: 2.8753
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.7939 - mse: 12.6431 - mae: 2.7881 - val_loss: 13.8843 - val_mse: 13.7337 - val_mae: 2.8753
bias -0.0031522077
si 0.5334334
rmse 0.037059046
kgeprime [0.60547458]
rmse_95 0.05781773
rmse_99 0.084254585
pearson 0.8282103248552017
pearson_95 0.6144815722299647
pearson_99 0.5395544582638954
rscore 0.6801642517448692
rscore_95 -0.32505018267598396
rscore_99 -0.287403137219274
nse [0.68016425]
nse_95 [-0.32505018]
nse_99 [-0.28740314]
kge [0.67962348]
ext_kge_95 [0.56242128]
ext_kge_99 [0.43058946]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([60238 60262 60286], shape=(3,), dtype=int64) Times out: tf.Tensor(60286, shape=(), dtype=int64)
Times in: tf.Tensor([36322 36346 36370], shape=(3,), dtype=int64) Times out: tf.Tensor(36370, shape=(), dtype=int64)
Times in: tf.Tensor([40909 40933 40957], shape=(3,), dtype=int64) Times out: tf.Tensor(40957, shape=(), dtype=int64)
Times in: tf.Tensor([3572 3596 3620], shape=(3,), dtype=int64) Times out: tf.Tensor(3620, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_345&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_346 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_690 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_691 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_345 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_690 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_345 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_691 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.3387 - mse: 18.2900 - mae: 3.3038 - val_loss: 15.4908 - val_mse: 15.4328 - val_mae: 3.0420
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.8778 - mse: 14.8135 - mae: 3.0121 - val_loss: 14.6295 - val_mse: 14.5608 - val_mae: 2.9768
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.4384 - mse: 14.3645 - mae: 2.9645 - val_loss: 14.4612 - val_mse: 14.3839 - val_mae: 2.9465
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.1167 - mse: 14.0346 - mae: 2.9317 - val_loss: 14.0392 - val_mse: 13.9543 - val_mae: 2.9080
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.9300 - mse: 13.8406 - mae: 2.9129 - val_loss: 13.8856 - val_mse: 13.7941 - val_mae: 2.8853
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.7803 - mse: 13.6847 - mae: 2.8954 - val_loss: 13.7923 - val_mse: 13.6949 - val_mae: 2.8723
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.6492 - mse: 13.5481 - mae: 2.8813 - val_loss: 13.7552 - val_mse: 13.6527 - val_mae: 2.8715
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.5405 - mse: 13.4349 - mae: 2.8692 - val_loss: 13.6927 - val_mse: 13.5856 - val_mae: 2.8546
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.4231 - mse: 13.3129 - mae: 2.8574 - val_loss: 13.4367 - val_mse: 13.3253 - val_mae: 2.8284
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.3164 - mse: 13.2022 - mae: 2.8462 - val_loss: 13.3823 - val_mse: 13.2674 - val_mae: 2.8204
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.2198 - mse: 13.1025 - mae: 2.8348 - val_loss: 13.3261 - val_mse: 13.2078 - val_mae: 2.8127
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.1193 - mse: 12.9987 - mae: 2.8259 - val_loss: 13.3365 - val_mse: 13.2153 - val_mae: 2.8153
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.0179 - mse: 12.8943 - mae: 2.8156 - val_loss: 13.0845 - val_mse: 12.9605 - val_mae: 2.7858
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 12.9550 - mse: 12.8287 - mae: 2.8067 - val_loss: 13.0878 - val_mse: 12.9608 - val_mae: 2.7825
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 12.8826 - mse: 12.7533 - mae: 2.8005 - val_loss: 13.0478 - val_mse: 12.9180 - val_mae: 2.7756
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 12.8145 - mse: 12.6824 - mae: 2.7956 - val_loss: 13.0320 - val_mse: 12.8992 - val_mae: 2.7731
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 12.7477 - mse: 12.6127 - mae: 2.7837 - val_loss: 13.1520 - val_mse: 13.0166 - val_mae: 2.7820
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 12.7012 - mse: 12.5634 - mae: 2.7802 - val_loss: 12.7740 - val_mse: 12.6356 - val_mae: 2.7449
Epoch 19/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 12.6425 - mse: 12.5019 - mae: 2.7715 - val_loss: 13.0796 - val_mse: 12.9385 - val_mae: 2.7730
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 12.5299 - mse: 12.3866 - mae: 2.7610 - val_loss: 13.1242 - val_mse: 12.9803 - val_mae: 2.7715
bias 0.004228681
si 0.53467506
rmse 0.03602823
kgeprime [0.7303583]
rmse_95 0.07355796
rmse_99 0.10830082
pearson 0.8295850698970726
pearson_95 0.1991918969145397
pearson_99 -0.43961601674783907
rscore 0.6770480863897442
rscore_95 -5.265110258061786
rscore_99 -50.20633281754098
nse [0.67704809]
nse_95 [-5.26511026]
nse_99 [-50.20633282]
kge [0.63708329]
ext_kge_95 [0.03217183]
ext_kge_99 [-2.38621326]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([2398 2422 2446], shape=(3,), dtype=int64) Times out: tf.Tensor(2446, shape=(), dtype=int64)
Times in: tf.Tensor([13794 13818 13842], shape=(3,), dtype=int64) Times out: tf.Tensor(13842, shape=(), dtype=int64)
Times in: tf.Tensor([11778 11802 11826], shape=(3,), dtype=int64) Times out: tf.Tensor(11826, shape=(), dtype=int64)
Times in: tf.Tensor([11364 11388 11412], shape=(3,), dtype=int64) Times out: tf.Tensor(11412, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_346&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_347 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_692 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_693 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_346 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_692 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_346 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_693 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.9689 - mse: 20.9252 - mae: 3.5626 - val_loss: 19.0873 - val_mse: 19.0373 - val_mae: 3.4852
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.1676 - mse: 17.1155 - mae: 3.2390 - val_loss: 16.1192 - val_mse: 16.0655 - val_mae: 3.1906
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 15.3836 - mse: 15.3267 - mae: 3.0522 - val_loss: 15.4267 - val_mse: 15.3669 - val_mae: 3.1158
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.8921 - mse: 14.8295 - mae: 3.0003 - val_loss: 15.4011 - val_mse: 15.3365 - val_mae: 3.1183
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.6355 - mse: 14.5686 - mae: 2.9726 - val_loss: 15.1393 - val_mse: 15.0706 - val_mae: 3.0913
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.4384 - mse: 14.3676 - mae: 2.9517 - val_loss: 15.1212 - val_mse: 15.0491 - val_mae: 3.0888
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.3063 - mse: 14.2323 - mae: 2.9370 - val_loss: 14.8610 - val_mse: 14.7857 - val_mae: 3.0608
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.1466 - mse: 14.0696 - mae: 2.9195 - val_loss: 14.7812 - val_mse: 14.7028 - val_mae: 3.0550
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.0560 - mse: 13.9760 - mae: 2.9105 - val_loss: 14.6541 - val_mse: 14.5731 - val_mae: 3.0408
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.9383 - mse: 13.8556 - mae: 2.8957 - val_loss: 14.5415 - val_mse: 14.4578 - val_mae: 3.0294
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.8487 - mse: 13.7634 - mae: 2.8878 - val_loss: 14.4651 - val_mse: 14.3791 - val_mae: 3.0233
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7095 - mse: 13.6222 - mae: 2.8734 - val_loss: 14.5563 - val_mse: 14.4682 - val_mae: 3.0254
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7011 - mse: 13.6119 - mae: 2.8702 - val_loss: 14.3898 - val_mse: 14.3002 - val_mae: 3.0152
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.5902 - mse: 13.4993 - mae: 2.8619 - val_loss: 14.2393 - val_mse: 14.1479 - val_mae: 2.9971
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4985 - mse: 13.4060 - mae: 2.8496 - val_loss: 14.0811 - val_mse: 13.9881 - val_mae: 2.9828
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4323 - mse: 13.3382 - mae: 2.8433 - val_loss: 14.0437 - val_mse: 13.9491 - val_mae: 2.9796
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.3652 - mse: 13.2696 - mae: 2.8366 - val_loss: 13.9701 - val_mse: 13.8740 - val_mae: 2.9727
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.3431 - mse: 13.2459 - mae: 2.8332 - val_loss: 13.8748 - val_mse: 13.7770 - val_mae: 2.9629
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.2371 - mse: 13.1383 - mae: 2.8227 - val_loss: 13.7955 - val_mse: 13.6962 - val_mae: 2.9547
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.1876 - mse: 13.0872 - mae: 2.8155 - val_loss: 13.8868 - val_mse: 13.7860 - val_mae: 2.9582
bias -0.0045760875
si 0.5580593
rmse 0.03712954
kgeprime [0.5414346]
rmse_95 0.05006631
rmse_99 0.055180307
pearson 0.8095649523044115
pearson_95 0.6808621825507785
pearson_99 0.6294597718163514
rscore 0.6497255924521044
rscore_95 -2.1866714871986903
rscore_99 -3.0852422847649494
nse [0.64972559]
nse_95 [-2.18667149]
nse_99 [-3.08524228]
kge [0.64024256]
ext_kge_95 [0.44780763]
ext_kge_99 [0.47589815]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([43777 43801 43825], shape=(3,), dtype=int64) Times out: tf.Tensor(43825, shape=(), dtype=int64)
Times in: tf.Tensor([61986 62010 62034], shape=(3,), dtype=int64) Times out: tf.Tensor(62034, shape=(), dtype=int64)
Times in: tf.Tensor([162450 162474 162498], shape=(3,), dtype=int64) Times out: tf.Tensor(162498, shape=(), dtype=int64)
Times in: tf.Tensor([51882 51906 51930], shape=(3,), dtype=int64) Times out: tf.Tensor(51930, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_347&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_348 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_694 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_695 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_347 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_694 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_347 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_695 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.0440 - mse: 19.9966 - mae: 3.4475 - val_loss: 14.4202 - val_mse: 14.3582 - val_mae: 2.9481
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.6791 - mse: 15.6121 - mae: 3.0823 - val_loss: 13.8973 - val_mse: 13.8251 - val_mae: 2.8975
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.2825 - mse: 15.2061 - mae: 3.0472 - val_loss: 13.6379 - val_mse: 13.5570 - val_mae: 2.8707
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.0880 - mse: 15.0038 - mae: 3.0277 - val_loss: 13.3057 - val_mse: 13.2173 - val_mae: 2.8321
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.8401 - mse: 14.7486 - mae: 3.0016 - val_loss: 13.0860 - val_mse: 12.9907 - val_mae: 2.8134
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.6522 - mse: 14.5543 - mae: 2.9838 - val_loss: 12.8325 - val_mse: 12.7313 - val_mae: 2.7826
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.4883 - mse: 14.3847 - mae: 2.9675 - val_loss: 12.6463 - val_mse: 12.5395 - val_mae: 2.7635
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.3408 - mse: 14.2321 - mae: 2.9496 - val_loss: 12.5567 - val_mse: 12.4455 - val_mae: 2.7557
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1766 - mse: 14.0636 - mae: 2.9330 - val_loss: 12.4163 - val_mse: 12.3013 - val_mae: 2.7389
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1271 - mse: 14.0108 - mae: 2.9277 - val_loss: 12.3265 - val_mse: 12.2085 - val_mae: 2.7274
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.0133 - mse: 13.8941 - mae: 2.9166 - val_loss: 12.2545 - val_mse: 12.1340 - val_mae: 2.7220
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.9781 - mse: 13.8566 - mae: 2.9121 - val_loss: 12.2459 - val_mse: 12.1232 - val_mae: 2.7210
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.8879 - mse: 13.7643 - mae: 2.9007 - val_loss: 12.2159 - val_mse: 12.0910 - val_mae: 2.7175
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.8024 - mse: 13.6767 - mae: 2.8936 - val_loss: 12.1495 - val_mse: 12.0228 - val_mae: 2.7115
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7862 - mse: 13.6586 - mae: 2.8884 - val_loss: 12.0023 - val_mse: 11.8736 - val_mae: 2.6943
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7042 - mse: 13.5747 - mae: 2.8833 - val_loss: 12.0480 - val_mse: 11.9175 - val_mae: 2.7007
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6320 - mse: 13.5008 - mae: 2.8736 - val_loss: 12.0198 - val_mse: 11.8877 - val_mae: 2.6978
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5062 - mse: 13.3733 - mae: 2.8626 - val_loss: 11.7936 - val_mse: 11.6597 - val_mae: 2.6737
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5296 - mse: 13.3949 - mae: 2.8617 - val_loss: 11.8169 - val_mse: 11.6813 - val_mae: 2.6748
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4880 - mse: 13.3516 - mae: 2.8582 - val_loss: 11.7816 - val_mse: 11.6443 - val_mae: 2.6689
bias 0.000985998
si 0.5265958
rmse 0.034123704
kgeprime [0.76479962]
rmse_95 0.050442867
rmse_99 0.062897734
pearson 0.8286658491756407
pearson_95 0.5183836264315228
pearson_99 0.5341997382891925
rscore 0.6859742391936908
rscore_95 -1.7148861618088338
rscore_99 -8.837353543384529
nse [0.68597424]
nse_95 [-1.71488616]
nse_99 [-8.83735354]
kge [0.73840876]
ext_kge_95 [0.42884891]
ext_kge_99 [0.14383788]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 23, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -40.75 -40.43 -40.12 ... -34.5 -34.19
  * longitude       (longitude) float32 174.7 175.0 175.3 ... 180.9 181.2 181.6
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 0.9892 0.7285 ... 6.018
    vgrd10m         (time, latitude, longitude) float32 -4.41 -3.709 ... -3.457
    uw2             (time, latitude, longitude) float32 0.9784 0.5307 ... 36.22
    vw2             (time, latitude, longitude) float32 19.44 13.76 ... 11.95
    wind_magnitude  (time, latitude, longitude) float32 4.519 3.78 ... 6.94
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([68401 68425 68449], shape=(3,), dtype=int64) Times out: tf.Tensor(68449, shape=(), dtype=int64)
Times in: tf.Tensor([76734 76758 76782], shape=(3,), dtype=int64) Times out: tf.Tensor(76782, shape=(), dtype=int64)
Times in: tf.Tensor([145959 145983 146007], shape=(3,), dtype=int64) Times out: tf.Tensor(146007, shape=(), dtype=int64)
Times in: tf.Tensor([5255 5279 5303], shape=(3,), dtype=int64) Times out: tf.Tensor(5303, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_348&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_349 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_696 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_697 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_348 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_696 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_348 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_697 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 21.2793 - mse: 21.2340 - mae: 3.5849 - val_loss: 12.9555 - val_mse: 12.8973 - val_mae: 2.8324
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 16.0859 - mse: 16.0226 - mae: 3.1444 - val_loss: 12.2547 - val_mse: 12.1871 - val_mae: 2.7541
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.5936 - mse: 15.5223 - mae: 3.0911 - val_loss: 12.1980 - val_mse: 12.1232 - val_mae: 2.7558
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.3286 - mse: 15.2506 - mae: 3.0676 - val_loss: 12.1456 - val_mse: 12.0646 - val_mae: 2.7538
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.0921 - mse: 15.0084 - mae: 3.0406 - val_loss: 11.7639 - val_mse: 11.6776 - val_mae: 2.7076
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.9573 - mse: 14.8691 - mae: 3.0281 - val_loss: 11.7486 - val_mse: 11.6584 - val_mae: 2.7083
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.8064 - mse: 14.7147 - mae: 3.0122 - val_loss: 11.7637 - val_mse: 11.6705 - val_mae: 2.7076
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.7315 - mse: 14.6373 - mae: 3.0033 - val_loss: 11.8778 - val_mse: 11.7824 - val_mae: 2.7209
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.6690 - mse: 14.5727 - mae: 2.9991 - val_loss: 11.9250 - val_mse: 11.8275 - val_mae: 2.7291
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.5703 - mse: 14.4720 - mae: 2.9890 - val_loss: 11.6127 - val_mse: 11.5136 - val_mae: 2.6902
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.4604 - mse: 14.3605 - mae: 2.9790 - val_loss: 11.5555 - val_mse: 11.4549 - val_mae: 2.6832
Epoch 12/20
4857/4857 [==============================] - 8s 2ms/step - loss: 14.4249 - mse: 14.3235 - mae: 2.9697 - val_loss: 11.5712 - val_mse: 11.4691 - val_mae: 2.6864
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.3401 - mse: 14.2375 - mae: 2.9628 - val_loss: 11.5183 - val_mse: 11.4150 - val_mae: 2.6786
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.3117 - mse: 14.2080 - mae: 2.9618 - val_loss: 11.4607 - val_mse: 11.3563 - val_mae: 2.6683
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.2652 - mse: 14.1604 - mae: 2.9533 - val_loss: 11.3530 - val_mse: 11.2477 - val_mae: 2.6585
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1898 - mse: 14.0838 - mae: 2.9475 - val_loss: 11.3306 - val_mse: 11.2241 - val_mae: 2.6570
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1762 - mse: 14.0691 - mae: 2.9447 - val_loss: 11.3134 - val_mse: 11.2056 - val_mae: 2.6524
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1443 - mse: 14.0359 - mae: 2.9394 - val_loss: 11.4548 - val_mse: 11.3457 - val_mae: 2.6689
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.0992 - mse: 13.9897 - mae: 2.9350 - val_loss: 11.1949 - val_mse: 11.0850 - val_mae: 2.6386
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.0130 - mse: 13.9025 - mae: 2.9279 - val_loss: 11.3949 - val_mse: 11.2838 - val_mae: 2.6593
bias -0.0048254714
si 0.5340942
rmse 0.03359135
kgeprime [0.53028831]
rmse_95 0.046004474
rmse_99 0.061177235
pearson 0.8271310941329967
pearson_95 0.6793976184851398
pearson_99 0.38497842486230094
rscore 0.6764798961216132
rscore_95 -0.7794447073617026
rscore_99 -2.1368917459349013
nse [0.6764799]
nse_95 [-0.77944471]
nse_99 [-2.13689175]
kge [0.63678526]
ext_kge_95 [0.59175842]
ext_kge_99 [0.34479242]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([115219 115243 115267], shape=(3,), dtype=int64) Times out: tf.Tensor(115267, shape=(), dtype=int64)
Times in: tf.Tensor([82036 82060 82084], shape=(3,), dtype=int64) Times out: tf.Tensor(82084, shape=(), dtype=int64)
Times in: tf.Tensor([16699 16723 16747], shape=(3,), dtype=int64) Times out: tf.Tensor(16747, shape=(), dtype=int64)
Times in: tf.Tensor([88137 88161 88185], shape=(3,), dtype=int64) Times out: tf.Tensor(88185, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_349&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_350 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_698 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_699 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_349 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_698 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_349 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_699 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 20.3106 - mse: 20.2724 - mae: 3.5072 - val_loss: 15.7176 - val_mse: 15.6706 - val_mae: 3.0855
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 15.7153 - mse: 15.6628 - mae: 3.1162 - val_loss: 14.6091 - val_mse: 14.5513 - val_mae: 2.9734
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 15.1130 - mse: 15.0511 - mae: 3.0512 - val_loss: 14.4313 - val_mse: 14.3658 - val_mae: 2.9538
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.8819 - mse: 14.8135 - mae: 3.0291 - val_loss: 14.2516 - val_mse: 14.1804 - val_mae: 2.9323
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.6687 - mse: 14.5950 - mae: 3.0077 - val_loss: 14.4239 - val_mse: 14.3481 - val_mae: 2.9520
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.5677 - mse: 14.4895 - mae: 2.9994 - val_loss: 14.0318 - val_mse: 13.9517 - val_mae: 2.9074
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.4209 - mse: 14.3386 - mae: 2.9872 - val_loss: 14.0524 - val_mse: 13.9685 - val_mae: 2.9067
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.3177 - mse: 14.2317 - mae: 2.9745 - val_loss: 14.0214 - val_mse: 13.9337 - val_mae: 2.9056
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.1925 - mse: 14.1030 - mae: 2.9612 - val_loss: 13.9151 - val_mse: 13.8245 - val_mae: 2.8884
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.1646 - mse: 14.0721 - mae: 2.9600 - val_loss: 13.8444 - val_mse: 13.7508 - val_mae: 2.8809
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 14.0651 - mse: 13.9700 - mae: 2.9499 - val_loss: 13.8978 - val_mse: 13.8017 - val_mae: 2.8869
Epoch 12/20
4855/4855 [==============================] - 8s 2ms/step - loss: 14.0185 - mse: 13.9208 - mae: 2.9414 - val_loss: 13.7358 - val_mse: 13.6372 - val_mae: 2.8692
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.9020 - mse: 13.8019 - mae: 2.9347 - val_loss: 13.7161 - val_mse: 13.6151 - val_mae: 2.8641
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.9020 - mse: 13.7996 - mae: 2.9300 - val_loss: 13.6429 - val_mse: 13.5399 - val_mae: 2.8566
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.8321 - mse: 13.7278 - mae: 2.9236 - val_loss: 13.6997 - val_mse: 13.5947 - val_mae: 2.8607
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7521 - mse: 13.6459 - mae: 2.9125 - val_loss: 13.5814 - val_mse: 13.4746 - val_mae: 2.8499
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7083 - mse: 13.6007 - mae: 2.9102 - val_loss: 13.5206 - val_mse: 13.4125 - val_mae: 2.8407
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6871 - mse: 13.5781 - mae: 2.9073 - val_loss: 13.4382 - val_mse: 13.3287 - val_mae: 2.8354
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6341 - mse: 13.5237 - mae: 2.9032 - val_loss: 13.4943 - val_mse: 13.3835 - val_mae: 2.8375
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6396 - mse: 13.5279 - mae: 2.9006 - val_loss: 13.4323 - val_mse: 13.3203 - val_mae: 2.8312
bias -0.00010629996
si 0.54142445
rmse 0.03649703
kgeprime [0.71550257]
rmse_95 0.055891365
rmse_99 0.07629856
pearson 0.8242016565474537
pearson_95 0.6221978177859921
pearson_99 0.7285020538235071
rscore 0.6773896423069738
rscore_95 -1.1378147629448314
rscore_99 -1.103478067254216
nse [0.67738964]
nse_95 [-1.13781476]
nse_99 [-1.10347807]
kge [0.71867555]
ext_kge_95 [0.52239797]
ext_kge_99 [0.58911587]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([1026 1050 1074], shape=(3,), dtype=int64) Times out: tf.Tensor(1074, shape=(), dtype=int64)
Times in: tf.Tensor([24457 24481 24505], shape=(3,), dtype=int64) Times out: tf.Tensor(24505, shape=(), dtype=int64)
Times in: tf.Tensor([1063 1087 1111], shape=(3,), dtype=int64) Times out: tf.Tensor(1111, shape=(), dtype=int64)
Times in: tf.Tensor([26138 26162 26186], shape=(3,), dtype=int64) Times out: tf.Tensor(26186, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_350&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_351 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_700 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_701 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_350 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_700 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_350 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_701 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 18.8116 - mse: 18.7744 - mae: 3.3873 - val_loss: 15.5742 - val_mse: 15.5312 - val_mae: 3.0732
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 15.3901 - mse: 15.3440 - mae: 3.0840 - val_loss: 15.1589 - val_mse: 15.1102 - val_mae: 3.0291
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.9903 - mse: 14.9390 - mae: 3.0461 - val_loss: 14.8160 - val_mse: 14.7624 - val_mae: 2.9867
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.7719 - mse: 14.7162 - mae: 3.0240 - val_loss: 14.4255 - val_mse: 14.3680 - val_mae: 2.9421
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.6399 - mse: 14.5801 - mae: 3.0117 - val_loss: 14.4565 - val_mse: 14.3949 - val_mae: 2.9550
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.4442 - mse: 14.3807 - mae: 2.9920 - val_loss: 14.2409 - val_mse: 14.1760 - val_mae: 2.9299
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.3336 - mse: 14.2668 - mae: 2.9791 - val_loss: 14.1209 - val_mse: 14.0529 - val_mae: 2.9187
Epoch 8/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.2984 - mse: 14.2287 - mae: 2.9790 - val_loss: 14.3200 - val_mse: 14.2491 - val_mae: 2.9406
Epoch 9/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.1723 - mse: 14.1000 - mae: 2.9651 - val_loss: 14.0040 - val_mse: 13.9306 - val_mae: 2.9110
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.0924 - mse: 14.0177 - mae: 2.9552 - val_loss: 13.9778 - val_mse: 13.9021 - val_mae: 2.9084
Epoch 11/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.9880 - mse: 13.9109 - mae: 2.9446 - val_loss: 13.8947 - val_mse: 13.8167 - val_mae: 2.8989
Epoch 12/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.8980 - mse: 13.8187 - mae: 2.9336 - val_loss: 13.7236 - val_mse: 13.6436 - val_mae: 2.8817
Epoch 13/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.8568 - mse: 13.7756 - mae: 2.9315 - val_loss: 13.6566 - val_mse: 13.5747 - val_mae: 2.8693
Epoch 14/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.7887 - mse: 13.7057 - mae: 2.9195 - val_loss: 13.5895 - val_mse: 13.5058 - val_mae: 2.8632
Epoch 15/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.8039 - mse: 13.7191 - mae: 2.9218 - val_loss: 13.4212 - val_mse: 13.3358 - val_mae: 2.8433
Epoch 16/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.7725 - mse: 13.6861 - mae: 2.9204 - val_loss: 13.5700 - val_mse: 13.4831 - val_mae: 2.8611
Epoch 17/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.6480 - mse: 13.5601 - mae: 2.9095 - val_loss: 13.6255 - val_mse: 13.5371 - val_mae: 2.8663
Epoch 18/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.5993 - mse: 13.5098 - mae: 2.8992 - val_loss: 13.2747 - val_mse: 13.1845 - val_mae: 2.8250
Epoch 19/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.5533 - mse: 13.4621 - mae: 2.8973 - val_loss: 13.3763 - val_mse: 13.2846 - val_mae: 2.8378
Epoch 20/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.5364 - mse: 13.4436 - mae: 2.8951 - val_loss: 13.5170 - val_mse: 13.4234 - val_mae: 2.8565
bias -0.0017155991
si 0.5460616
rmse 0.036637995
kgeprime [0.61648219]
rmse_95 0.07071392
rmse_99 0.10627049
pearson 0.8267975285174902
pearson_95 0.3483282289489176
pearson_99 -0.5004039243383093
rscore 0.6730208696491682
rscore_95 -4.843074399267477
rscore_99 -46.08025085571004
nse [0.67302087]
nse_95 [-4.8430744]
nse_99 [-46.08025086]
kge [0.6655469]
ext_kge_95 [0.12719527]
ext_kge_99 [-2.39291124]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([2866 2890 2914], shape=(3,), dtype=int64) Times out: tf.Tensor(2914, shape=(), dtype=int64)
Times in: tf.Tensor([18201 18225 18249], shape=(3,), dtype=int64) Times out: tf.Tensor(18249, shape=(), dtype=int64)
Times in: tf.Tensor([31630 31654 31678], shape=(3,), dtype=int64) Times out: tf.Tensor(31678, shape=(), dtype=int64)
Times in: tf.Tensor([-31  -7  17], shape=(3,), dtype=int64) Times out: tf.Tensor(17, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_351&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_352 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_702 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_703 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_351 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_702 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_351 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_703 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.2647 - mse: 19.2150 - mae: 3.3915 - val_loss: 15.9050 - val_mse: 15.8453 - val_mae: 3.1767
Epoch 2/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 7s 2ms/step - loss: 15.3027 - mse: 15.2387 - mae: 3.0455 - val_loss: 15.3066 - val_mse: 15.2392 - val_mae: 3.1149
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.8109 - mse: 14.7399 - mae: 2.9982 - val_loss: 14.9503 - val_mse: 14.8761 - val_mae: 3.0836
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.5695 - mse: 14.4920 - mae: 2.9734 - val_loss: 14.6391 - val_mse: 14.5592 - val_mae: 3.0528
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.3681 - mse: 14.2851 - mae: 2.9532 - val_loss: 14.5211 - val_mse: 14.4362 - val_mae: 3.0409
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.2525 - mse: 14.1651 - mae: 2.9409 - val_loss: 14.2999 - val_mse: 14.2109 - val_mae: 3.0182
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.1500 - mse: 14.0588 - mae: 2.9338 - val_loss: 14.2571 - val_mse: 14.1647 - val_mae: 3.0115
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.0042 - mse: 13.9096 - mae: 2.9193 - val_loss: 14.1423 - val_mse: 14.0465 - val_mae: 3.0013
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.8888 - mse: 13.7912 - mae: 2.9102 - val_loss: 14.0503 - val_mse: 13.9520 - val_mae: 2.9904
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.8382 - mse: 13.7380 - mae: 2.9028 - val_loss: 14.0457 - val_mse: 13.9449 - val_mae: 2.9883
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7280 - mse: 13.6255 - mae: 2.8921 - val_loss: 13.9435 - val_mse: 13.8403 - val_mae: 2.9771
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7013 - mse: 13.5966 - mae: 2.8880 - val_loss: 13.9702 - val_mse: 13.8648 - val_mae: 2.9807
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6301 - mse: 13.5233 - mae: 2.8810 - val_loss: 13.8577 - val_mse: 13.7505 - val_mae: 2.9689
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6065 - mse: 13.4978 - mae: 2.8801 - val_loss: 13.8682 - val_mse: 13.7587 - val_mae: 2.9678
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.5299 - mse: 13.4194 - mae: 2.8720 - val_loss: 13.7753 - val_mse: 13.6641 - val_mae: 2.9580
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4510 - mse: 13.3387 - mae: 2.8604 - val_loss: 13.8087 - val_mse: 13.6960 - val_mae: 2.9611
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4052 - mse: 13.2913 - mae: 2.8576 - val_loss: 13.7178 - val_mse: 13.6034 - val_mae: 2.9513
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.3361 - mse: 13.2206 - mae: 2.8496 - val_loss: 13.7046 - val_mse: 13.5887 - val_mae: 2.9481
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.2698 - mse: 13.1529 - mae: 2.8418 - val_loss: 13.6884 - val_mse: 13.5711 - val_mae: 2.9481
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.3125 - mse: 13.1941 - mae: 2.8437 - val_loss: 13.7791 - val_mse: 13.6602 - val_mae: 2.9571
bias 0.0008488307
si 0.57432455
rmse 0.03695974
kgeprime [0.76645789]
rmse_95 0.053167537
rmse_99 0.0625833
pearson 0.8005181970678517
pearson_95 0.3669250487387974
pearson_99 0.26124079969267155
rscore 0.6384367502013641
rscore_95 -4.99155642042826
rscore_99 -33.12825766961977
nse [0.63843675]
nse_95 [-4.99155642]
nse_99 [-33.12825767]
kge [0.74492724]
ext_kge_95 [0.19699759]
ext_kge_99 [-0.13751868]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([185723 185747 185771], shape=(3,), dtype=int64) Times out: tf.Tensor(185771, shape=(), dtype=int64)
Times in: tf.Tensor([155676 155700 155724], shape=(3,), dtype=int64) Times out: tf.Tensor(155724, shape=(), dtype=int64)
Times in: tf.Tensor([138294 138318 138342], shape=(3,), dtype=int64) Times out: tf.Tensor(138342, shape=(), dtype=int64)
Times in: tf.Tensor([72340 72364 72388], shape=(3,), dtype=int64) Times out: tf.Tensor(72388, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_352&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_353 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_704 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_705 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_352 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_704 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_352 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_705 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.6106 - mse: 18.5640 - mae: 3.3678 - val_loss: 14.8006 - val_mse: 14.7461 - val_mae: 3.0089
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.4034 - mse: 15.3433 - mae: 3.0834 - val_loss: 14.0551 - val_mse: 13.9894 - val_mae: 2.9298
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.8747 - mse: 14.8037 - mae: 3.0260 - val_loss: 13.8308 - val_mse: 13.7551 - val_mae: 2.9122
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.5838 - mse: 14.5038 - mae: 2.9946 - val_loss: 13.6047 - val_mse: 13.5209 - val_mae: 2.8896
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.4506 - mse: 14.3635 - mae: 2.9794 - val_loss: 13.4018 - val_mse: 13.3117 - val_mae: 2.8683
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.3117 - mse: 14.2191 - mae: 2.9687 - val_loss: 13.2632 - val_mse: 13.1684 - val_mae: 2.8557
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.2084 - mse: 14.1116 - mae: 2.9593 - val_loss: 13.2135 - val_mse: 13.1145 - val_mae: 2.8543
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1186 - mse: 14.0180 - mae: 2.9472 - val_loss: 13.1728 - val_mse: 13.0704 - val_mae: 2.8506
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.0207 - mse: 13.9165 - mae: 2.9395 - val_loss: 13.0185 - val_mse: 12.9127 - val_mae: 2.8336
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.9463 - mse: 13.8389 - mae: 2.9314 - val_loss: 13.0705 - val_mse: 12.9616 - val_mae: 2.8410
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.9138 - mse: 13.8033 - mae: 2.9261 - val_loss: 12.9746 - val_mse: 12.8629 - val_mae: 2.8311
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7911 - mse: 13.6778 - mae: 2.9145 - val_loss: 12.8690 - val_mse: 12.7544 - val_mae: 2.8187
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7157 - mse: 13.5996 - mae: 2.9022 - val_loss: 12.9502 - val_mse: 12.8327 - val_mae: 2.8290
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7180 - mse: 13.5992 - mae: 2.9034 - val_loss: 12.6692 - val_mse: 12.5491 - val_mae: 2.7931
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6454 - mse: 13.5242 - mae: 2.8969 - val_loss: 12.6716 - val_mse: 12.5495 - val_mae: 2.7923
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6046 - mse: 13.4815 - mae: 2.8911 - val_loss: 12.7868 - val_mse: 12.6627 - val_mae: 2.8077
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5480 - mse: 13.4229 - mae: 2.8851 - val_loss: 12.5051 - val_mse: 12.3790 - val_mae: 2.7743
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4852 - mse: 13.3579 - mae: 2.8796 - val_loss: 12.5209 - val_mse: 12.3926 - val_mae: 2.7723
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4720 - mse: 13.3428 - mae: 2.8763 - val_loss: 12.5183 - val_mse: 12.3885 - val_mae: 2.7770
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4377 - mse: 13.3068 - mae: 2.8734 - val_loss: 12.4447 - val_mse: 12.3130 - val_mae: 2.7644
bias 0.00027080026
si 0.5476852
rmse 0.035089906
kgeprime [0.74078965]
rmse_95 0.053723402
rmse_99 0.07323013
pearson 0.816249443171345
pearson_95 0.5164512135874668
pearson_99 0.8126380496104927
rscore 0.666154061783298
rscore_95 -1.5110697385037026
rscore_99 -3.0212293466173072
nse [0.66615406]
nse_95 [-1.51106974]
nse_99 [-3.02122935]
kge [0.73304521]
ext_kge_95 [0.45073291]
ext_kge_99 [0.61459078]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -40.43 -40.12 -39.81 ... -34.19 -33.88
  * longitude       (longitude) float32 172.2 172.5 172.8 ... 178.1 178.4 178.8
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -1.109 -1.25 ... 7.283
    vgrd10m         (time, latitude, longitude) float32 -1.97 -1.899 ... -2.948
    uw2             (time, latitude, longitude) float32 1.229 1.562 ... 53.04
    vw2             (time, latitude, longitude) float32 3.88 3.607 ... 8.692
    wind_magnitude  (time, latitude, longitude) float32 2.26 2.274 ... 7.857
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 

Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([20610 20634 20658], shape=(3,), dtype=int64) Times out: tf.Tensor(20658, shape=(), dtype=int64)
Times in: tf.Tensor([148665 148689 148713], shape=(3,), dtype=int64) Times out: tf.Tensor(148713, shape=(), dtype=int64)
Times in: tf.Tensor([13558 13582 13606], shape=(3,), dtype=int64) Times out: tf.Tensor(13606, shape=(), dtype=int64)
Times in: tf.Tensor([52514 52538 52562], shape=(3,), dtype=int64) Times out: tf.Tensor(52562, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_353&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_354 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_706 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_707 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_353 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_706 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_353 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_707 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.7159 - mse: 18.6723 - mae: 3.3300 - val_loss: 11.5539 - val_mse: 11.5015 - val_mae: 2.6477
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.9085 - mse: 14.8506 - mae: 3.0033 - val_loss: 11.2232 - val_mse: 11.1605 - val_mae: 2.6054
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.5390 - mse: 14.4719 - mae: 2.9663 - val_loss: 11.0595 - val_mse: 10.9883 - val_mae: 2.5869
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.2715 - mse: 14.1965 - mae: 2.9376 - val_loss: 11.0075 - val_mse: 10.9287 - val_mae: 2.5797
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.1481 - mse: 14.0661 - mae: 2.9240 - val_loss: 10.9758 - val_mse: 10.8907 - val_mae: 2.5822
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.9457 - mse: 13.8577 - mae: 2.9028 - val_loss: 10.8136 - val_mse: 10.7230 - val_mae: 2.5651
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.8588 - mse: 13.7657 - mae: 2.8904 - val_loss: 10.7466 - val_mse: 10.6513 - val_mae: 2.5548
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6960 - mse: 13.5984 - mae: 2.8749 - val_loss: 10.7683 - val_mse: 10.6690 - val_mae: 2.5547
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6187 - mse: 13.5176 - mae: 2.8665 - val_loss: 10.6462 - val_mse: 10.5436 - val_mae: 2.5406
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5776 - mse: 13.4734 - mae: 2.8612 - val_loss: 10.5757 - val_mse: 10.4702 - val_mae: 2.5299
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4537 - mse: 13.3467 - mae: 2.8476 - val_loss: 10.5497 - val_mse: 10.4414 - val_mae: 2.5260
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.4364 - mse: 13.3270 - mae: 2.8439 - val_loss: 10.6415 - val_mse: 10.5310 - val_mae: 2.5362
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.3647 - mse: 13.2532 - mae: 2.8364 - val_loss: 10.4840 - val_mse: 10.3717 - val_mae: 2.5167
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.2557 - mse: 13.1423 - mae: 2.8282 - val_loss: 10.5166 - val_mse: 10.4024 - val_mae: 2.5244
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.2221 - mse: 13.1069 - mae: 2.8234 - val_loss: 10.4882 - val_mse: 10.3723 - val_mae: 2.5173
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.1951 - mse: 13.0783 - mae: 2.8179 - val_loss: 10.4403 - val_mse: 10.3227 - val_mae: 2.5149
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.1275 - mse: 13.0093 - mae: 2.8150 - val_loss: 10.4416 - val_mse: 10.3227 - val_mae: 2.5157
Epoch 18/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.1074 - mse: 12.9877 - mae: 2.8116 - val_loss: 10.3946 - val_mse: 10.2742 - val_mae: 2.5081
Epoch 19/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.0463 - mse: 12.9251 - mae: 2.8016 - val_loss: 10.4128 - val_mse: 10.2912 - val_mae: 2.5127
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 12.9808 - mse: 12.8585 - mae: 2.7962 - val_loss: 10.5187 - val_mse: 10.3958 - val_mae: 2.5251
bias 0.0028558597
si 0.5237085
rmse 0.032242496
kgeprime [0.76485788]
rmse_95 0.05089842
rmse_99 0.058986645
pearson 0.8317336068895583
pearson_95 0.702391546967058
pearson_99 0.7513039191836713
rscore 0.6871032639797432
rscore_95 -1.5007126924474945
rscore_99 -5.847659380965035
nse [0.68710326]
nse_95 [-1.50071269]
nse_99 [-5.84765938]
kge [0.69450097]
ext_kge_95 [0.56378396]
ext_kge_99 [0.31552617]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([20213 20237 20261], shape=(3,), dtype=int64) Times out: tf.Tensor(20261, shape=(), dtype=int64)
Times in: tf.Tensor([110342 110366 110390], shape=(3,), dtype=int64) Times out: tf.Tensor(110390, shape=(), dtype=int64)
Times in: tf.Tensor([39641 39665 39689], shape=(3,), dtype=int64) Times out: tf.Tensor(39689, shape=(), dtype=int64)
Times in: tf.Tensor([72033 72057 72081], shape=(3,), dtype=int64) Times out: tf.Tensor(72081, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_354&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_355 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_708 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_709 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_354 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_708 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_354 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_709 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 17.4737 - mse: 17.4357 - mae: 3.2255 - val_loss: 14.4018 - val_mse: 14.3562 - val_mae: 2.9228
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 14.0829 - mse: 14.0330 - mae: 2.9249 - val_loss: 13.9838 - val_mse: 13.9308 - val_mae: 2.8854
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.7075 - mse: 13.6507 - mae: 2.8826 - val_loss: 13.5406 - val_mse: 13.4813 - val_mae: 2.8464
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.5082 - mse: 13.4454 - mae: 2.8621 - val_loss: 13.3366 - val_mse: 13.2717 - val_mae: 2.8282
Epoch 5/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.3090 - mse: 13.2411 - mae: 2.8402 - val_loss: 13.2148 - val_mse: 13.1451 - val_mae: 2.8174
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.1757 - mse: 13.1033 - mae: 2.8246 - val_loss: 13.1850 - val_mse: 13.1112 - val_mae: 2.8138
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.0398 - mse: 12.9636 - mae: 2.8123 - val_loss: 13.0372 - val_mse: 12.9598 - val_mae: 2.7984
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.9498 - mse: 12.8703 - mae: 2.8037 - val_loss: 13.1352 - val_mse: 13.0546 - val_mae: 2.8103
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 12.8612 - mse: 12.7786 - mae: 2.7921 - val_loss: 12.8480 - val_mse: 12.7646 - val_mae: 2.7810
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.8081 - mse: 12.7228 - mae: 2.7843 - val_loss: 12.8687 - val_mse: 12.7826 - val_mae: 2.7848
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.6938 - mse: 12.6060 - mae: 2.7718 - val_loss: 12.7835 - val_mse: 12.6949 - val_mae: 2.7773
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.6297 - mse: 12.5395 - mae: 2.7657 - val_loss: 12.7820 - val_mse: 12.6911 - val_mae: 2.7823
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.5891 - mse: 12.4967 - mae: 2.7603 - val_loss: 12.7479 - val_mse: 12.6547 - val_mae: 2.7772
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.5216 - mse: 12.4270 - mae: 2.7541 - val_loss: 12.7041 - val_mse: 12.6089 - val_mae: 2.7730
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.4332 - mse: 12.3366 - mae: 2.7461 - val_loss: 12.7603 - val_mse: 12.6632 - val_mae: 2.7847
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.3957 - mse: 12.2972 - mae: 2.7400 - val_loss: 12.6255 - val_mse: 12.5263 - val_mae: 2.7703
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.3035 - mse: 12.2029 - mae: 2.7312 - val_loss: 12.6321 - val_mse: 12.5308 - val_mae: 2.7728
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.2650 - mse: 12.1624 - mae: 2.7261 - val_loss: 12.5715 - val_mse: 12.4684 - val_mae: 2.7669
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 12.1895 - mse: 12.0850 - mae: 2.7188 - val_loss: 12.7132 - val_mse: 12.6082 - val_mae: 2.7865
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 12.1783 - mse: 12.0719 - mae: 2.7156 - val_loss: 12.5830 - val_mse: 12.4762 - val_mae: 2.7692
bias 0.0013264944
si 0.52037686
rmse 0.035321724
kgeprime [0.76334746]
rmse_95 0.0564973
rmse_99 0.071372695
pearson 0.8376087692777018
pearson_95 0.6211023102088967
pearson_99 0.7577929336474256
rscore 0.6986486492299179
rscore_95 -0.7077236318679223
rscore_99 -1.0640179446313032
nse [0.69864865]
nse_95 [-0.70772363]
nse_99 [-1.06401794]
kge [0.72547694]
ext_kge_95 [0.53524853]
ext_kge_99 [0.6389753]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([62886 62910 62934], shape=(3,), dtype=int64) Times out: tf.Tensor(62934, shape=(), dtype=int64)
Times in: tf.Tensor([36250 36274 36298], shape=(3,), dtype=int64) Times out: tf.Tensor(36298, shape=(), dtype=int64)
Times in: tf.Tensor([29989 30013 30037], shape=(3,), dtype=int64) Times out: tf.Tensor(30037, shape=(), dtype=int64)
Times in: tf.Tensor([73050 73074 73098], shape=(3,), dtype=int64) Times out: tf.Tensor(73098, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_355&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_356 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_710 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_711 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_355 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_710 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_355 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_711 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 19.2194 - mse: 19.1738 - mae: 3.3809 - val_loss: 14.7839 - val_mse: 14.7271 - val_mae: 2.9854
Epoch 2/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4856/4856 [==============================] - 7s 2ms/step - loss: 14.8953 - mse: 14.8350 - mae: 3.0034 - val_loss: 14.1194 - val_mse: 14.0557 - val_mae: 2.9105
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.6065 - mse: 14.5412 - mae: 2.9711 - val_loss: 13.9427 - val_mse: 13.8758 - val_mae: 2.8924
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.4323 - mse: 14.3640 - mae: 2.9535 - val_loss: 13.7436 - val_mse: 13.6740 - val_mae: 2.8780
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.3214 - mse: 14.2505 - mae: 2.9407 - val_loss: 13.6311 - val_mse: 13.5589 - val_mae: 2.8612
Epoch 6/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.2301 - mse: 14.1570 - mae: 2.9311 - val_loss: 13.7464 - val_mse: 13.6725 - val_mae: 2.8595
Epoch 7/20
4856/4856 [==============================] - 7s 2ms/step - loss: 14.0839 - mse: 14.0089 - mae: 2.9180 - val_loss: 13.5638 - val_mse: 13.4879 - val_mae: 2.8353
Epoch 8/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.9911 - mse: 13.9144 - mae: 2.9053 - val_loss: 13.5927 - val_mse: 13.5152 - val_mae: 2.8418
Epoch 9/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.9803 - mse: 13.9018 - mae: 2.9026 - val_loss: 13.4223 - val_mse: 13.3430 - val_mae: 2.8441
Epoch 10/20
4856/4856 [==============================] - 7s 2ms/step - loss: 13.8706 - mse: 13.7903 - mae: 2.8943 - val_loss: 13.3376 - val_mse: 13.2566 - val_mae: 2.8331
Epoch 11/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.8405 - mse: 13.7585 - mae: 2.8896 - val_loss: 13.5507 - val_mse: 13.4678 - val_mae: 2.8168
Epoch 12/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.7722 - mse: 13.6882 - mae: 2.8831 - val_loss: 13.2382 - val_mse: 13.1536 - val_mae: 2.8006
Epoch 13/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.6751 - mse: 13.5892 - mae: 2.8744 - val_loss: 13.2591 - val_mse: 13.1725 - val_mae: 2.8018
Epoch 14/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.6574 - mse: 13.5698 - mae: 2.8704 - val_loss: 13.3129 - val_mse: 13.2249 - val_mae: 2.8014
Epoch 15/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.6050 - mse: 13.5159 - mae: 2.8658 - val_loss: 12.9801 - val_mse: 12.8905 - val_mae: 2.7670
Epoch 16/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.4983 - mse: 13.4077 - mae: 2.8538 - val_loss: 13.0238 - val_mse: 12.9326 - val_mae: 2.7898
Epoch 17/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.4838 - mse: 13.3917 - mae: 2.8531 - val_loss: 13.0336 - val_mse: 12.9410 - val_mae: 2.7679
Epoch 18/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.4050 - mse: 13.3116 - mae: 2.8440 - val_loss: 12.8909 - val_mse: 12.7970 - val_mae: 2.7583
Epoch 19/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.3651 - mse: 13.2703 - mae: 2.8375 - val_loss: 12.8858 - val_mse: 12.7905 - val_mae: 2.7507
Epoch 20/20
4856/4856 [==============================] - 8s 2ms/step - loss: 13.2925 - mse: 13.1965 - mae: 2.8279 - val_loss: 12.7240 - val_mse: 12.6279 - val_mae: 2.7579
bias -0.0029399565
si 0.5454118
rmse 0.0355357
kgeprime [0.60716053]
rmse_95 0.06489226
rmse_99 0.109665856
pearson 0.8189405695413144
pearson_95 0.19137923162544598
pearson_99 -0.5851625431778106
rscore 0.6667748321501686
rscore_95 -3.945424165716118
rscore_99 -35.52195206827743
nse [0.66677483]
nse_95 [-3.94542417]
nse_99 [-35.52195207]
kge [0.68068009]
ext_kge_95 [0.02911203]
ext_kge_99 [-2.1072753]
Returning fold  3  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 1999-05-08 14:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([16432 16456 16480], shape=(3,), dtype=int64) Times out: tf.Tensor(16480, shape=(), dtype=int64)
Times in: tf.Tensor([33723 33747 33771], shape=(3,), dtype=int64) Times out: tf.Tensor(33771, shape=(), dtype=int64)
Times in: tf.Tensor([10000 10024 10048], shape=(3,), dtype=int64) Times out: tf.Tensor(10048, shape=(), dtype=int64)
Times in: tf.Tensor([9702 9726 9750], shape=(3,), dtype=int64) Times out: tf.Tensor(9750, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2003-10-14 05:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_356&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_357 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_712 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_713 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_356 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_712 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_356 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_713 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4855/4855 [==============================] - 8s 2ms/step - loss: 19.4111 - mse: 19.3809 - mae: 3.3959 - val_loss: 16.1008 - val_mse: 16.0653 - val_mae: 3.1867
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 15.3025 - mse: 15.2633 - mae: 3.0318 - val_loss: 15.1811 - val_mse: 15.1389 - val_mae: 3.0884
Epoch 3/20
4855/4855 [==============================] - 8s 2ms/step - loss: 14.6411 - mse: 14.5962 - mae: 2.9606 - val_loss: 15.3761 - val_mse: 15.3287 - val_mae: 3.1081
Epoch 4/20
4855/4855 [==============================] - 8s 2ms/step - loss: 14.3934 - mse: 14.3437 - mae: 2.9333 - val_loss: 14.8857 - val_mse: 14.8340 - val_mae: 3.0582
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 14.1271 - mse: 14.0734 - mae: 2.9063 - val_loss: 14.5977 - val_mse: 14.5425 - val_mae: 3.0239
Epoch 6/20
4855/4855 [==============================] - 8s 2ms/step - loss: 14.0189 - mse: 13.9623 - mae: 2.8935 - val_loss: 14.2548 - val_mse: 14.1971 - val_mae: 2.9887
Epoch 7/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.9021 - mse: 13.8431 - mae: 2.8816 - val_loss: 14.4369 - val_mse: 14.3768 - val_mae: 3.0047
Epoch 8/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.8510 - mse: 13.7899 - mae: 2.8744 - val_loss: 14.2534 - val_mse: 14.1913 - val_mae: 2.9847
Epoch 9/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.7885 - mse: 13.7255 - mae: 2.8689 - val_loss: 13.9861 - val_mse: 13.9226 - val_mae: 2.9578
Epoch 10/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.7284 - mse: 13.6637 - mae: 2.8641 - val_loss: 13.9493 - val_mse: 13.8837 - val_mae: 2.9490
Epoch 11/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.6448 - mse: 13.5780 - mae: 2.8552 - val_loss: 14.1099 - val_mse: 14.0423 - val_mae: 2.9668
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.6118 - mse: 13.5434 - mae: 2.8500 - val_loss: 13.6859 - val_mse: 13.6169 - val_mae: 2.9187
Epoch 13/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.5233 - mse: 13.4533 - mae: 2.8423 - val_loss: 14.2843 - val_mse: 14.2138 - val_mae: 2.9827
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.4833 - mse: 13.4119 - mae: 2.8411 - val_loss: 13.9954 - val_mse: 13.9235 - val_mae: 2.9554
Epoch 15/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 8s 2ms/step - loss: 13.4414 - mse: 13.3687 - mae: 2.8359 - val_loss: 13.6427 - val_mse: 13.5695 - val_mae: 2.9170
Epoch 16/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.3587 - mse: 13.2847 - mae: 2.8242 - val_loss: 13.7109 - val_mse: 13.6365 - val_mae: 2.9226
Epoch 17/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.3279 - mse: 13.2526 - mae: 2.8225 - val_loss: 13.9162 - val_mse: 13.8405 - val_mae: 2.9449
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 13.2357 - mse: 13.1593 - mae: 2.8130 - val_loss: 13.6700 - val_mse: 13.5931 - val_mae: 2.9171
Epoch 19/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.2879 - mse: 13.2102 - mae: 2.8184 - val_loss: 13.6206 - val_mse: 13.5425 - val_mae: 2.9172
Epoch 20/20
4855/4855 [==============================] - 8s 2ms/step - loss: 13.2166 - mse: 13.1378 - mae: 2.8111 - val_loss: 14.1343 - val_mse: 14.0551 - val_mae: 2.9699
bias -0.009228307
si 0.5697883
rmse 0.037490144
kgeprime [0.32959812]
rmse_95 0.04685415
rmse_99 0.05718353
pearson 0.8007153369478832
pearson_95 0.5814230126488326
pearson_99 0.6407058894194055
rscore 0.6174973987061352
rscore_95 -1.4852782252861574
rscore_99 -2.5125940377099276
nse [0.6174974]
nse_95 [-1.48527823]
nse_99 [-2.51259404]
kge [0.46307295]
ext_kge_95 [0.46822958]
ext_kge_99 [0.51308293]
Returning fold  4  of  5  e.g. 80.0 percent training data

1999-05-08 14:00:00 2017-02-01 01:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([40945 40969 40993], shape=(3,), dtype=int64) Times out: tf.Tensor(40993, shape=(), dtype=int64)
Times in: tf.Tensor([105685 105709 105733], shape=(3,), dtype=int64) Times out: tf.Tensor(105733, shape=(), dtype=int64)
Times in: tf.Tensor([116245 116269 116293], shape=(3,), dtype=int64) Times out: tf.Tensor(116293, shape=(), dtype=int64)
Times in: tf.Tensor([123729 123753 123777], shape=(3,), dtype=int64) Times out: tf.Tensor(123777, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_357&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_358 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_714 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_715 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_357 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_714 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_357 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_715 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 20.4674 - mse: 20.4212 - mae: 3.4650 - val_loss: 13.5711 - val_mse: 13.5136 - val_mae: 2.8689
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 15.1728 - mse: 15.1118 - mae: 3.0222 - val_loss: 12.9221 - val_mse: 12.8585 - val_mae: 2.7929
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.6914 - mse: 14.6251 - mae: 2.9723 - val_loss: 12.7767 - val_mse: 12.7086 - val_mae: 2.7771
Epoch 4/20
4857/4857 [==============================] - 8s 2ms/step - loss: 14.4805 - mse: 14.4103 - mae: 2.9529 - val_loss: 12.4742 - val_mse: 12.4024 - val_mae: 2.7391
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.2910 - mse: 14.2174 - mae: 2.9315 - val_loss: 12.2826 - val_mse: 12.2074 - val_mae: 2.7208
Epoch 6/20
4857/4857 [==============================] - 8s 2ms/step - loss: 14.2796 - mse: 14.2027 - mae: 2.9314 - val_loss: 12.2712 - val_mse: 12.1929 - val_mae: 2.7219
Epoch 7/20
4857/4857 [==============================] - 8s 2ms/step - loss: 14.1596 - mse: 14.0799 - mae: 2.9163 - val_loss: 12.2290 - val_mse: 12.1481 - val_mae: 2.7154
Epoch 8/20
4857/4857 [==============================] - 8s 2ms/step - loss: 14.0713 - mse: 13.9888 - mae: 2.9128 - val_loss: 12.0889 - val_mse: 12.0052 - val_mae: 2.7043
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 14.0139 - mse: 13.9289 - mae: 2.9043 - val_loss: 12.0118 - val_mse: 11.9258 - val_mae: 2.6955
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.9651 - mse: 13.8780 - mae: 2.9007 - val_loss: 12.0004 - val_mse: 11.9125 - val_mae: 2.6957
Epoch 11/20
4857/4857 [==============================] - 8s 2ms/step - loss: 13.9516 - mse: 13.8625 - mae: 2.8973 - val_loss: 12.0087 - val_mse: 11.9189 - val_mae: 2.6957
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.8708 - mse: 13.7800 - mae: 2.8871 - val_loss: 11.9262 - val_mse: 11.8347 - val_mae: 2.6896
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.8197 - mse: 13.7274 - mae: 2.8850 - val_loss: 11.9352 - val_mse: 11.8423 - val_mae: 2.6894
Epoch 14/20
4857/4857 [==============================] - 8s 2ms/step - loss: 13.8277 - mse: 13.7338 - mae: 2.8854 - val_loss: 11.8149 - val_mse: 11.7205 - val_mae: 2.6778
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7475 - mse: 13.6521 - mae: 2.8766 - val_loss: 11.8337 - val_mse: 11.7377 - val_mae: 2.6759
Epoch 16/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.7227 - mse: 13.6260 - mae: 2.8739 - val_loss: 11.7597 - val_mse: 11.6623 - val_mae: 2.6672
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.6426 - mse: 13.5445 - mae: 2.8646 - val_loss: 11.7585 - val_mse: 11.6597 - val_mae: 2.6690
Epoch 18/20
4857/4857 [==============================] - 8s 2ms/step - loss: 13.6412 - mse: 13.5417 - mae: 2.8657 - val_loss: 11.9164 - val_mse: 11.8164 - val_mae: 2.6857
Epoch 19/20
4857/4857 [==============================] - 8s 2ms/step - loss: 13.6092 - mse: 13.5083 - mae: 2.8610 - val_loss: 11.6003 - val_mse: 11.4986 - val_mae: 2.6511
Epoch 20/20
4857/4857 [==============================] - 7s 2ms/step - loss: 13.5940 - mse: 13.4917 - mae: 2.8564 - val_loss: 11.6370 - val_mse: 11.5340 - val_mae: 2.6577
bias -0.0021092624
si 0.5363052
rmse 0.033961788
kgeprime [0.63169893]
rmse_95 0.057496704
rmse_99 0.07496148
pearson 0.824807608263983
pearson_95 0.5245812009636195
pearson_99 0.3643965336269899
rscore 0.674970520439704
rscore_95 -2.340593619930125
rscore_99 -4.194789605695198
nse [0.67497052]
nse_95 [-2.34059362]
nse_99 [-4.19478961]
kge [0.68721814]
ext_kge_95 [0.38289842]
ext_kge_99 [0.27752496]
Loading U
Done
Loading V
Done
Calculating relative winds

 calculating winds with: 

 &lt;xarray.Dataset&gt;
Dimensions:         (latitude: 22, longitude: 22, time: 195745)
Coordinates:
  * latitude        (latitude) float32 -40.43 -40.12 -39.81 ... -34.19 -33.88
  * longitude       (longitude) float32 171.2 171.6 171.9 ... 177.2 177.5 177.8
  * time            (time) datetime64[ns] 1994-11-01 ... 2017-03-01
Data variables:
    ugrd10m         (time, latitude, longitude) float32 -2.06 -1.348 ... 7.381
    vgrd10m         (time, latitude, longitude) float32 -2.559 -2.299 ... -3.26
    uw2             (time, latitude, longitude) float32 4.242 1.817 ... 54.48
    vw2             (time, latitude, longitude) float32 6.551 5.284 ... 10.63
    wind_magnitude  (time, latitude, longitude) float32 3.285 2.665 ... 8.069
Attributes:
    short_name:  ugrd10m
    long_name:   U-Component of Wind
    level:       10 m above ground
    units:       m/s 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Done
Clearing U,V
Done
Loading MSLP
Done

 adding the wind to the predictor... 


 calculating the gradient of the sea-level-pressure fields... 


 pressure/gradient predictor both with shape: 
 (195745, 10, 10) 

Returning fold  0  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2012-08-26 10:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([151797 151821 151845], shape=(3,), dtype=int64) Times out: tf.Tensor(151845, shape=(), dtype=int64)
Times in: tf.Tensor([9325 9349 9373], shape=(3,), dtype=int64) Times out: tf.Tensor(9373, shape=(), dtype=int64)
Times in: tf.Tensor([39357 39381 39405], shape=(3,), dtype=int64) Times out: tf.Tensor(39405, shape=(), dtype=int64)
Times in: tf.Tensor([91866 91890 91914], shape=(3,), dtype=int64) Times out: tf.Tensor(91914, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_358&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_359 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_716 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_717 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_358 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_716 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_358 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_717 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4857/4857 [==============================] - 8s 2ms/step - loss: 28.3108 - mse: 28.2699 - mae: 4.0558 - val_loss: 18.4017 - val_mse: 18.3545 - val_mae: 3.3194
Epoch 2/20
4857/4857 [==============================] - 7s 2ms/step - loss: 22.0442 - mse: 21.9943 - mae: 3.6150 - val_loss: 17.4699 - val_mse: 17.4173 - val_mae: 3.2407
Epoch 3/20
4857/4857 [==============================] - 7s 2ms/step - loss: 21.0570 - mse: 21.0015 - mae: 3.5333 - val_loss: 16.9425 - val_mse: 16.8839 - val_mae: 3.2006
Epoch 4/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.5984 - mse: 20.5371 - mae: 3.4913 - val_loss: 16.6277 - val_mse: 16.5638 - val_mae: 3.1667
Epoch 5/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.3549 - mse: 20.2888 - mae: 3.4720 - val_loss: 16.6004 - val_mse: 16.5323 - val_mae: 3.1662
Epoch 6/20
4857/4857 [==============================] - 7s 2ms/step - loss: 20.0825 - mse: 20.0123 - mae: 3.4490 - val_loss: 16.1663 - val_mse: 16.0939 - val_mae: 3.1287
Epoch 7/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.8285 - mse: 19.7541 - mae: 3.4258 - val_loss: 15.8313 - val_mse: 15.7550 - val_mae: 3.0926
Epoch 8/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.5871 - mse: 19.5090 - mae: 3.4028 - val_loss: 15.5086 - val_mse: 15.4288 - val_mae: 3.0673
Epoch 9/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.3857 - mse: 19.3041 - mae: 3.3809 - val_loss: 15.2841 - val_mse: 15.2008 - val_mae: 3.0513
Epoch 10/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.2474 - mse: 19.1627 - mae: 3.3720 - val_loss: 15.2562 - val_mse: 15.1700 - val_mae: 3.0428
Epoch 11/20
4857/4857 [==============================] - 7s 2ms/step - loss: 19.0785 - mse: 18.9909 - mae: 3.3585 - val_loss: 15.1860 - val_mse: 15.0966 - val_mae: 3.0431
Epoch 12/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.9508 - mse: 18.8602 - mae: 3.3492 - val_loss: 14.7715 - val_mse: 14.6794 - val_mae: 2.9977
Epoch 13/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8304 - mse: 18.7371 - mae: 3.3421 - val_loss: 14.9002 - val_mse: 14.8055 - val_mae: 3.0138
Epoch 14/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.8170 - mse: 18.7211 - mae: 3.3379 - val_loss: 14.4617 - val_mse: 14.3643 - val_mae: 2.9644
Epoch 15/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.6963 - mse: 18.5978 - mae: 3.3261 - val_loss: 14.5694 - val_mse: 14.4697 - val_mae: 2.9792
Epoch 16/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.6105 - mse: 18.5095 - mae: 3.3179 - val_loss: 14.4620 - val_mse: 14.3596 - val_mae: 2.9679
Epoch 17/20
4857/4857 [==============================] - 7s 2ms/step - loss: 18.5811 - mse: 18.4776 - mae: 3.3132 - val_loss: 14.6575 - val_mse: 14.5527 - val_mae: 2.9901
Epoch 18/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.5169 - mse: 18.4109 - mae: 3.3104 - val_loss: 14.1613 - val_mse: 14.0541 - val_mae: 2.9348
Epoch 19/20
4857/4857 [==============================] - 8s 2ms/step - loss: 18.4681 - mse: 18.3597 - mae: 3.3056 - val_loss: 14.2926 - val_mse: 14.1829 - val_mae: 2.9498
Epoch 20/20
4857/4857 [==============================] - 9s 2ms/step - loss: 18.3393 - mse: 18.2286 - mae: 3.2923 - val_loss: 14.4168 - val_mse: 14.3047 - val_mae: 2.9668
bias -0.0006753856
si 0.45109713
rmse 0.037821546
kgeprime [0.7394549]
rmse_95 0.062452234
rmse_99 0.09272007
pearson 0.8841359275225792
pearson_95 0.6434411098209343
pearson_99 0.7357660145014733
rscore 0.7721682265910825
rscore_95 -0.744472479720617
rscore_99 -2.6231087702753095
nse [0.77216823]
nse_95 [-0.74447248]
nse_99 [-2.62310877]
kge [0.75618324]
ext_kge_95 [0.56469324]
ext_kge_99 [0.58888438]
Returning fold  1  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2008-03-20 19:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([76753 76777 76801], shape=(3,), dtype=int64) Times out: tf.Tensor(76801, shape=(), dtype=int64)
Times in: tf.Tensor([20383 20407 20431], shape=(3,), dtype=int64) Times out: tf.Tensor(20431, shape=(), dtype=int64)
Times in: tf.Tensor([116085 116109 116133], shape=(3,), dtype=int64) Times out: tf.Tensor(116133, shape=(), dtype=int64)
Times in: tf.Tensor([81767 81791 81815], shape=(3,), dtype=int64) Times out: tf.Tensor(81815, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2012-08-26 10:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_359&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_360 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_718 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_719 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_359 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_718 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_359 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_719 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4855/4855 [==============================] - 8s 2ms/step - loss: 27.1754 - mse: 27.1289 - mae: 3.9644 - val_loss: 21.0327 - val_mse: 20.9737 - val_mae: 3.5344
Epoch 2/20
4855/4855 [==============================] - 7s 2ms/step - loss: 20.1431 - mse: 20.0791 - mae: 3.4571 - val_loss: 19.9517 - val_mse: 19.8825 - val_mae: 3.4477
Epoch 3/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.4365 - mse: 19.3642 - mae: 3.3939 - val_loss: 19.2626 - val_mse: 19.1871 - val_mae: 3.3877
Epoch 4/20
4855/4855 [==============================] - 7s 2ms/step - loss: 19.2022 - mse: 19.1256 - mae: 3.3725 - val_loss: 19.4549 - val_mse: 19.3762 - val_mae: 3.4018
Epoch 5/20
4855/4855 [==============================] - 8s 2ms/step - loss: 18.9887 - mse: 18.9096 - mae: 3.3530 - val_loss: 19.2392 - val_mse: 19.1583 - val_mae: 3.3830
Epoch 6/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.9824 - mse: 18.9013 - mae: 3.3490 - val_loss: 19.1855 - val_mse: 19.1026 - val_mae: 3.3787
Epoch 7/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.7669 - mse: 18.6838 - mae: 3.3336 - val_loss: 19.1006 - val_mse: 19.0158 - val_mae: 3.3713
Epoch 8/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.6496 - mse: 18.5642 - mae: 3.3229 - val_loss: 18.5235 - val_mse: 18.4364 - val_mae: 3.3199
Epoch 9/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.5106 - mse: 18.4229 - mae: 3.3096 - val_loss: 18.2081 - val_mse: 18.1182 - val_mae: 3.2895
Epoch 10/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.3032 - mse: 18.2123 - mae: 3.2926 - val_loss: 18.2087 - val_mse: 18.1156 - val_mae: 3.2876
Epoch 11/20
4855/4855 [==============================] - 7s 2ms/step - loss: 18.1139 - mse: 18.0200 - mae: 3.2803 - val_loss: 17.7451 - val_mse: 17.6487 - val_mae: 3.2467
Epoch 12/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.9660 - mse: 17.8689 - mae: 3.2656 - val_loss: 17.5994 - val_mse: 17.4999 - val_mae: 3.2328
Epoch 13/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.8338 - mse: 17.7336 - mae: 3.2565 - val_loss: 17.7126 - val_mse: 17.6100 - val_mae: 3.2371
Epoch 14/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.6763 - mse: 17.5728 - mae: 3.2420 - val_loss: 17.2517 - val_mse: 17.1456 - val_mae: 3.1990
Epoch 15/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.5758 - mse: 17.4689 - mae: 3.2315 - val_loss: 17.6460 - val_mse: 17.5367 - val_mae: 3.2315
Epoch 16/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.5739 - mse: 17.4640 - mae: 3.2307 - val_loss: 17.3808 - val_mse: 17.2685 - val_mae: 3.2043
Epoch 17/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.4548 - mse: 17.3420 - mae: 3.2165 - val_loss: 17.2719 - val_mse: 17.1567 - val_mae: 3.1977
Epoch 18/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.4359 - mse: 17.3201 - mae: 3.2146 - val_loss: 17.0849 - val_mse: 16.9669 - val_mae: 3.1819
Epoch 19/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.3007 - mse: 17.1818 - mae: 3.2018 - val_loss: 17.1535 - val_mse: 17.0322 - val_mae: 3.1882
Epoch 20/20
4855/4855 [==============================] - 7s 2ms/step - loss: 17.2697 - mse: 17.1479 - mae: 3.2022 - val_loss: 17.0280 - val_mse: 16.9040 - val_mae: 3.1751
bias 0.001853318
si 0.45618463
rmse 0.041114435
kgeprime [0.82668201]
rmse_95 0.062087815
rmse_99 0.053222835
pearson 0.878177936269764
pearson_95 0.6945135109986368
pearson_99 0.3930548780657428
rscore 0.7690101767386174
rscore_95 -2.227794587699268
rscore_99 -8.431442575503734
nse [0.76901018]
nse_95 [-2.22779459]
nse_99 [-8.43144258]
kge [0.78587092]
ext_kge_95 [0.20731806]
ext_kge_99 [-0.80240425]
Returning fold  2  of  5  e.g. 80.0 percent training data

1994-12-01 00:00:00 2003-10-14 05:00:00
&lt;class &#39;numpy.ndarray&#39;&gt;
All integer correspond to number of hours with respect to reference date
Times in: tf.Tensor([1336 1360 1384], shape=(3,), dtype=int64) Times out: tf.Tensor(1384, shape=(), dtype=int64)
Times in: tf.Tensor([64985 65009 65033], shape=(3,), dtype=int64) Times out: tf.Tensor(65033, shape=(), dtype=int64)
Times in: tf.Tensor([71238 71262 71286], shape=(3,), dtype=int64) Times out: tf.Tensor(71286, shape=(), dtype=int64)
Times in: tf.Tensor([35992 36016 36040], shape=(3,), dtype=int64) Times out: tf.Tensor(36040, shape=(), dtype=int64)

&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
2008-03-20 19:00:00 2017-02-01 01:00:00
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
&lt;class &#39;xarray.core.dataarray.DataArray&#39;&gt;
Model: &quot;model_360&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_361 (InputLayer)       [(None, 3, 10, 10, 3)]    0         
_________________________________________________________________
time_distributed_720 (TimeDi (None, 3, 10, 10, 24)     672       
_________________________________________________________________
time_distributed_721 (TimeDi (None, 3, 5, 5, 24)       0         
_________________________________________________________________
flatten_360 (Flatten)        (None, 1800)              0         
_________________________________________________________________
dense_720 (Dense)            (None, 20)                36020     
_________________________________________________________________
dropout_360 (Dropout)        (None, 20)                0         
_________________________________________________________________
dense_721 (Dense)            (None, 1)                 21        
=================================================================
Total params: 36,713
Trainable params: 36,713
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/20
4856/4856 [==============================] - 8s 2ms/step - loss: 32.1387 - mse: 32.0979 - mae: 4.3075 - val_loss: 20.7360 - val_mse: 20.6841 - val_mae: 3.3998
Epoch 2/20
4856/4856 [==============================] - 7s 2ms/step - loss: 21.3423 - mse: 21.2889 - mae: 3.5746 - val_loss: 19.4074 - val_mse: 19.3521 - val_mae: 3.2817
Epoch 3/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.6696 - mse: 20.6135 - mae: 3.5192 - val_loss: 19.2919 - val_mse: 19.2338 - val_mae: 3.2791
Epoch 4/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.5424 - mse: 20.4837 - mae: 3.5072 - val_loss: 19.0642 - val_mse: 19.0036 - val_mae: 3.2531
Epoch 5/20
4856/4856 [==============================] - 7s 2ms/step - loss: 20.2264 - mse: 20.1653 - mae: 3.4823 - val_loss: 19.1669 - val_mse: 19.1043 - val_mae: 3.2663
Epoch 6/20
3021/4856 [=================&gt;............] - ETA: 2s - loss: 19.9567 - mse: 19.8942 - mae: 3.4661
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;pearson&#39;</span>
<span class="n">site_id</span> <span class="o">=</span> <span class="mi">116</span>
<span class="c1">#for fold, res in new_results[site_id].items():</span>
<span class="k">for</span> <span class="n">site_id</span> <span class="ow">in</span> <span class="n">new_results</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">site_id</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">new_results</span><span class="p">[</span><span class="n">site_id</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span> <span class="mi">3</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">new_results</span><span class="p">[</span><span class="n">site_id</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>116 [0.847 0.867 0.866 0.854 0.871] 0.8611770913274729
200 [0.853 0.883 0.861 0.851 0.869] 0.8632056424892649
224 [0.817 0.845 0.841 0.814 0.852] 0.8337756483409251
328 [0.814 0.839 0.82  0.797 0.836] 0.8211992680271774
393 [0.862 0.888 0.867 0.846 0.878] 0.8682299182188972
480 [0.814 0.832 0.808 0.775 0.813] 0.8085695716082986
488 [0.813 0.83  0.81  0.781 0.818] 0.8101712131604506
578 [0.807 0.83  0.825 0.779 0.815] 0.8110911228516129
613 [0.841 0.861 0.856 0.821 0.846] 0.8448179255251743
689 [0.84  0.856 0.847 0.818 0.848] 0.8415008402036503
708 [0.761 0.779 0.793 0.743 0.768] 0.7687048446076666
744 [0.758 0.781 0.79  0.741 0.764] 0.766885375028296
780 [0.833 0.848 0.831 0.801 0.838] 0.8303676838023997
803 [0.757 0.761 0.781 0.744 0.751] 0.7587142767975468
949 [0.783 0.802 0.786 0.761 0.779] 0.7820531250784312
999 [0.839 0.852 0.835 0.824 0.842] 0.8385857036833592
1025 [0.753 0.763 0.736 0.727 0.727] 0.7413224775402689
1064 [0.844 0.853 0.835 0.822 0.844] 0.839551039120131
1124 [0.705 0.722 0.725 0.694 0.693] 0.7078216268584046
1146 [0.72  0.734 0.721 0.706 0.688] 0.7135973888399679
1174 [0.734 0.748 0.746 0.705 0.728] 0.732272623606829
1177 [0.839 0.844 0.824 0.814 0.828] 0.8297387165090664
1214 [0.697 0.705 0.703 0.667 0.681] 0.6904741648410955
1217 [0.643 0.66  0.682 0.625 0.655] 0.6531601329722718
1260 [0.829 0.833 0.814 0.807 0.818] 0.8202487534441525
1296 [0.69  0.704 0.711 0.673 0.673] 0.6900572184850307
1327 [0.577 0.598 0.607 0.566 0.587] 0.5867976023675936
1442 [0.629 0.641 0.65  0.592 0.64 ] 0.6305863410713477
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.axes_grid1</span> <span class="kn">import</span> <span class="n">make_axes_locatable</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">linear_results</span> <span class="o">=</span> <span class="n">xr</span><span class="o">.</span><span class="n">open_dataset</span><span class="p">(</span><span class="s1">&#39;/home/metocean/geocean-nz-ss/data/statistics/experiments/experiment_linear_final_20211113.nc&#39;</span><span class="p">)</span>
<span class="n">best_linear_results</span> <span class="o">=</span> <span class="n">linear_results</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">winds</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tlapse</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">region</span><span class="o">=</span><span class="s1">&#39;local_2.5_2.5&#39;</span><span class="p">,</span> <span class="n">tresample</span><span class="o">=</span><span class="s1">&#39;1D&#39;</span><span class="p">)</span>
<span class="n">best_linear_results</span>

<span class="n">sites</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">best_linear_results</span><span class="o">.</span><span class="n">site</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">new_results</span><span class="p">]</span>
<span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;ext_kge_99&#39;</span>
<span class="n">metric</span> <span class="o">=</span> <span class="s1">&#39;pearson&#39;</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;pearson&#39;</span><span class="p">,</span> <span class="s1">&#39;si&#39;</span><span class="p">,</span> <span class="s1">&#39;rel_rmse&#39;</span><span class="p">,</span> <span class="s1">&#39;kgeprime&#39;</span><span class="p">]</span>

<span class="n">lons</span> <span class="o">=</span> <span class="n">ss_dset</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">sites</span><span class="p">)</span><span class="o">.</span><span class="n">lon</span><span class="o">.</span><span class="n">values</span>
<span class="n">lats</span> <span class="o">=</span> <span class="n">ss_dset</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">sites</span><span class="p">)</span><span class="o">.</span><span class="n">lat</span><span class="o">.</span><span class="n">values</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">4</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">im</span><span class="p">,</span> <span class="n">metric</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">metrics</span><span class="p">):</span>
    
    <span class="n">vals_linear</span> <span class="o">=</span> <span class="n">best_linear_results</span><span class="o">.</span><span class="n">sel</span><span class="p">(</span><span class="n">site</span><span class="o">=</span><span class="n">sites</span><span class="p">)[</span><span class="n">metric</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">vals_nn</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">new_results</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sites</span><span class="p">]</span>
    <span class="n">vals_nn_max</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">new_results</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sites</span><span class="p">]</span>
    <span class="n">vals_nn_min</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">new_results</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sites</span><span class="p">]</span>
    
    <span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">vals_linear</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">vals_nn</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">vals_nn_max</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">vals_nn_min</span><span class="p">)])</span>
    <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vals_linear</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vals_nn</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vals_nn_max</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">vals_nn_min</span><span class="p">)])</span>

    <span class="n">p</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lons</span><span class="p">,</span> <span class="n">lats</span><span class="p">,</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">vals_linear</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
    <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;5%&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Linear model: &quot;</span><span class="o">+</span><span class="n">metric</span><span class="p">)</span>

    <span class="n">p</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lons</span><span class="p">,</span> <span class="n">lats</span><span class="p">,</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">vals_nn</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
    <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;5%&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;CNN model: mean &quot;</span><span class="o">+</span><span class="n">metric</span><span class="p">)</span>

    <span class="n">p</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lons</span><span class="p">,</span> <span class="n">lats</span><span class="p">,</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">vals_nn_max</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
    <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;5%&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;CNN model max: &quot;</span><span class="o">+</span><span class="n">metric</span><span class="p">)</span>

    <span class="n">p</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lons</span><span class="p">,</span> <span class="n">lats</span><span class="p">,</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">vals_nn_min</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
    <span class="n">divider</span> <span class="o">=</span> <span class="n">make_axes_locatable</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">divider</span><span class="o">.</span><span class="n">append_axes</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="s1">&#39;5%&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">cax</span><span class="o">=</span><span class="n">cax</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">im</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;CNN model min: &quot;</span><span class="o">+</span><span class="n">metric</span><span class="p">)</span>

<span class="c1">#for v1,v2,v3 in zip(vals_nn, vals_nn_max, vals_nn_min):</span>
<span class="c1">#    print(v1,v2,v3)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/model_CNN-single_site_13_0.png" src="../_images/model_CNN-single_site_13_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#fig, axes = plt.subplots(figure=(16,4))</span>
<span class="c1">#axes.plot(np.concatenate([y.numpy()[:,0] for x, y in dset_val], axis=0))</span>
<span class="c1">#axes.plot(prediction_val[:,0])</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">fig</span><span class="p">,</span><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[:,</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dset_val</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">20000</span><span class="p">:</span><span class="mi">40000</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Truth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">prediction_val</span><span class="p">[</span><span class="mi">20000</span><span class="p">:</span><span class="mi">40000</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;CNN model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7f055a2e2a90&gt;
</pre></div>
</div>
<img alt="../_images/model_CNN-single_site_14_1.png" src="../_images/model_CNN-single_site_14_1.png" />
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "javitausia/geocean-nz-ss",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-.conda-ssnz-py"
        },
        kernelOptions: {
            kernelName: "conda-env-.conda-ssnz-py",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-.conda-ssnz-py'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Javier Tausa Hoyal<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>