{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff98c49",
   "metadata": {},
   "source": [
    "# RUN experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e2b76",
   "metadata": {},
   "source": [
    "In this notebook, we will run all the required experiments using the `Experiment` class, which can be found at `sscode/experiment.py`, and can be seen below:\n",
    "\n",
    "```python\n",
    "    class Experiment(object):\n",
    "        \"\"\"\n",
    "        This class Experiment summarizes all the previous work done with the linear and the\n",
    "        knn models, as this class allows the user to perform a detailed analysis of one\n",
    "        requested model given a set of parameters\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, slp_data, wind_data, ss_data, # this must have several stations\n",
    "                     sites_to_analyze: list = list(np.random.randint(10,1000,5)),\n",
    "                     model: str = 'linear', # this is the model to analyze\n",
    "                     model_metrics: list = [\n",
    "                         'bias','si','rmse','pearson','spearman','rscore',\n",
    "                         'mae', 'me', 'expl_var', # ...\n",
    "                     ], # these are the metrics to evaluate\n",
    "                     pca_attrs: dict = pca_attrs_default,\n",
    "                     model_attrs: dict = linear_attrs_default):\n",
    "            \"\"\"\n",
    "            As the initializator, the __init__ funciton creates the instance of the class,\n",
    "            given a set of parameters, which are described below\n",
    "\n",
    "            Args:\n",
    "                slp_data (xarray.Dataset): These are the sea-level-pressure fields, previously\n",
    "                    loaded with the Loader class, loader.predictor_slp!!\n",
    "                wind_data (xarray.Dataset): These are the wind fields, previously\n",
    "                    loaded with the Loader class, loader.predictor_wind!!\n",
    "                ss_data (xarray.Dataset): This is the storm surge from the moana hindcast, previously\n",
    "                    loaded with the Loader class, loader.predictand!!\n",
    "                sites_to_analyze (list, optional): This is the list with all the moana v2\n",
    "                    hindcast locations to analyze. Defaults to random locations.\n",
    "                model (str, optional): Type of model to analyze. Defaults to 'linear'.\n",
    "                model_metrics (list, optional): These are all the evaluation metrics that might\n",
    "                    be used to evaluate the model performance. Defaults to simple list.\n",
    "                pca_attrs (dict, optional): PCA dictionary with all the parameters to use.\n",
    "                    Defaults to pca_attrs_default.\n",
    "                model_attrs (dict, optional): Model dictionary with all the parameters to use. \n",
    "                    Defaults to linear_attrs_default.\n",
    "            \"\"\"\n",
    "\n",
    "            # lets build the experiment!!\n",
    "```\n",
    "\n",
    "where it can be seen how, given the sea-level-pressure / wind data, the storm surge (it is preferable to use the Moana v2 hindcast nearshore), all the individual sites to analyze, the type of model that will be used (with its evaluation metrics), and the PCA attributes and the MODEL attributes that will be used to calculate the model.\n",
    "\n",
    "Given these parameters, we construct the object of the class, and then running `execute_cross_model_calculations`, we can perform all the experiments given the possible combinations!!\n",
    "\n",
    "```{warning}\n",
    "Be careful with the [\"curse of dimensionality\"](https://en.wikipedia.org/wiki/Curse_of_dimensionality), as it seems very easy to add more options for the parameters of the models, but once we have more than 3-4 parameters with 2-3 different values, the number of models that will be performed can increase to hundreds!!\n",
    "```\n",
    "\n",
    "Below, different code cells with imports, loading functions, visualizations... will appear, but thw worflow is similar to all the notebooks in the repository. Once we load the data, we use the pre-built python classes to generate results!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829c7637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os, sys\n",
    "import progressbar\n",
    "\n",
    "# arrays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# append sscode to path\n",
    "sys.path.insert(0, os.path.join(os.path.abspath(''), '..'))\n",
    "\n",
    "# custom\n",
    "from sscode.config import data_path, default_location, \\\n",
    "    default_region, default_region_reduced\n",
    "from sscode.data import Loader\n",
    "from sscode.experiment import Experiment\n",
    "\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for autocomplete code\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666fb87",
   "metadata": {},
   "source": [
    "## Load the data, SLP and SS\n",
    "\n",
    "Below, we load the slp fields and the Moana v2 hindcast data nearshore. Have three things in mind:\n",
    "\n",
    "* First, the winds are **NOT** loaded, as we prefer to calculate the gradient of the slp fields. Having in mind that winds have more resolution (more computation is required) and the fact that we are playing around with projected winds, adds this new calculation to each step (and the Loader class is calculated outside the class Experiment, so the projedted winds are calculated just to one location).\n",
    "\n",
    "* The `plot` parameter is set to `False`, but can be set to `True` if all the plots are wanted to be seen.\n",
    "\n",
    "* The Moana v2 hindcast nearshore is used, and although the gridded netCDF data can be used, which includes ss data all over the New Zealand region, the data might be transformed to `dim=site` inside the dataset, as we did in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8458cdab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " loading the sea-level-pressure fields... \n",
      "\n",
      "\n",
      " loading daily resampled data... \n",
      "\n",
      "\n",
      " loading the Moana v2 hindcast data... \n",
      "\n",
      "\n",
      " loading and plotting the UHSLC tidal guages... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "load_cfsr_moana_uhslc = Loader(\n",
    "    data_to_load=['cfsr','moana','uhslc'], plot=False, \n",
    "    time_resample='1D', load_winds=True\n",
    ")\n",
    "\n",
    "# ---- maybe DAC reanalysis wants to be used ----\n",
    "# dataset1 = load_cfsr_moana_uhslc.predictand\n",
    "# dataset1_coords = ('longitude','latitude',None,'DAC ...')\n",
    "# dataset1 = xr.Dataset(\n",
    "#     {\n",
    "#         'ss': (('time','site'), dataset1.values.reshape(\n",
    "#             -1,len(dataset1[dataset1_coords[0]])*len(dataset1[dataset1_coords[1]]))),\n",
    "#         dataset1_coords[0]: (('site'), list(dataset1[dataset1_coords[0]].values)*int(\n",
    "#             (len(dataset1[dataset1_coords[0]])*len(dataset1[dataset1_coords[1]]))\\\n",
    "#                 /len(dataset1[dataset1_coords[0]]))),\n",
    "#         dataset1_coords[1]: (('site'), np.repeat(dataset1[dataset1_coords[1]].values,\n",
    "#             (len(dataset1[dataset1_coords[0]])*len(dataset1[dataset1_coords[1]]))\\\n",
    "#                 /len(dataset1[dataset1_coords[1]])))\n",
    "#     }, coords={\n",
    "#         'site': np.arange(len(dataset1[dataset1_coords[0]])*len(dataset1[dataset1_coords[1]])),\n",
    "#         'time': dataset1.time.values\n",
    "#     }\n",
    "# ).dropna(dim='site',how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9cdd9",
   "metadata": {},
   "source": [
    "## Create / Run -- Experiment object\n",
    "\n",
    "In the two cells below, we first choose all the values that will have the different parameters for the model that will be used, and then we create the instance of the class `Experiment`, where different logs will appear before running all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e5a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment attributes\n",
    "# ---------------------\n",
    "\n",
    "sites_to_analyze = np.unique( # closest Moana v2 Hindcast to tidal gauges\n",
    "    [ 689,328,393,1327,393,480,999,116,224,1124,949,708, # UHSLC\n",
    "      1296,378,1124,780,613,488,1442,1217,578,200,1177,1025,689,949,224,1146, # LINZ\n",
    "      1174,1260,1217,744,1064,1214,803,999 # OTHER (ports...)\n",
    "    ]\n",
    ")\n",
    "pca_attrs_exp = {\n",
    "    'calculate_gradient': [False,True],\n",
    "    'winds': [False,True],\n",
    "    'time_lapse': [1,2,3], # 1 equals to NO time delay \n",
    "    'time_resample': ['6H','12H','1D'], # 6H and 12H available...\n",
    "    'region': [('local',(2.1,2.1)),('local',(4.1,4.1)),(True,default_region_reduced)]\n",
    "}\n",
    "linear_attrs_exp = {\n",
    "    'train_size': [0.7], 'percentage_PCs': [0.98]\n",
    "}\n",
    "knn_attrs_exp = {\n",
    "    'train_size': [0.7], 'percentage_PCs': [0.98],\n",
    "    'k_neighbors': [None,3,6,9] # None calculates the optimum k-neighs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760ce63",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Please check `Experiment` logs before running `execute_cross_model_calculations()`!!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a5347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The model has been correctly initialized with || model = linear ||             \n",
      "\n",
      " and model evaluation metrics = ['expl_var', 'mae', 'mse', 'me', 'medae', 'tweedie', 'ext_mae', 'ext_mse', 'ext_rmse', 'ext_pearson', 'bias', 'si', 'rmse', 'pearson', 'spearman', 'rscore']             \n",
      "\n",
      " pca_params = {'calculate_gradient': [False, True], 'winds': [False, True], 'time_lapse': [1, 2, 3], 'time_resample': ['6H', '12H', '1D'], 'region': [('local', (2.1, 2.1)), ('local', (4.1, 4.1)), (True, (160, 185, -30, -52))]} \n",
      "\n",
      " model_params = {'train_size': [0.7], 'percentage_PCs': [0.98]}             \n",
      "\n",
      " which makes a total of 108 iterations as there are (2, 2, 3, 3, 3, 1, 1) values for each parameter             \n",
      "\n",
      " the experiment will be performed in sites = [ 116  200  224  328  378  393  480  488  578  613  689  708  744  780\n",
      "  803  949  999 1025 1064 1124 1146 1174 1177 1214 1217 1260 1296 1327\n",
      " 1442]             \n",
      "\n",
      " RUN CELL BELOW if this information is correct!!\n"
     ]
    }
   ],
   "source": [
    "# create the experiment\n",
    "experiment = Experiment(\n",
    "    load_cfsr_moana_uhslc.predictor_slp, load_cfsr_moana_uhslc.predictor_wind,\n",
    "    # load_cfsr_moana_uhslc.predictor_wind,\n",
    "    load_cfsr_moana_uhslc.predictand, # all the sites are passed to exp at first\n",
    "    sites_to_analyze=sites_to_analyze, \n",
    "    model='linear', # model that will be used to predict\n",
    "    model_metrics=['expl_var','mae','mse','me',\n",
    "        'medae','tweedie', # check theory\n",
    "        'ext_mae','ext_mse','ext_rmse','ext_pearson',\n",
    "        'bias','si','rmse','pearson','spearman','rscore'\n",
    "    ],\n",
    "    pca_attrs=pca_attrs_exp,\n",
    "    model_attrs=linear_attrs_exp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b1ad6",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Once the instance of the class is well created, lets run all the models running the cell below\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f697ea9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# run all the models\n",
    "exp_parameters, exp_mean_parameters = experiment.execute_cross_model_calculations(\n",
    "    verbose=True, plot=False # plot logs when computing the models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7053bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can save the results, using the following function\n",
    "def save_final_results(to_save_name: str = \n",
    "    '../data/statistics/experiments/experiment_linear_final.nc'):\n",
    "    \n",
    "    \"\"\"\n",
    "        This function saves the results in netCDF4 format\n",
    "    \"\"\"\n",
    "    \n",
    "    sites_datasets = [] # save all xarray.Dataset's in sites\n",
    "    for site,exp_metrics_site in zip(experiment.ss_sites,exp_parameters):\n",
    "        site_metrics = {}\n",
    "        for im,metric in enumerate(experiment.model_metrics):\n",
    "            site_metrics[metric] = (\n",
    "                ('grad','winds','tlapse','tresample','region','tsize','perpcs'),\n",
    "                exp_metrics_site[:,:,:,:,:,:,:,im]) \n",
    "        sites_datasets.append(xr.Dataset(site_metrics).expand_dims(\n",
    "            {'site':[site]}\n",
    "        ))\n",
    "    experiment_metrics = xr.concat(sites_datasets,dim='site')\n",
    "    experiment_metrics.to_netcdf(to_save_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c891cc4",
   "metadata": {},
   "source": [
    "save_final_results('../data/statistics/experiments/experiment_linear_final_args.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
